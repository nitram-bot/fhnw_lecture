{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98593719",
   "metadata": {},
   "source": [
    "Als Regressions-Residuen bezeichnet man\n",
    " 1. den Anteil der abhängingen Variable der durch die unabhängigen Variablen nicht erklärt werden kann\n",
    " 2. die Länge des Lots von der Regressions-Gerade zu den Messwerten $y$\n",
    " 3. die quadrierten Vorhersagefehler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56184ce",
   "metadata": {},
   "source": [
    "Die Koeffizienten einer klassischen linearen Regression \n",
    " 1. können durch einfache algebraische Formeln bestimmt werden\n",
    " 2. müssen durch ein iteratives Verfahren geschätzt werden\n",
    " 3. müssen mit Hilfe eines Newton-Verfahrens bestimmt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d645d9",
   "metadata": {},
   "source": [
    "Die Koeffizienten der klassischen linearen Regression mit nur einer unabhängigen Variablen entsprechen graphisch\n",
    " 1. dem Punkt an dem die Regressions-Gerade die y-Achse schneidet und der Steigung der Regressionsgeraden\n",
    " 2. dem arithmetischen Mittel der x-Variablen und der Steigung der Geraden an dieser Stelle\n",
    " 3. dem Mittelwert der x-Variablen und dem Mittelwert der y-Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5156084",
   "metadata": {},
   "source": [
    "Für die klassische lineare Regression mit gleich vielen unabhängigen Variablen wie Beobachtungen gilt\n",
    " 1. die Regression kann nicht mehr besser werden, nur Gradient-Boosting liefert noch bessere Ergebnisse\n",
    " 2. das Modell passt perfekt auf die Beobachtungen, neue Beobachtungen werden aber wahrscheinlich grosse Vorhersagefehler haben\n",
    " 3. overfitting tritt nur bei kleinen Datensätzen auf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e74c76",
   "metadata": {},
   "source": [
    "Ridge-Regression ist ein Verfahren\n",
    " 1. overfitting zu verhindern\n",
    " 2. Vorhersagen auch bei nicht-linearen Zusammenhängen machen zu können\n",
    " 3. um trotz der Verletzung der Voraussetzungen eine lineare Regression rechnen zu können"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d1758",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    " 1. Bei Lasso-Regression werden Ausreisser durch geschickte Koeffizientenwahl eingefangen\n",
    " 2. Bei Lasso-Regression gehen die absoluten Beträge der Koeffizienten in die Fehlerfunktion mit ein\n",
    " 3. Lasso-Regression verwendet die quadrierten und absoluten Werte der Koeffizienten um overfitting zu verhindern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398b07b",
   "metadata": {},
   "source": [
    "Um bei Verletzung der Voraussetzungen eines analytischen Verfahrens dennoch Konfidenzintervalle bestimmen zu können\n",
    " 1. kann ein Bootstrapping-Verfahren verwendet werden\n",
    " 2. muss immer Kreuzvalidierung verwendet werden\n",
    " 3. kommt der Elastic-Net Algorithmus zu Einsatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee67d2",
   "metadata": {},
   "source": [
    "Logistische Regression\n",
    " 1. kommt vor allem bei Transportunternehmen zum Einsatz\n",
    " 2. ist streng genommen auch eine lineare Regression\n",
    " 3. hat eine lineare Aktivierungsfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0e147",
   "metadata": {},
   "source": [
    "Neuronale Feed-Forward Netzwerke beinhalten\n",
    " 1. viele lineare Regressionen parallel und nacheinander geschaltet\n",
    " 2. Bestandteile, die ihnen hohe Plastizität geben\n",
    " 3. sind ein Beispiel für non-lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbe307",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    " 1. Data-Leakage kann mit Kreuz-Validierung verhindert werden\n",
    " 2. Wenn bei Trainieren eines Modells Information verfügbar ist, die beim Einsatz des Modells nicht zur Verfügung steht, kann Data-Leakage entstehen.\n",
    " 3. Data-Leakage ist in der Korrelation-Matrix erkennbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8426b1e",
   "metadata": {},
   "source": [
    "Welche Kombination aus oversampling und Kreuzvalidierung führt zur besten Schätzung?\n",
    " 1. oversampling und anschliessende Kreuzvalidierung\n",
    " 2. oversampling, anschliessende Kreuzvalidierung und innerhalb der folds undersampling\n",
    " 3. oversampling innerhalb der folds einer Kreuzvalidierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b6f0d",
   "metadata": {},
   "source": [
    "Wann liegt eine Interaktion vor?\n",
    " 1. Wenn der Effekt zweier Variablen $x_1$ und $x_2$ nicht unabhängig von der Variablen $y$ sind\n",
    " 2. Wenn die Werte der abhängigen Variablen $y$ mit den Werten einer anderen Variablen $x$ multipliziert werden\n",
    " 3. Wenn der Effekt zwischen einer Variablen $x_1$ und der abhängigen Variablen $y$ von dem Wert einer anderen Variablen $x_2$ abhängig ist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d20e5",
   "metadata": {},
   "source": [
    "Klassifikations-Bäume können fehlende Werte sehr gut handhaben, weil\n",
    " 1. sie fehlende Werte einfach ignorieren\n",
    " 2. weil sie fehlende Werte einfach durch den Mittelwert der Variablen ersetzen\n",
    " 3. weil sie fehlende Werte als zusätzliche Kategorie der Variablen modellieren können"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0947a4",
   "metadata": {},
   "source": [
    "Für Regressions-Bäume gilt: Ist eine Variable nicht normal verteilt, \n",
    " 1. so muss sie durch eine Box-Cox Transformation angepasst werden\n",
    " 2. so stellt das für den Baum-Algorithmus kein Problem dar\n",
    " 3. so muss die Variable auf den Mittelwert $0$ normiert werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e415ff4",
   "metadata": {},
   "source": [
    "Klassifikations-Bäume haben Variablen-Splits, die\n",
    " 1. mit Koeffizienten gewichtet werden, so dass beliebige Entscheidungs-Grenzen angenähert werden können.\n",
    " 2. immer orthogonal zu den Variablen-Achsen sind.\n",
    " 3. immer nur an den Endpunkten der Variablen-Bins gemacht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cf9cf",
   "metadata": {},
   "source": [
    "Der Random-Forest Algorithmus kann\n",
    " 1. beliebige lineare und nicht-lineare Entscheidungsgrenzen approximieren.\n",
    " 2. kann immer nur sequenziell Bäume miteinander kombinieren\n",
    " 3. keine brauchbare feature-importance liefern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2c74d",
   "metadata": {},
   "source": [
    "Quadratische Fehlerfunktionen sind leicht zu optimieren weil\n",
    " 1. es immer eine einfache algebraische Lösung gibt.\n",
    " 2. für sie schon viele mathematische Verfahren existieren.\n",
    " 3. sie immer nur einen Extremwert (Minimum/Maximum) haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e9835",
   "metadata": {},
   "source": [
    "Beim Gradient-Boosting Algorithmus werden\n",
    " 1. sequentiell die Ergebnisse vieler Entscheidungsbäume gewichtet und aufsummiert.\n",
    " 2. sehr viele einzelne Bäume berechnet und ihre Ergebnisse mit Hilfe von Ridge-Regression miteinander kombiniert.\n",
    " 3. Bäume so berechnet dass die Fehlerfunktion immer einen positiven Gradienten hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed16d24",
   "metadata": {},
   "source": [
    "Feature-Importance mit Gini-Impurity\n",
    " 1. bevorzugt Variablen mit hoher Kardinalität auch wenn die Variable keinerlei Information beinhaltet\n",
    " 2. permutiert eine Variable immer zufällig um so die Wichtigkeit der Variable zu ermitteln\n",
    " 3. kann auch mit entropy als Kriterium berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2eca7",
   "metadata": {},
   "source": [
    "Ist eine Variable one-hot-encoded, so ist die feature importance einer einzelnen Indikator-Variable\n",
    " 1. stets genau so hoch, wie die der nicht-enkodierten Variablen\n",
    " 2. stets kleiner als die der nicht-enkodierten Variablen\n",
    " 3. stets um den Faktor $1/n$ kleiner als die der nicht-enkodierten Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d088a",
   "metadata": {},
   "source": [
    "Variablen die identische Information beinhalten\n",
    " 1. erkennt man mit Hilfe der feature-importance anhand ähnlicher Wichtigkeit\n",
    " 2. erkennt man in der Korrelations-Matrix anhand einer hohen Korrelation\n",
    " 3. erkennt man erst, wenn eine aus dem Modell entfernt wird und das Modell sich nicht ändert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad9e65",
   "metadata": {},
   "source": [
    "Support-Vektor-Machines eignen sich\n",
    " 1. Daten mit niedriger Dimensionalität.\n",
    " 2. nur wenn Kernel-Projektionen durchführbar sind.\n",
    " 3. besonders für kleine Datensätze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbf73d",
   "metadata": {},
   "source": [
    "Der Kernel-Trick besteht darin, dass\n",
    " 1. die Ähnlichkeit der Datenpunkte in einer höheren Dimension bestimmt werden kann, ohne die Daten explizit in diese zu projizieren\n",
    " 2. die Daten in eine höhere Dimension projiziert werden und in dieser miteinander multipliziert werden\n",
    " 3. man nur Datenpunkte berücksichtigt, die in einer Nachbarschaft mit positiver Kernel-Density liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8ed41",
   "metadata": {},
   "source": [
    "Das Skalarprodukt (dot-product) zweier standardisierter Vektoren ist gleich\n",
    " 1. dem Determinationskoeffizienten $R^2$.\n",
    " 2. der cosine-similarity.\n",
    " 3. dem quadrierten Korrelationskoeffizienten $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef5bfb",
   "metadata": {},
   "source": [
    "Machine-Learning-Algorithmen mit vielen Parametern\n",
    " 1. haben immer bessere Ergebnisse als Algorithmen mit weniger Parametern, wenn diese weniger Kombinationsmöglichkeiten haben.\n",
    " 2. sind aufgrund der hohen Komplexität sehr flexibel anwendbar.\n",
    " 3. sind aufgrund der grossen Anzahl an möglichen Parameterkombinationenschwer schwer zu optimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4e5fa",
   "metadata": {},
   "source": [
    "Was sind support-Vektoren im Zusammenhang mit der Support-Vektor-Machine?\n",
    " 1. Die Vectoren $\\pmb{x}^T$ und $\\pmb{x}$, deren Skalar-Produkt (dot-product) berechnet wird.\n",
    " 2. Die Datenpunkte, die auf dem sog. 'margin' zum liegen kommen, der um die Trennungsebene gezogen wird.\n",
    " 3. Die Vektoren, die addiert werden um die mathematischen Lösung der charakteristischen Gleichung zu stabilisieren."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
