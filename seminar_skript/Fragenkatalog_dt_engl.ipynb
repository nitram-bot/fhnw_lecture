{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6343e5",
   "metadata": {},
   "source": [
    "### Frage 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98593719",
   "metadata": {},
   "source": [
    "Als Regressions-Residuen bezeichnet man\n",
    " 1. den Anteil der abhängingen Variablen, der durch die unabhängigen Variablen nicht erklärt werden kann. X\n",
    " 2. die Länge des Lots von der Regressions-Gerade auf die Messwerte $y$.\n",
    " 3. die quadrierten Vorhersagefehler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbeaa1",
   "metadata": {},
   "source": [
    "Regression residuals are defined as\n",
    " 1. the proportion of the dependent variable that cannot be explained by the independent variables.\n",
    " 2. the length of the perpendicular from the regression line to the measured values $y$.\n",
    " 3. the squared prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd948b73",
   "metadata": {},
   "source": [
    "### Frage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56184ce",
   "metadata": {},
   "source": [
    "Die Koeffizienten einer klassischen linearen Regression \n",
    " 1. können durch einfache algebraische Formeln bestimmt werden. X\n",
    " 2. müssen durch ein iteratives Verfahren geschätzt werden.\n",
    " 3. müssen mit Hilfe eines Newton-Verfahrens bestimmt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd385e",
   "metadata": {},
   "source": [
    "The coefficients of a classical linear regression \n",
    " 1. can be determined by simple algebraic formulae. \n",
    " 2. must be estimated by an iterative procedure.\n",
    " 3. must be determined by means of a Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff6117",
   "metadata": {},
   "source": [
    "### Frage 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d645d9",
   "metadata": {},
   "source": [
    "Die Koeffizienten der klassischen linearen Regression mit nur einer unabhängigen Variablen entsprechen graphisch\n",
    " 1. dem Punkt an dem die Regressions-Gerade die y-Achse schneidet und der Steigung der Regressionsgeraden. X\n",
    " 2. dem arithmetischen Mittel der x-Variablen und der Steigung der Geraden an dieser Stelle.\n",
    " 3. dem Mittelwert der x-Variablen und dem Mittelwert der y-Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39bec8",
   "metadata": {},
   "source": [
    "The coefficients of the classical linear regression with only one independent variable graphically correspond to\n",
    " 1. the point at which the regression line intersects the y-axis and the slope of the regression line.\n",
    " 2. the arithmetic mean of the x variables and the slope of the straight line at this point.\n",
    " 3. the mean value of the x-variable and the mean value of the y-variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1f612",
   "metadata": {},
   "source": [
    "### Frage 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5156084",
   "metadata": {},
   "source": [
    "Für die klassische lineare Regression mit gleich vielen unabhängigen Variablen wie Beobachtungen gilt\n",
    " 1. die Regression kann nicht mehr besser werden, nur Gradient-Boosting liefert noch bessere Ergebnisse.\n",
    " 2. das Modell passt perfekt auf die Beobachtungen, neue Beobachtungen werden aber wahrscheinlich grosse Vorhersagefehler haben. X\n",
    " 3. overfitting tritt nur bei kleinen Datensätzen auf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de981d5",
   "metadata": {},
   "source": [
    "For classical linear regression with the same number of independent variables as observations, the following applies\n",
    " 1. the regression cannot get any better, only gradient boosting gives even better results.\n",
    " 2. the model fits the observations perfectly, but new observations are likely to have large prediction errors.\n",
    " 3. overfitting only occurs with small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8630d8",
   "metadata": {},
   "source": [
    "### Frage 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e74c76",
   "metadata": {},
   "source": [
    "Ridge-Regression ist ein Verfahren\n",
    " 1. overfitting zu verhindern. X\n",
    " 2. Vorhersagen auch bei nicht-linearen Zusammenhängen machen zu können.\n",
    " 3. das ermöglicht, trotz der Verletzung der Voraussetzungen eine lineare Regression rechnen zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d83207",
   "metadata": {},
   "source": [
    "Ridge regression is a method\n",
    " 1. to prevent overfitting.\n",
    " 2. to be able to make predictions even in the case of non-linear correlations.\n",
    " 3. that allows for calculating a linear regression despite the violation of the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d313222",
   "metadata": {},
   "source": [
    "### Frage 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d1758",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    " 1. Bei Lasso-Regression werden Ausreisser durch geschickte Koeffizientenwahl eingefangen.\n",
    " 2. Bei Lasso-Regression gehen die absoluten Beträge der Koeffizienten in die Fehlerfunktion mit ein. X\n",
    " 3. Lasso-Regression verwendet die quadrierten und absoluten Werte der Koeffizienten um overfitting zu verhindern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c4d9c",
   "metadata": {},
   "source": [
    "Which statement is correct?\n",
    " 1. In lasso regression, outliers are caught by clever choice of coefficients.\n",
    " 2. In lasso regression the absolute values of the coefficients are included in the error function.\n",
    " 3. Lasso regression uses the squared and absolute values of the coefficients to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73089a07",
   "metadata": {},
   "source": [
    "### Frage 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1398b07b",
   "metadata": {},
   "source": [
    "Um bei Verletzung der Voraussetzungen eines analytischen Verfahrens dennoch Konfidenzintervalle bestimmen zu können\n",
    " 1. kann ein Bootstrapping-Verfahren verwendet werden. X\n",
    " 2. muss immer Kreuzvalidierung verwendet werden.\n",
    " 3. kommt der Elastic-Net Algorithmus zu Einsatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6d5e7",
   "metadata": {},
   "source": [
    "In order to still be able to determine confidence intervals when the prerequisites of an analytical procedure are violated\n",
    " 1. a bootstrapping procedure can be used.\n",
    " 2. cross-validation must always be used.\n",
    " 3. the Elastic-Net algorithm is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0ed32",
   "metadata": {},
   "source": [
    "### Frage 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee67d2",
   "metadata": {},
   "source": [
    "Logistische Regression\n",
    " 1. kommt vor allem bei Transportunternehmen zum Einsatz.\n",
    " 2. ist streng genommen auch eine lineare Regression. X\n",
    " 3. hat eine lineare Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866faabd",
   "metadata": {},
   "source": [
    "Logistic regression\n",
    " 1. is mainly used by transport companies.\n",
    " 2. is strictly speaking also a linear regression.\n",
    " 3. has a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284137e0",
   "metadata": {},
   "source": [
    "### Frage 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a0e147",
   "metadata": {},
   "source": [
    "Neuronale Feed-Forward Netzwerke beinhalten\n",
    " 1. viele lineare Regressionen parallel und nacheinander geschaltet. X\n",
    " 2. Bestandteile, die ihnen hohe Plastizität geben. \n",
    " 3. sind ein Beispiel für non-lineare Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb1fc0",
   "metadata": {},
   "source": [
    "Feed-forward neural networks involve\n",
    " 1. many linear regressions connected in parallel and in sequence.\n",
    " 2. components that give them high plasticity.\n",
    " 3. are an example of non-linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8791d0",
   "metadata": {},
   "source": [
    "### Frage 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbe307",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    " 1. Data-Leakage kann mit Kreuz-Validierung verhindert werden.\n",
    " 2. Wenn beim Trainieren eines Modells Information verfügbar ist, die beim Einsatz des Modells nicht zur Verfügung steht, kann Data-Leakage entstehen. X\n",
    " 3. Data-Leakage ist in der Korrelation-Matrix erkennbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d102a",
   "metadata": {},
   "source": [
    "Which statement is correct?\n",
    " 1. Data leakage can be prevented with cross-validation.\n",
    " 2. If information is available when training a model that is not available when using the model, data leakage can occur.\n",
    " 3. Data leakage is recognisable in the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819af10",
   "metadata": {},
   "source": [
    "### Frage 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8426b1e",
   "metadata": {},
   "source": [
    "Welche Kombination aus oversampling und Kreuzvalidierung führt zur besten Schätzung?\n",
    " 1. oversampling und anschliessende Kreuzvalidierung\n",
    " 2. oversampling, anschliessende Kreuzvalidierung und innerhalb der folds undersampling\n",
    " 3. oversampling innerhalb der folds einer Kreuzvalidierung X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6f374",
   "metadata": {},
   "source": [
    "Which combination of oversampling and cross-validation leads to the best estimate?\n",
    " 1. oversampling and subsequent cross-validation\n",
    " 2. oversampling, followed by cross-validation and undersampling within the folds.\n",
    " 3. oversampling within the folds of cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddaa70e",
   "metadata": {},
   "source": [
    "### Frage 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b6f0d",
   "metadata": {},
   "source": [
    "Wann liegt eine Interaktion vor?\n",
    " 1. Wenn der Effekt zweier Variablen $x_1$ und $x_2$ nicht unabhängig von der Variablen $y$ sind.\n",
    " 2. Wenn die Werte der abhängigen Variablen $y$ mit den Werten einer anderen Variablen $x$ multipliziert werden.\n",
    " 3. Wenn der Effekt zwischen einer Variablen $x_1$ und der abhängigen Variablen $y$ von dem Wert einer anderen Variablen $x_2$ abhängig ist. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b17cc",
   "metadata": {},
   "source": [
    "When is there an interaction?\n",
    " 1. When the effect of two variables $x_1$ and $x_2$ are not independent of the variable $y$.\n",
    " 2. When the values of the dependent variable $y$ are multiplied by the values of another variable $x$.\n",
    " 3. When the effect between a variable $x_1$ and the dependent variable $y$ depends on the value of another variable $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0827176",
   "metadata": {},
   "source": [
    "### Frage 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d20e5",
   "metadata": {},
   "source": [
    "Klassifikations-Bäume können fehlende Werte sehr gut handhaben, weil\n",
    " 1. sie fehlende Werte einfach ignorieren.\n",
    " 2. weil sie fehlende Werte einfach durch den Mittelwert der Variablen ersetzen.\n",
    " 3. weil sie fehlende Werte als zusätzliche Kategorie der Variablen modellieren können. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a7dbd",
   "metadata": {},
   "source": [
    "Classification trees can handle missing values very well because\n",
    " 1. they simply ignore missing values.\n",
    " 2. because they simply replace missing values with the mean value of the variable.\n",
    " 3. because they can model missing values as an additional category of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafc72a",
   "metadata": {},
   "source": [
    "### Frage 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0947a4",
   "metadata": {},
   "source": [
    "Für Regressions-Bäume gilt: Ist eine Variable nicht normal verteilt, \n",
    " 1. so muss sie durch eine Box-Cox Transformation angepasst werden.\n",
    " 2. so stellt das für den Baum-Algorithmus kein Problem dar. X\n",
    " 3. so muss die Variable auf den Mittelwert $0$ normiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55640ab",
   "metadata": {},
   "source": [
    "For regression trees, if a variable is not normally distributed, \n",
    " 1. it must be adjusted by a Box-Cox transformation.\n",
    " 2. this is not a problem for the tree algorithm.\n",
    " 3. the variable must be normalised to the mean $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb590",
   "metadata": {},
   "source": [
    "### Frage 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e415ff4",
   "metadata": {},
   "source": [
    "Klassifikations-Bäume haben Variablen-Splits, die\n",
    " 1. mit Koeffizienten gewichtet werden, so dass beliebige Entscheidungs-Grenzen angenähert werden können.\n",
    " 2. immer orthogonal zu den Variablen-Achsen sind. X\n",
    " 3. immer nur an den Endpunkten der Variablen-Bins gemacht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8c262",
   "metadata": {},
   "source": [
    "Classification trees have variable splits that are\n",
    " 1. are weighted with coefficients so that arbitrary decision boundaries can be approximated.\n",
    " 2. are always orthogonal to the variable axes.\n",
    " 3. are always made only at the endpoints of the variable bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049341d0",
   "metadata": {},
   "source": [
    "### Frage 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cf9cf",
   "metadata": {},
   "source": [
    "Der Random-Forest Algorithmus kann\n",
    " 1. beliebige lineare und nicht-lineare Entscheidungsgrenzen approximieren. X\n",
    " 2. kann immer nur sequenziell Bäume miteinander kombinieren.\n",
    " 3. keine brauchbare feature-importance liefern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843e80f",
   "metadata": {},
   "source": [
    "The Random Forest algorithm can\n",
    " 1. approximate arbitrary linear and non-linear decision boundaries.\n",
    " 2. can only ever combine trees sequentially.\n",
    " 3. does not provide useful feature-importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c4158",
   "metadata": {},
   "source": [
    "### Frage 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2c74d",
   "metadata": {},
   "source": [
    "Quadratische Fehlerfunktionen sind leicht zu optimieren weil\n",
    " 1. es immer eine einfache algebraische Lösung gibt.\n",
    " 2. für sie schon viele mathematische Verfahren existieren. \n",
    " 3. sie immer nur einen Extremwert (Minimum/Maximum) haben. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b64ee",
   "metadata": {},
   "source": [
    "Quadratic error functions are easy to optimise because\n",
    " 1. there is always a simple algebraic solution.\n",
    " 2. many mathematical methods already exist for them.\n",
    " 3. they always have only one extreme value (minimum/maximum)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4beab8",
   "metadata": {},
   "source": [
    "### Frage 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e9835",
   "metadata": {},
   "source": [
    "Beim Gradient-Boosted-Trees Algorithmus werden\n",
    " 1. sequentiell die Ergebnisse vieler Entscheidungsbäume gewichtet und aufsummiert. X\n",
    " 2. sehr viele einzelne Bäume berechnet und ihre Ergebnisse mit Hilfe von Ridge-Regression miteinander kombiniert.\n",
    " 3. Bäume so berechnet dass die Fehlerfunktion immer einen positiven Gradienten hat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6a26d",
   "metadata": {},
   "source": [
    "In the Gradient-Boosted-Trees algorithm\n",
    " 1. sequentially weights and sums the results of many decision trees.\n",
    " 2. a large number of individual trees are calculated and their results combined using ridge regression.\n",
    " 3. trees are calculated so that the error function always has a positive gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881e468",
   "metadata": {},
   "source": [
    "### Frage 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed16d24",
   "metadata": {},
   "source": [
    "Feature-Importance mit Gini-Impurity\n",
    " 1. bevorzugt Variablen mit hoher Kardinalität auch wenn die Variable keinerlei Information beinhaltet. X\n",
    " 2. permutiert eine Variable immer zufällig um so die Wichtigkeit der Variable zu ermitteln.\n",
    " 3. kann auch mit entropy als Kriterium berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93de7d",
   "metadata": {},
   "source": [
    "Feature-Importance with Gini-Impurity\n",
    " 1. prefers variables with high cardinality even if the variable does not contain any information.\n",
    " 2. always permutes a variable randomly to determine the importance of the variable.\n",
    " 3. can also be calculated with entropy as a criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a423c4",
   "metadata": {},
   "source": [
    "### Frage 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2eca7",
   "metadata": {},
   "source": [
    "Ist eine Variable one-hot-encoded, so ist die feature importance einer einzelnen Indikator-Variable\n",
    " 1. stets genau so hoch, wie die der nicht-enkodierten Variablen.\n",
    " 2. stets kleiner als die der nicht-enkodierten Variablen. X\n",
    " 3. stets um den Faktor $1/n$ kleiner als die der nicht-enkodierten Variablen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867d035",
   "metadata": {},
   "source": [
    "If a variable is one-hot-encoded, then the feature importance of a single indicator variable is\n",
    " 1. is always as high as that of the non-coded variables.\n",
    " 2. always smaller than that of the non-coded variables.\n",
    " 3. always smaller by a factor of $1/n$ than that of the non-coded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f891f",
   "metadata": {},
   "source": [
    "### Frage 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d088a",
   "metadata": {},
   "source": [
    "Variablen die identische Information beinhalten\n",
    " 1. erkennt man mit Hilfe der feature-importance anhand ähnlicher Wichtigkeit.\n",
    " 2. erkennt man in der Korrelations-Matrix anhand einer hohen Korrelation. X\n",
    " 3. erkennt man erst, wenn eine aus dem Modell entfernt wird und das Modell sich nicht ändert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae860b",
   "metadata": {},
   "source": [
    "Variables that contain identical information\n",
    " 1. can be recognised with the help of feature-importance on the basis of similar importance.\n",
    " 2. can be recognised in the correlation matrix by a high correlation.\n",
    " 3. can only be recognised when one is removed from the model and the model does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7cf44",
   "metadata": {},
   "source": [
    "### Frage 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad9e65",
   "metadata": {},
   "source": [
    "Support-Vektor-Machines eignen sich\n",
    " 1. für Daten mit niedriger Dimensionalität.\n",
    " 2. nur wenn Kernel-Projektionen durchführbar sind.\n",
    " 3. besonders für kleine Datensätze. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ef1e1",
   "metadata": {},
   "source": [
    "Support vector machines are suitable for\n",
    " 1. data with low dimensionality.\n",
    " 2. only if kernel projections are feasible.\n",
    " 3. especially for small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d1ded3",
   "metadata": {},
   "source": [
    "### Frage 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbf73d",
   "metadata": {},
   "source": [
    "Der Kernel-Trick besteht darin, dass\n",
    " 1. die Ähnlichkeit der Datenpunkte in einem höher dimensionalen Raum bestimmt werden kann, ohne die Daten explizit in diesen Raum projizieren zu müssen. X\n",
    " 2. die Daten in einen höher dimensionalen Raum projiziert werden und in diesem miteinander multipliziert werden.\n",
    " 3. man nur Datenpunkte berücksichtigt, die in einer Nachbarschaft mit positiver Kernel-Density liegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102fee9",
   "metadata": {},
   "source": [
    "The kernel trick is that\n",
    " 1. the similarity of the data points can be determined in a higher dimensional space without having to explicitly project the data into this space.\n",
    " 2. the data is projected into a higher dimension space and multiplied together in that space.\n",
    " 3. one only considers data points that lie in a neighbourhood with positive kernel density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6d50",
   "metadata": {},
   "source": [
    "### Frage 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8ed41",
   "metadata": {},
   "source": [
    "Das Skalarprodukt (dot-product) zweier standardisierter Vektoren ist gleich\n",
    " 1. dem Determinationskoeffizienten $R^2$.\n",
    " 2. der cosine-similarity. X\n",
    " 3. dem quadrierten Korrelationskoeffizienten $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b90f03",
   "metadata": {},
   "source": [
    "The dot-product of two standardised vectors is equal to\n",
    " 1. the coefficient of determination $R^2$.\n",
    " 2. the cosine-similarity.\n",
    " 3. the squared correlation coefficient $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b72d90",
   "metadata": {},
   "source": [
    "### Frage 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef5bfb",
   "metadata": {},
   "source": [
    "Machine-Learning-Algorithmen mit vielen Parametern\n",
    " 1. haben immer bessere Ergebnisse als Algorithmen mit weniger Parametern, weil diese weniger Kombinationsmöglichkeiten haben.\n",
    " 2. sind aufgrund der hohen Komplexität sehr flexibel anwendbar.\n",
    " 3. sind aufgrund der grossen Anzahl an möglichen Parameterkombinationen schwer zu optimieren. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928484e9",
   "metadata": {},
   "source": [
    "Machine learning algorithms with many parameters\n",
    " 1. always have better results than algorithms with fewer parameters because the later have fewer possible combinations.\n",
    " 2. are very flexible to use because of their high complexity.\n",
    " 3. are difficult to optimise because of the large number of possible parameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08cd71",
   "metadata": {},
   "source": [
    "### Frage 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b4e5fa",
   "metadata": {},
   "source": [
    "Was sind support-Vektoren im Zusammenhang mit der Support-Vektor-Machine?\n",
    " 1. Die Vectoren $\\pmb{x}^T$ und $\\pmb{x}$, deren Skalar-Produkt (dot-product) berechnet wird.\n",
    " 2. Die Datenpunkte, die auf dem sog. 'margin' zum liegen kommen, der um die Trennungsebene gezogen wird. X\n",
    " 3. Die Vektoren, die addiert werden um die mathematischen Lösung der charakteristischen Gleichung zu stabilisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551726d",
   "metadata": {},
   "source": [
    "What are support vectors in the context of the support vector machine?\n",
    " 1. the vectors $\\pmb{x}^T$ and $\\pmb{x}$ whose scalar product (dot-product) is calculated.\n",
    " 2. the data points that lie on the so-called 'margin', which is drawn around the separation plane.\n",
    " 3. the vectors that are added to stabilise the mathematical solution of the characteristic equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482353e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dccbe87e",
   "metadata": {},
   "source": [
    "### Frage 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d1821",
   "metadata": {},
   "source": [
    "Warum wird in der Mathematik für die Bezeichnung der Matrix der unabhängigen Variablen $\\pmb{X}$ ein grosser Buchstabe verwendet?\n",
    "  1. Der Vektor der abhängigen Variablen $\\pmb{y}$ hat zur Bezeichung einen kleinen Buchstaben, daher benötigen die unabhängigen Variablen einen Grossbuchstaben.\n",
    "  2. Matrizen werden im Gegensatz zu Vektoren mit Grossbuchstaben benannt. X\n",
    "  3. Vektoren werden mit Grossbuchstaben, Matrizen mit kleinen Buchstaben bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a31cc",
   "metadata": {},
   "source": [
    "Why is a capital letter used in mathematics to denote the matrix of independent variables $\\pmb{X}$?\n",
    "  1. The vector of the dependent variable $\\pmb{y}$ is denoted by a lower case letter, so the independent variables need a capital letter.\n",
    "  2. Matrices are named with capital letters, unlike vectors.\n",
    "  3. Vectors are named with capital letters, matrices with small letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df6229",
   "metadata": {},
   "source": [
    "### Frage 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfac47",
   "metadata": {},
   "source": [
    "Das Wort \"__linear__\" in \"Lineare Regression\" bedeutet, dass \n",
    "  1. die modellierten statistischen Zusammenhänge am besten durch eine gerade Linie angenähert werden können.\n",
    "  2. sich die Regressionsfunktion in ihren Parametern linear verhält. X\n",
    "  3. sich die Parameter der Regressionsfunktion alle auf einer Geraden befinden, welch die y-Achse beim Wert $\\bar{\\pmb{y}}$ schneidet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0863af2",
   "metadata": {},
   "source": [
    "The word \"__linear__\" in \"linear regression\" means that \n",
    "  1. the modelled statistical relationships can best be approximated by a straight line.\n",
    "  2. the regression function behaves linearly in its parameters.\n",
    "  3. the parameters of the regression function are all on a straight line that intersects the y-axis at the value $\\bar{\\pmb{y}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71278633",
   "metadata": {},
   "source": [
    "### Frage 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8f4dc",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    "  1. In der klassischen Regression kann es maximal so viele Variablen geben, wie unabhängive Beobachtungen vorliegen. X\n",
    "  2. Die klassische Regression dient zur Absicherung wissenschaftlicher Ergebnisse. In den Wissenschaften gibt es immer weniger freie Parameter als Beobachtungen.\n",
    "  3. Da die Parameter der klassischen Regression durch ein iteratives Verfahren gefunden werden, spielt die Anzahl der Variablen keine Rolle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b99ad8",
   "metadata": {},
   "source": [
    "Which statement is correct?\n",
    "  1. In classical regression there can be at most as many variables as there are independent observations.\n",
    "  2. Classical regression is used to validate scientific results. In the sciences, there are always fewer free parameters than observations.\n",
    "  3. Since the parameters of classical regression are found by an iterative procedure, the number of variables does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c622ddc",
   "metadata": {},
   "source": [
    "### Frage 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefb704",
   "metadata": {},
   "source": [
    "Lasso Regression\n",
    "  1. macht viele Koeffizienten sehr klein (faktisch Null) und einige Wenige sehr gross. X\n",
    "  2. reguliert alle Koeffizienten in gleicher Weise.\n",
    "  3. bevorzugt grosse Koeffizienten und ignoriert viele der kleinen Koeffizienten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f03df",
   "metadata": {},
   "source": [
    "Lasso regression\n",
    "  1. makes many coefficients very small (effectively zero) and a few very large.\n",
    "  2. regulates all coefficients in the same way.\n",
    "  3. favours large coefficients and ignores many of the small coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8dd2b",
   "metadata": {},
   "source": [
    "### Frage 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0629f74",
   "metadata": {},
   "source": [
    "Random-Forest Classifier sind gut parallelisierbar \n",
    "  1. weil die Variablen-Histogramme auf getrennten compute-nodes berechnet werden können.\n",
    "  2. weil grosse Datensätze einfach zwischen den compute-nodes verteilt werden können.\n",
    "  3. weil die einzelnen Bäume unabhängig voneinander auf verschiedenen compute-nodes trainiert werden können. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371858ae",
   "metadata": {},
   "source": [
    "Random forest classifiers can be parallelised well \n",
    "  1. because the variable histograms can be computed on separate compute-nodes.\n",
    "  2. because large data sets can be easily distributed between the compute-nodes.\n",
    "  3. because the individual trees can be trained independently on different compute-nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc26e9",
   "metadata": {},
   "source": [
    "### Frage 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d6512",
   "metadata": {},
   "source": [
    "Welche Aussage ist richtig?\n",
    "  1. Random Forest kombiniert viele grosse, zufällig trainierte Bäume, während Gradient Boosted Trees kleine sequentielle Bäume bevorzugt.\n",
    "  2. Sowohl Random Forest, wie auch Gradient Boosted Trees kombinieren viele kleine Bäume miteinander. X\n",
    "  3. Grosse Bäume haben immer bessere Vorhersagekraft als kleine Bäume, weshalb Gradient Boosted Trees und Random Forest diese klar bevorzugen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7671dba",
   "metadata": {},
   "source": [
    "Which statement is correct?\n",
    "  1. Random forest combines many large, randomly trained trees, while gradient boosted trees favours small sequential trees.\n",
    "  2. Both, Random Forest and Gradient Boosted Trees combine many small trees.\n",
    "  3. Large trees always have better predictive power than small trees, which is why Gradient Boosted Trees and Random Forest clearly prefer them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc413b",
   "metadata": {},
   "source": [
    "### Frage 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce390b4",
   "metadata": {},
   "source": [
    "In der Machine Learning community bezeichnen kernel\n",
    "  1. den Python-kernel, der die Python-Instanz der Jupyter-notebooks ist.\n",
    "  2. die verschieden Unix-Betriebssysteme. Der bekanntest kernel ist der Linux-kernel.\n",
    "  3. ganz allgemein ein Ähnlichkeitsmass, welches nicht unbedingt euklidscher Distanz oder cosine similarity entsprechen muss. X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0a54a",
   "metadata": {},
   "source": [
    "In the machine learning community, kernel refers to\n",
    "  1. the Python kernel, which is the Python instance of the Jupyter notebooks.\n",
    "  2. the various Unix operating systems. The best known kernel is the Linux kernel.\n",
    "  3. a similarity measure, which does not necessarily have to correspond to Euclidean distance or cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1209d0d",
   "metadata": {},
   "source": [
    "### Frage 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e1755",
   "metadata": {},
   "source": [
    "In Bayesian Optimization\n",
    "  1. wird eine Ersatzfunktion (surrogate function) trainiert, die Güte eines Machine Learning Algorithmus (objective function) für verschiedene Parameter-Kombinationen vorher zu sagen. X\n",
    "  2. wird die bedingte Wahrscheinlichkeit A gegeben B durch die bedingte Wahrscheinlichkeit von B gegeben A und die jeweiligen prior-Wahrscheinlichkeiten ausgedrückt. \n",
    "  3. wird ein parameterfreies Netzwerk trainiert Parameter voher zu sagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb96be2",
   "metadata": {},
   "source": [
    "In Bayesian Optimization,\n",
    "  1. a surrogate function is trained to predict the performance of a machine learning algorithm (objective function) for different parameter combinations.\n",
    "  2. the conditional probability A given B is expressed by the conditional probability of B given A and the respective prior probabilities. \n",
    "  3. a parameter-free network is trained to predict parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e5076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
