%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        

\title{My sample book}
\date{Jul 14, 2021}
\release{}
\author{The Jupyter Book Community}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\sphinxAtStartPar
This is a small sample book to give you a feel for how book content is
structured.

\sphinxAtStartPar
:::\{note\}
Here is a note!
:::

\sphinxAtStartPar
And here is a code block:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{e} \PYG{o}{=} \PYG{n}{mc}\PYG{o}{\PYGZca{}}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\sphinxAtStartPar
Check out the content pages bundled with this sample book to see more.

\sphinxAtStartPar
I wonder if we can have some math formulae as well:

\sphinxAtStartPar
\(\alpha = 3\)

\sphinxAtStartPar
or like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{}\PYGZbs{}alpha = 3\PYGZdl{}
\end{sphinxVerbatim}

\sphinxAtStartPar
and even the more advanced stuff?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}


\chapter{Lineare Regression}
\label{\detokenize{Regression_Techniques:lineare-regression}}\label{\detokenize{Regression_Techniques::doc}}
\sphinxAtStartPar
In der nachfolgenden Zelle werden zuerst Daten geladen, die zur Veranschaulichung der linearen Regression dienen.
Anschliessend wird ein lineares Modell mit Hilfe der der Klasse Lineare Regression aus \sphinxcode{\sphinxupquote{sklearn.linear\_model}} gerechnet. Die Vorhersage (d.h. die Geradengleichung) ergibt sich aus den Koeffizienten durch \(y = a + bX\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}} \PYG{o}{*} \PYG{n}{X} \PYG{o}{+} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Warum wird für \(\mathbf{X}\) immer ein Grossbuchstabe verwendet und für \(\mathbf{y}\) ein kleiner Buchstabe ?

\sphinxAtStartPar
Die Matrix der Variablen X wird gross geschrieben, da in Matrix\sphinxhyphen{}Notation Matrizen immer mit grossen Buchstaben bezeichnet werden, Vektoren \sphinxhyphen{} so wie die abhängige Variable y \sphinxhyphen{} werden mit kleinen Buchstaben benannt.

\noindent\sphinxincludegraphics{{Regression_Techniques_5_0}.png}

\sphinxAtStartPar
Der Plot zeigt die berechnete Regressionsgerade, sowie die Abweichungen (die Fehler) der wirklichen Messwerte von dieser Geraden. Diese Abweichungen werden als \sphinxstylestrong{Residuen} bezeichnet, weil es der Anteil der gemessenen Werte ist, der “übrig bleibt”, d.h. nicht durch das Modell erklärt werden kann. Vorhergesagte Variablen werden meist mit einem Dach (Hut) bezeichnet, sowie \(\hat{y}\).


\chapter{Analytische Herleitung der Parameter der Linearen Regression}
\label{\detokenize{Regression_Techniques:analytische-herleitung-der-parameter-der-linearen-regression}}
\sphinxAtStartPar
Allgemein kann man den Nullpunkt einer quadratischen Funktion bestimmen, indem man ihre erste Ableitung gleich \(0\) setzt. Die erste Ableitung gibt die Steigung der Funktion an. In der Physik ist dies of die Beschleunigung. Die Steigung ist am Minimum der Funktion schliesslich \(0\). Man beachte, dass quadratische Funktionen immer nur einen Maximalwert haben können.

\sphinxAtStartPar
Nachfolgend ist dieser Sachverhalt für die quadratische Funktion \(f(x) = (x-1)^2\) dargestellt. Die Ableitung
\(2x-2\) ist ebenfalls eingetragen. Bei dem Minimum der Funktion ist die erste Ableitung gleich \(0\) (die Stelle an der der Funktionsgraph, der der ersten Ableitung und die rote, horizontale Linie sich schneiden).

\noindent\sphinxincludegraphics{{Regression_Techniques_8_0}.png}

\sphinxAtStartPar
Die Parameter einer linearen Regression können analytisch berechnet werden. Dazu wird der quadrierte Fehler \((y_i-\hat{y}_i)^2\) über alle Messwerte aufsummiert. Diese Summe wird nach den Parametern abgeleitet und gleich \(0\) gesetzt. Somit erhält man die Stelle an der die quadratische Funktion keine Steigung (erste Ableitung ist Steigung) hat. Weil eine quadratische Funktion als einzige Nullstelle der Steigung ein Minimum hat, erhalten wir somit die Parameter an dem Minimum unserer quadratischen Fehlerfunktion.


\section{derivative of the error term \protect\((y - \hat{y})^2\protect\):}
\label{\detokenize{Regression_Techniques:derivative-of-the-error-term-y-hat-y-2}}\begin{itemize}
\item {} 
\sphinxAtStartPar
für \(\hat{y}\) können wir auch schreiben: \(a + b\cdot x\), dies ist die Vorhersage mit Hilfe der Regression\sphinxhyphen{}Gerade (der Geraden\sphinxhyphen{}Gleichung):

\end{itemize}
\begin{equation*}
\begin{split}\sum_i^{n}(y_i - \hat{y_i})^2 = \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
wir leiten diese Fehler\sphinxhyphen{}Funktion nach \(a\) ab und setzen diese erste Ableitung gleich \(0\) (Hierbei wird die Kettenregel verwendet):

\end{itemize}
\begin{align*}
\frac{\delta \sum_i^{n}(y_i - \hat{y_i})^2}{\delta a} = -2\sum_i^{n}y_i + 2b\sum_i^{n}x_i + 2na =& 0\\
2na =& 2\sum_i^{n}y_i - 2b\sum_i^{n}x_i\\
  a =& \frac{2\sum_i^{n}y_i}{2n} - \frac{2b\sum_i^{n}x_i}{2n}
\end{align*}\begin{itemize}
\item {} 
\sphinxAtStartPar
die Summe über alle \(x_i\) geteilt durch \(n\) – die Anzahl aller Beobachtungen – ergibt den Mittelwert \(\bar{x}\), gleiches gilt für \(\bar{y}\):

\end{itemize}
\begin{equation*}
\begin{split}a = \bar{y} - b\bar{x}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
die Lösung für \(b\) ergibt sich analog; hier ersetzen wir \(a\) mit obigen Ergebnis und erhalten:

\end{itemize}
\begin{equation*}
\begin{split} b = \frac{\frac{1}{n}\sum_i^n(x_i - \bar{x})(y_i - \bar{y})}{\frac{1}{n}\sum_i^n (x_i - \bar{x})^2} = 
\frac{\text{cov}_{xy}}{\text{var}_x}\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Vereinfacht ist die Former: Kovarianz der beiden Variablen \(x\) und \(y\) geteilt durch die Varianz von \(x\).

\end{itemize}

\sphinxAtStartPar
Nachfolgend wird demonstriert, wie die hergeleiteten Formeln, in python angewendet dieselben Parameter\sphinxhyphen{}Schätzer ergeben wie die aus der Klasse \sphinxcode{\sphinxupquote{LineareRegression}} aus \sphinxcode{\sphinxupquote{sklearn.linear\_model}}. Dies soll einfach nur demonstrieren, dass die alles ganz leicht zu rechnen ist und keiner komplizierten Algorithmen bedarf.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we can easily verify these results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameter b is the coefficient of the linear model }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameter a is called the intercept of the model because it indicates}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ where the regression line intercepts the y\PYGZhy{}axis at x=0 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{cov\PYGZus{}xy} \PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{var\PYGZus{}x} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}\PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{b} \PYG{o}{=} \PYG{n}{cov\PYGZus{}xy}\PYG{o}{/}\PYG{n}{var\PYGZus{}x}
\PYG{n}{a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{b}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{our self\PYGZhy{}computed b parameter is: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{b}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{our self\PYGZhy{}computed a parameter is: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{a}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
the parameter b is the coefficient of the linear model [[8.07912445]]
the parameter a is called the intercept of the model because it indicates
 where the regression line intercepts the y\PYGZhy{}axis at x=0 [\PYGZhy{}8.49032154]

our self\PYGZhy{}computed b parameter is: 8.079124453577005
our self\PYGZhy{}computed a parameter is: \PYGZhy{}8.490321540681798
\end{sphinxVerbatim}


\chapter{multivariate case: more than one x variable}
\label{\detokenize{Regression_Techniques:multivariate-case-more-than-one-x-variable}}
\sphinxAtStartPar
Für Multivariate Lineare Regression kann die Schreibweise mit Matrizen zusammengefasst werden. Dafür kann es lohnend sein, sich die Matrizen\sphinxhyphen{}Multiplikation noch einmal kurz anzusehen.
\begin{align*}
    y_1&=a+b_1\cdot x_{11}+b_2\cdot x_{21}+\cdots + b_p\cdot x_{p1}\\
    y_2&=a+b_1\cdot x_{12}+b_2\cdot x_{22}+\cdots + b_p\cdot x_{p2}\\
    \ldots& \ldots\\
    y_i&=a+b_1\cdot x_{1i}+b_2\cdot x_{2i}+\cdots + b_p\cdot x_{pi}\\
\end{align*}\begin{equation*}
    \begin{bmatrix}
      y_1\\
      y_2\\
      .  \\
      .  \\
      .  \\
      y_i
    \end{bmatrix}
    = a+
    \begin{bmatrix}
      x_{11} & x_{21} & x_{31} & \ldots & x_{p1}\\
      x_{12} & x_{22} & x_{32} & \ldots & x_{p2}\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      x_{1i} & x_{2i} & x_{3i} & \ldots & x_{pi}\\
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
 b_1\\
      b_2\\
      .\\
      .\\
      .\\
      b_p
    \end{bmatrix}
\end{equation*}
\sphinxAtStartPar
Den konstanten inercept Term (\(a\)) können wir mit in den Vektor der Parameter \(\mathbf{b}\) aufnehmen, indem wir in \(\mathbf{X}\) eine Einser\sphinxhyphen{}Spalte hinzufügen. Somit wird die Schreibweise sehr kompakt und der intercept \(a\) wird nicht mehr explizit aufgeführt:
\begin{equation*}
     \begin{bmatrix}
      y_1\\
      y_2\\
      .  \\
      .  \\
      .  \\
      y_i
    \end{bmatrix}
    =
    \begin{bmatrix}
      1& x_{11} & x_{21} & x_{31} & \ldots & x_{p1}\\
      1 &  x_{12} & x_{22} & x_{32} & \ldots & x_{p2}\\
      &\ldots&\ldots&\ldots&\ldots&\ldots\\
      &\ldots&\ldots&\ldots&\ldots&\ldots\\
      1& x_{1i} & x_{2i} & x_{3i} & \ldots & x_{pi}
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
      a\\
      b_1\\
      b_2\\
      .\\
      .\\
      b_p
    \end{bmatrix}
  \end{equation*}
\sphinxAtStartPar
In Matrizen\sphinxhyphen{}Schreibweise können wir jetzt einfach schreiben:
\(\mathbf{y} = \mathbf{X}\mathbf{b}\)


\chapter{derivation of \protect\(\mathbf{b}\protect\) for the matrix notation}
\label{\detokenize{Regression_Techniques:derivation-of-mathbf-b-for-the-matrix-notation}}
\sphinxAtStartPar
Anschliessend wird die Berechnung der Parameter der Multivariaten Regression in Matrizen\sphinxhyphen{}Schreibweise erläutert.  Konzeptionell ist dies nicht vom univariaten Fall verschieden. Diese Formel wird nur hergeleitet um demonstrieren zu können, wie das Ergebnis der expliziten Berechnung in Python mit dem aus der sklearn Klasse \sphinxcode{\sphinxupquote{LinearRegression}} übereinstimmt.
\begin{itemize}
\item {} 
\sphinxAtStartPar
we expand the error term:
\begin{align*}
    \text{min}=&(\mathbf{y}-\hat{\mathbf{y}})^2=(\mathbf{y}-\mathbf{X}\mathbf{b})'(\mathbf{y}-\mathbf{X}\mathbf{b})=\\
    &(\mathbf{y}'-\mathbf{b}'\mathbf{X}')(\mathbf{y}-\mathbf{X}\mathbf{b})=\\
    &\mathbf{y}'\mathbf{y}-\mathbf{b}'\mathbf{X}'\mathbf{y}-\mathbf{y}'
    \mathbf{X}\mathbf{b}+\mathbf{b}'\mathbf{X}'\mathbf{X}\mathbf{b}=\\
    &\mathbf{y}'\mathbf{y}-2\mathbf{b}'\mathbf{X}'\mathbf{y}+\mathbf{b}'\mathbf{X}'
    \mathbf{X}\mathbf{b}\\
  \end{align*}
\item {} 
\sphinxAtStartPar
derivative of the error term with respect to \(\mathbf{b}\)

\item {} 
\sphinxAtStartPar
we set the result equal to zero and solve for \(\mathbf{b}\)

\end{itemize}
\begin{align*}
    \frac{\delta}{\delta
      \mathbf{b}}=&-2\mathbf{X}'\mathbf{y}+2\mathbf{X}'\mathbf{X}\mathbf{b}=0\\
    2\mathbf{X}'\mathbf{X}\mathbf{b}=&2\mathbf{X}'\mathbf{y}\\
    \mathbf{b}=&(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\quad
  \end{align*}
\sphinxAtStartPar
Hierbei bedarf es der Inversion des Kreuzproduktes der Variablen\sphinxhyphen{}Matrix \((\mathbf{X}'\mathbf{X})^{-1}\). Die Matrizen\sphinxhyphen{}Inversion ist für grosse Anzahl von Variablen mathematisch sehr aufwändig und kann unter Umständen zu Ungenauigkeiten führen. In der Vergangenheit wurde viel an Algorithmen geforscht um die Inversion schneller und stabiler zu machen. Oftmals stehen Fehlermeldungen in Zusammenhang mit diesem Berechnungsschritt.


\chapter{Polynomial regression as an example for more than one variable}
\label{\detokenize{Regression_Techniques:polynomial-regression-as-an-example-for-more-than-one-variable}}
\sphinxAtStartPar
Um einfach Multivariate Lineare Regression an einem Beispiel zeigen zu können wird die quadratische Regression (ein Spezial\sphinxhyphen{}Fall der Multivariaten Regression) eingeführt. Eine neue Variable entsteht durch das Quadrieren der bisherigen univiaraten Variable x. Das Praktische ist, dass sich der Sachverhalt der Multivariaten Regression noch immer sehr schön 2\sphinxhyphen{}dimensional darstellen lässt.
\(y = a + b_1 x + b_2 x^2\)

\sphinxAtStartPar
Hier ist zu beachten:
\begin{itemize}
\item {} 
\sphinxAtStartPar
wir haben jetzt zwei Variablen und können folglich unsere Formel in Matrizen\sphinxhyphen{}Schreibweise anwenden

\item {} 
\sphinxAtStartPar
mehr Variablen führen hoffentlich zu einem besseren Modell

\item {} 
\sphinxAtStartPar
durch den quadratischen Term ist die resultierende Regressions\sphinxhyphen{}Funktion keine Gerade mehr.
\sphinxstylestrong{Der Ausdruck “linear” in Linearer Regression bedeutet dass die Funktion linear in den Parametern
\(a, \mathbf{b}_\mathbf{1}, \mathbf{b}_\mathbf{2}\) ist. Für alle Werte einer Variablen \(\mathbf{x_1}\) gilt der gleiche Parameter \(\mathbf{b_1}\).
Es bedeutet nicht, dass die Regressions\sphinxhyphen{}Funktion durch eine gerade Linie gegeben ist!}

\end{itemize}

\sphinxAtStartPar
Nachfolgend fügen wir die weitere Variable durch Quadrieren der bisherigen Variable hinzu und berechnen abermals das Lineare Modell aus \sphinxcode{\sphinxupquote{sklearn.linear\_model}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{inv}
\PYG{c+c1}{\PYGZsh{} polynomial}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} underdetermined, ill\PYGZhy{}posed: infinitely many solutions}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} the x (small x) is just for plotting purpose}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}

\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x} \PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}  \PYG{o}{+} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}5.0, 110.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_18_1}.png}

\sphinxAtStartPar
Jetzt berechnen wir die Parameter der Multiplen Linearen Regression mit Hilfe der hergeleiteten Formeln. Hierfür fügen wir zu den bisherigen Variablen \(x\) und \(x^2\) noch eine Einser\sphinxhyphen{}Spalte für den intercpet ein. \sphinxcode{\sphinxupquote{np.dot}} berechnet das dot\sphinxhyphen{}product zweier Variablen. Um das Kreuzprodukt von \(\mathbf{X}\) berechnen zu können, muss eine der beiden Matrizen transponiert werden. Dies geschieht durch \sphinxcode{\sphinxupquote{.T}}.
\sphinxcode{\sphinxupquote{inv}} invertiert das Kreuzprodukt.

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{coefs = np.dot(np.dot(inv(np.dot(X\_intercept.T,X\_intercept)),X\_intercept.T),y)}} ist gleichbedeutend mit:
\begin{equation*}
\mathbf{b}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} again we can compare the parameters of the model with those resulting from }
\PYG{c+c1}{\PYGZsh{} our derived equation:}
\PYG{c+c1}{\PYGZsh{} b=(X\PYGZsq{}X)\PYGZca{}\PYGZob{}\PYGZhy{}1\PYGZcb{} X\PYGZsq{}y}
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{inv}

\PYG{c+c1}{\PYGZsh{} first we have to add the intercept into our X\PYGZhy{}Variable; we rename it X\PYGZus{}intercept}
\PYG{n}{X\PYGZus{}intercept} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{p}{]}
\PYG{n}{coefs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X\PYGZus{}intercept}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}\PYG{n}{X\PYGZus{}intercept}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{X\PYGZus{}intercept}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{p}{,}\PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameter b is the coefficient of the linear model }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameter a is called the intercept of the model because it indicates}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ where the regression line intercepts the y\PYGZhy{}axis at x=0 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{our coefs already include the intercept: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{coefs}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
the parameter b is the coefficient of the linear model [[\PYGZhy{}12.14930516   1.68570247]]
the parameter a is called the intercept of the model because it indicates
 where the regression line intercepts the y\PYGZhy{}axis at x=0 [35.33794262]
our coefs already include the intercept: [[ 35.33794262]
 [\PYGZhy{}12.14930516]
 [  1.68570247]]
\end{sphinxVerbatim}


\section{Overfitting}
\label{\detokenize{Regression_Techniques:overfitting}}
\sphinxAtStartPar
Nun wird diese Vorgehensweise für weitere Terme höherer Ordnung angewendet. Graphisch lässt sich zeigen, dass die Anpassung des Modells an die Daten immer besser wird, die Vorhersage für \sphinxstylestrong{neue Datenpunkte} aber sehr schlecht sein dürfte. Das Polynom hat an vielen Stellen Schlenker und absurde Kurven eingebaut. Dies ist ein erstes Beispiel für \sphinxstylestrong{“overfitting”}.Einen ‘perfekten’ fit erhält man, wenn man genausoviele Paramter (10 Steigunskoeffizienten + intercept) hat wie Daten\sphinxhyphen{}Messpunkte.

\sphinxAtStartPar
The important points to note here:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the fit to our empirical y\sphinxhyphen{}values gets better

\item {} 
\sphinxAtStartPar
at the same time, the regression line starts behaving strangly

\item {} 
\sphinxAtStartPar
the predictions made by the regression line in between the empirical y\sphinxhyphen{}values are grossly wrong: this is an example of \sphinxstylestrong{overfitting}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} underdetermined, ill\PYGZhy{}posed: infinitely many solutions}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{9}\PYG{p}{]}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{9}\PYG{p}{]}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x} \PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}  \PYG{o}{+} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 115.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_23_1}.png}


\section{perfect fit: as many variables as data samples}
\label{\detokenize{Regression_Techniques:perfect-fit-as-many-variables-as-data-samples}}
\sphinxAtStartPar
A perfect fit is possible as is demonstrated next. We have as many variables (terms derived from x) as observations (data points). So for each data point we have a variable to accommodate it.
\sphinxstylestrong{Note}, that a perfect fit is achieved with 10 variables + intercept. The intercept is also a parameter and in this case the number of observations \(n\) equals the number of variables \(p\), i.e. \(p=n\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} underdetermined, ill\PYGZhy{}posed: infinitely many solutions}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{10}\PYG{p}{]}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{10}\PYG{p}{]}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x} \PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}  \PYG{o}{+} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the intercept and the coefficients are: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
the intercept and the coefficients are: [\PYGZhy{}3441.3761578], [[ 9.78847039e+03 \PYGZhy{}1.13028575e+04  7.22272630e+03 \PYGZhy{}2.87529040e+03
   7.50863939e+02 \PYGZhy{}1.30675765e+02  1.49834150e+01 \PYGZhy{}1.08409478e+00
   4.47395935e\PYGZhy{}02 \PYGZhy{}8.00879370e\PYGZhy{}04]]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 125.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_27_1}.png}


\chapter{What happens if we have more variables than data points?}
\label{\detokenize{Regression_Techniques:what-happens-if-we-have-more-variables-than-data-points}}
\sphinxAtStartPar
Gibt es mehr Parameter als Datenpunkte, existieren unendlich viele Lösungen und das Problem ist nicht mehr eindeutig lösbar. Früher gelang die Inversion des Kreuzproduktes der Variablen \(\mathbf{X}'\mathbf{X}\) nicht. Mittlerweile gibt es Näherungsverfahren, die dennoch Ergebnisse liefern \sphinxhyphen{} wenn auch sehr Ungenaue.

\sphinxAtStartPar
Mittlerweile gibt es aber mathematische Näherungsverfahren die es ermöglichen auch singuläre Matrizen zu invertieren.
\sphinxcode{\sphinxupquote{numpy}} verwendet hierfür die sogenannte LU\sphinxhyphen{}decomposition.

\sphinxAtStartPar
One way to see in python that the solution is erroneous is to use the \sphinxcode{\sphinxupquote{scipy.linalg.solve}} package and solve for the matix S that solves \((\mathbf{X}'\mathbf{X})^{-1} \mathbf{S} = \mathbf{I}\). \(\mathbf{I}\) is called the eye\sphinxhyphen{}matrix wih 1s in the diagonale and zeros otherwise:
\begin{equation*}
\begin{split}
\mathbf{I}=\left[
\begin{array}{ccc}
   1 & \cdots & 0 \\
   \vdots & \ddots & \vdots \\
   0 & \cdots & 1
\end{array}
\right]
\end{split}
\end{equation*}
\sphinxAtStartPar
Die entscheidende Zeile im nachfolgenden Code ist:
\sphinxcode{\sphinxupquote{S = solve(inv(np.dot(X.T, X)), np.eye(13))}}

\sphinxAtStartPar
Sie besagt: gib mir die Matrix \(\mathbf{S}\), die multipliziert mit \((\mathbf{X}'\mathbf{X})^{-1}\) die Matrix \(\mathbf{I}\) gibt.
Für unseren Fall von mehr Variablen als Beobachtungspunkten werden wir gewarnt, dass das Ergebnis falsch sein könnte. Mit älteren Mathematik\sphinxhyphen{} oder Statistik\sphinxhyphen{}Programmen ist dies überhaupt nicht möglich.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{warnings}\PYG{o}{.}\PYG{n}{filterwarnings}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{default}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{numpy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{inv}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{linalg} \PYG{k+kn}{import} \PYG{n}{solve}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} underdetermined, ill\PYGZhy{}posed: infinitely many solutions}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{11}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{13}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} this should give at least a warning, because matrix inversion as done above is not possible}
\PYG{c+c1}{\PYGZsh{} any more, due to singular covariance matrix [X\PYGZsq{}X]}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}y\PYGZus{}hat = np.dot(x , model.coef\PYGZus{}.T)  + model.intercept\PYGZus{}}
\PYG{n}{S} \PYG{o}{=} \PYG{n}{solve}\PYG{p}{(}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{l+m+mi}{13}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/home/martin/miniconda3/envs/book/lib/python3.7/site\PYGZhy{}packages/ipykernel\PYGZus{}launcher.py:15: LinAlgWarning: Ill\PYGZhy{}conditioned matrix (rcond=3.8573e\PYGZhy{}21): result may not be accurate.
  from ipykernel import kernelapp as app
\end{sphinxVerbatim}


\section{statistical package R}
\label{\detokenize{Regression_Techniques:statistical-package-r}}
\sphinxAtStartPar
In der statistischen Programmiersprache R wird keine Warnung herausgegeben. Es werden einfach nur soviele Koeffizienten (intercept ist auch ein Koeffizient) berechnet, wie möglich ist. Alle weiteren Koeffizienten sind \sphinxcode{\sphinxupquote{NA}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/home/martin/miniconda3/envs/book/lib/python3.7/site\PYGZhy{}packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should\PYGZus{}run\PYGZus{}async` will not call `transform\PYGZus{}cell` automatically in the future. Please pass the result to `transformed\PYGZus{}cell` argument and any exception that happen during thetransform in `preprocessing\PYGZus{}exc\PYGZus{}tuple` in IPython 7.17 and above.
  and should\PYGZus{}run\PYGZus{}async(code)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_31_1}.png}


\chapter{Dealing with overfitting}
\label{\detokenize{Regression_Techniques:dealing-with-overfitting}}
\sphinxAtStartPar
Wie wir gesehen haben tendiert klassische Lineare Regression zu ‘overfitting’ sobald es wenige Datenpunkte gibt und mehrere Koeffizienten berechnet werden. 
Eine Lösung für dieses Problem ist, die Koeffizienten \(b_1, b_2, b_3, \ldots\) kleiner zu machen. Dies kann erreicht werden, wenn der Fehler der Regression mit grösseren Koeffizienten auch grösser wird. Um nun das Minimum der Fehlerfunktion zu finden ist ein probates Mittel, die Koeffizienten kleiner zu machen und somit implizit ‘overfitting’ zu verhindern.
Parameter können jetzt nur noch sehr gross werden, wenn dadurch gleichzeitig der Fehler stark reduziert werden kann.

\sphinxAtStartPar
Nachfolgend wird ein Strafterm (‘penalty’) für grosse Parameter eingeführt. Im Falle der Ridge\sphinxhyphen{}Regression gehen die Koeffizienten quadriert in die Fehlerfunktion mit ein. Der Gewichtungsfaktor \(\lambda\) bestimmt die Höhe des Strafterms und ist ein zusätzlicher Parameter für den – je nach Datensatz – ein optimaler Wert gefunden werden muss.


\section{Ridge regression}
\label{\detokenize{Regression_Techniques:ridge-regression}}
\sphinxAtStartPar
Remember this formula:
\begin{equation*}\sum_i^{n}(y_i - \hat{y_i})^2 = \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}\end{equation*}
\sphinxAtStartPar
To make the error term bigger, we could simply add \(\lambda\cdot b^2\) to the error:
\begin{equation*}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda b^2= \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}+ \lambda b^2\end{equation*}
\sphinxAtStartPar
The parameter \(\lambda\) is for scaling the amount of shrinkage.
Die beiden Ausdrücke
\label{equation:Regression_Techniques:9e0ee006-233d-4001-a276-fead6aca26e3}\begin{equation}\sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}\label{eq:fehler}\end{equation}\label{equation:Regression_Techniques:e9db3919-f45d-4421-9301-62a089f9975c}\begin{equation}\lambda b^2\label{eq:ridge_error}\end{equation}
\sphinxAtStartPar
sind wie Antagonisten. Der Koeffizient \(b\) darf nur gross werden, wenn er es vermag \(\eqref{eq:fehler}\) stark zu verkleinern, so dass der Zugewinn in \(\eqref{eq:fehler}\) den Strafterm in \(\eqref{eq:ridge_error}\) überwiegt.

\sphinxAtStartPar
For two variables we can write:
\begin{equation*}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda b_1^2 + \lambda b_2^2= \sum_i^{n}[y_i - (a + b_1\cdot x_{i1} + b_2\cdot x_{i2})]^{2}+ \lambda b_1^2 + \lambda b_2^2\end{equation*}
\sphinxAtStartPar
And in matrix notation for an arbitrary number of variables:
\begin{align*}
    \text{min}=&(\mathbf{y}-\hat{\mathbf{y}})^2 + \lambda \mathbf{b}^2=(\mathbf{y}-\mathbf{X}\mathbf{b})'(\mathbf{y}-\mathbf{X}\mathbf{b}) + \lambda \mathbf{b}'\mathbf{b}
\end{align*}
\sphinxAtStartPar
Interessanterweise gibt es für diesen Fall ebenfalls eine exakte analytische Lösung. 
Allerdings haben wir den intercept Koeffizienten \(a\) mit in \(\mathbf{b}\) aufgenommen und die zusätzliche Spalte mit lauter Einsern in \(\mathbf{X}\) hinzugefügt. Wenn wir nun \(\lambda \mathbf{b}'\mathbf{b}\) berechnen, den quadrierten Strafterm für den Parametervektor, dann würden wir auch \(a\) bestrafen. Die Rolle von \(a\) ist aber, die Höhenlage der Regressionsfunktion zu definieren (die Stelle an der die Funktion die y\sphinxhyphen{}Achse schneidet).
Der intercept \(a\) kann allerdings aus der Gleichung genommen werden, wenn die Variablen vorher standardisiert werden (Mittelwert \(\bar{x} = 0\) und \(\bar{y} = 0\)). Jetzt verschwindet \(a\) von ganz allein, wenn wir die standardisierten Mittelwerte in die Gleichung für \(a\) einfügen:
\begin{equation*}
a=\bar{y} - b\bar{x} = 0 - b\cdot 0 = 0
\end{equation*}


\sphinxAtStartPar
Nun muss \(a\) nicht mehr berücksichtigt werden und die Lösung für \(\mathbf{b}\) ergibt sich zu:
\begin{equation*}\hat{\mathbf{b}} = (\mathbf{X}'\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}'\mathbf{y}\end{equation*}
\sphinxAtStartPar
Nach Hastie et al., wurde dieses Verfahren ursprünglich verwendet um ‘rank deficiency’ Probleme zu beheben. Wenn Die Spalten oder Zeilen einer Matrix nicht lineare unabhängig sind, so hat die Matrix nicht vollen Rang. Beispielsweise kann sich eine Spalte durch Addition anderer Spalten ergeben. In diesem Fall funktionierte die Matrix Inversion nicht zufriedenstellend. Als Lösung hat man gefunden, dass es ausreichend ist, einen kleinen positiven Betrag zu den Diagonal\sphinxhyphen{}Elementen der Matrix zu addieren.
Dies wird nachfolgend in einem numerischen Beispiel gezeigt:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{np.c\_}} fügt die einzelenne Variablen zu einer Matrix zusammen

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{np.dot(X.T, X)}} ist das bekannte Kreuzprodukt der transponierten Matrix \(\mathbf{X'}\) und \(\mathbf{X}\)

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{np.linalg.matrix\_rank}} gibt uns den Rang der Matrix

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{np.eye(7) * 2}} erstellt eine Diagonal\sphinxhyphen{}Matrix mit 2 in der Diagonalen und 0 überall sonst

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}6} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{With 6 variables (polynom of 6th degree), the rank of the quare matrix}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ is }\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
      \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{matrix\PYGZus{}rank}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X\PYGZus{}6}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X\PYGZus{}6}\PYG{p}{)}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{X\PYGZus{}7} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{With 7 variables (polynom of 7th degree), the rank of the quare matrix}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ is }\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
      \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{matrix\PYGZus{}rank}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X\PYGZus{}7}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X\PYGZus{}7}\PYG{p}{)}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{By adding a small amount to the diagonal of the matrix, it is of full rank}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ again: }\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{matrix\PYGZus{}rank}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X\PYGZus{}7}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X\PYGZus{}7}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}\PYGZsh{} you can see how small this amount is, by having a glimpse on the diagonal elements:}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{to see how small the added amount in reality is, we display the diagonal elements:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X\PYGZus{}7}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{X\PYGZus{}7}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
With 6 variables (polynom of 6th degree), the rank of the quare matrix
 is 6
With 7 variables (polynom of 7th degree), the rank of the quare matrix
 is 6
By adding a small amount to the diagonal of the matrix, it is of full rank
 again: 7

to see how small the added amount in reality is, we display the diagonal elements:
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([            506,           39974,         3749966,       382090214,
           40851766526,   4505856912854, 507787636536686])
\end{sphinxVerbatim}


\subsection{example of ridge regression}
\label{\detokenize{Regression_Techniques:example-of-ridge-regression}}
\sphinxAtStartPar
Next, we will apply ridge regression as implemented in the python \sphinxcode{\sphinxupquote{sklearn}} library and compare the results to the linear algebra solution. Note, that we have to center the variables.
\begin{itemize}
\item {} 
\sphinxAtStartPar
we can center \(\mathbf{X}\) and \(\mathbf{y}\) and display the result in the centered coordinate system

\item {} 
\sphinxAtStartPar
or we can center \(\mathbf{X}\) and add the mean of \(\mathbf{y}\) to the predicted values to display the result in the original coordinate system. This approaches allows for an easy comparison to the overfitted result

\end{itemize}

\sphinxAtStartPar
Die Zeile \sphinxcode{\sphinxupquote{Xc = X \sphinxhyphen{} np.mean(X, axis=0)}} standardisiert die Variablen auf den Mittelwert von 0

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{Ridge}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{c+c1}{\PYGZsh{} here is the necessary standardization:}
\PYG{n}{Xc} \PYG{o}{=} \PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} for plotting purpose}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{xc} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{axis} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the result as obtained from the sklearn library}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{Ridge}\PYG{p}{(}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{fit\PYGZus{}intercept}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{Xc}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameters from the sklearn library:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the analytical result as discussed above}
\PYG{n}{inverse} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Xc}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Xc}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{Xc}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{Xy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Xc}\PYG{p}{)}\PYG{p}{,}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{params} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{inverse}\PYG{p}{,} \PYG{n}{Xy}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameters as obtained from the analytical solution:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{params}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{params\PYGZus{}ridge} \PYG{o}{=} \PYG{n}{params}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
the parameters from the sklearn library:
[[\PYGZhy{}1.96523108e\PYGZhy{}01 \PYGZhy{}6.47914003e\PYGZhy{}01 \PYGZhy{}9.37247119e\PYGZhy{}01  1.55320112e\PYGZhy{}01
   3.20681202e\PYGZhy{}02 \PYGZhy{}6.80277139e\PYGZhy{}03  3.08899915e\PYGZhy{}04]]
the parameters as obtained from the analytical solution:
[[\PYGZhy{}1.96523119e\PYGZhy{}01 \PYGZhy{}6.47914004e\PYGZhy{}01 \PYGZhy{}9.37247118e\PYGZhy{}01  1.55320112e\PYGZhy{}01
   3.20681203e\PYGZhy{}02 \PYGZhy{}6.80277139e\PYGZhy{}03  3.08899915e\PYGZhy{}04]]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_39_0}.png}


\section{Lasso}
\label{\detokenize{Regression_Techniques:lasso}}
\sphinxAtStartPar
Alternativ zu einem quadratischen Strafterm \(b^2\) könnte man auch den absoluten Wert nehmen \(|b|\). In diesem Fall erhält man die sog.\textasciitilde{}Lasso Regression; \(\lambda\cdot |b|\) wird zum Vorhersage\sphinxhyphen{}Fehler addiert:
\begin{equation*}
\begin{split}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda |b|= \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}+ \lambda |b|\end{split}
\end{equation*}
\sphinxAtStartPar
Für zwei Variablen würde man folglich schreiben:
\begin{equation*}
\begin{split}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda |b_1| + \lambda |b_2|= \sum_i^{n}[y_i - (a + b_1\cdot x_{i1} + b_2\cdot x_{i2})]^{2}+ \lambda |b_1| + \lambda |b_2|\end{split}
\end{equation*}
\sphinxAtStartPar
Leider gibt es im Gegesatz zur Ridge Regression keine eindeutige analytische Lösung um die Koeffizienten der Lasso Regression zu erhalten. Hier kommen iterative Verfahren zum Einsatz, wie wir sie in Session 2 kennen lernen werden.


\subsection{Vergleich der Koeffizienten der Lasso Regression mit denen der Ridge Regression}
\label{\detokenize{Regression_Techniques:vergleich-der-koeffizienten-der-lasso-regression-mit-denen-der-ridge-regression}}
\sphinxAtStartPar
Next, we will apply lasso regression as implemented in the python sklearn library and compare the results to the unconstraint regression results.
As before, we have to center the variables (\sphinxhyphen{}> see discussion above)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{Lasso}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{Xc} \PYG{o}{=} \PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} for plotting purpose}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{xc} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{axis} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the result as obtained from the sklearn library}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{Lasso}\PYG{p}{(}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{fit\PYGZus{}intercept}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{Xc}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{params\PYGZus{}lasso} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}

\PYG{c+c1}{\PYGZsh{} comparison of parameters ridge vs. lasso:}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameters of the ridge regression:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{params\PYGZus{}ridge}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{the parameters of the lasso regression:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{params\PYGZus{}lasso}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
the parameters of the ridge regression:
[[\PYGZhy{}1.96523119e\PYGZhy{}01 \PYGZhy{}6.47914004e\PYGZhy{}01 \PYGZhy{}9.37247118e\PYGZhy{}01  1.55320112e\PYGZhy{}01
   3.20681203e\PYGZhy{}02 \PYGZhy{}6.80277139e\PYGZhy{}03  3.08899915e\PYGZhy{}04]]
the parameters of the lasso regression:
[\PYGZhy{}0.00000000e+00 \PYGZhy{}1.27169261e+00  2.49755651e\PYGZhy{}01  7.47152651e\PYGZhy{}04
 \PYGZhy{}5.77539403e\PYGZhy{}04 \PYGZhy{}2.73002774e\PYGZhy{}05  1.76588437e\PYGZhy{}06]
\end{sphinxVerbatim}

\sphinxAtStartPar
Ridge Regression tendiert dazu alle Koeffizienten im gleichen Mass zu verkleinern. Lasso führt oft zu Lösungen, bei denen einige Koeffizienten ganz zu \(0\) konvergiert sind. Wenn man die Ergebnisse im obigen Beispiel betrachtet, fällt einem auf dass für Lasso eigentlich nur zwei Koeffizienten verschieden von \(0\) sind (for \(X^2\) and \(X^3\)).
Die Werte alle anderen Koeffizienten sind kleiner als \(0.000747 = 7.47\text{e}-04\).

\noindent\sphinxincludegraphics{{Regression_Techniques_43_0}.png}


\section{the difference between ridge and lasso}
\label{\detokenize{Regression_Techniques:the-difference-between-ridge-and-lasso}}
\sphinxAtStartPar
In der folgenden graphischen Darstellung haben die \sphinxstylestrong{wahren Koeffizienten} die Werte \(b_1=1.5,\quad b_2=0.5\). Für ein grid aus beliebigen Werten für \(b_1\) und \(b_2\) wird der \sphinxstylestrong{mean squared error} (MSE) berechnet und der Fehler als Kontur graphisch dargestellt. Wie man sieht, wird der Fehler umso geringer, je näher die Koeffizienten im grid an den wahren Koeffizienten liegen.
Als nächstes werden alle Koeffizienten\sphinxhyphen{}Kombinationen aus \(b_1\) und \(b_2\) eingetragen, deren Strafterm (\(b_1^2 + b_2^2\)
im Falle von Ridge und \(b_1 + b_2\) im Falle von Lasso) den Wert von \(1.0\) nicht übersteigt. Die Lösung, die den \sphinxstylestrong{wahren Koeffizienten} am nähesten ist, wird jeweils durch einen Punkt eingezeichnet.

\sphinxAtStartPar
Hierbei sieht man, dass sich die besten Lösungen von
Ridge auf einem Halbkreis bewegen, die von Lasso auf einem Dreieck. An der Stelle, an der die Lasso\sphinxhyphen{}Lösung der eigentlichen Lösung (b=1.5, b2=0.5) am Nähesten ist, ist ein Parameter (\(b_2\)) fast \(0\). Das zeigt die Tendenz von Lasso, einige Parameter gegen \(0\) zu schrumpfen. Dieses Verhalten kann man sich zum Beispiel bei Variablen\sphinxhyphen{}Selektion zu Nutzen machen.

\noindent\sphinxincludegraphics{{Regression_Techniques_45_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
optimal coefficients of the ridge solution: 0.9393939393939394 and 0.34283965148438655
optimal coefficients of the lasso solution: 1.0 and 0.0
\end{sphinxVerbatim}


\chapter{ElasticNet}
\label{\detokenize{Regression_Techniques:elasticnet}}
\sphinxAtStartPar
Aus der Physik kommend werden die Strafterme von Ridge und Lasso als \(\text{L}_2\) und \(\text{L}_1\) bezeichnet. Eigentlich ist die \(\text{L}_2\)\sphinxhyphen{}Norm die Quadratwurzel der Summe der quadrierten Elemente eines Vectors und die \(\text{L}_1\)\sphinxhyphen{}Norm nur die Summe der Vektorelemente.
ElasticNet ist ein lineares Regressions\sphinxhyphen{}Verfahren, in welches sowohl die regularization\sphinxhyphen{}terms von Lasso (\(\text{L}_1\)), als auch von Ridge (\(\text{L}_2\)) eingehen. Hier gibt es nicht nur einen \(\lambda\)\sphinxhyphen{}Paramter, der das Ausmass von regularization bestimmt, sondern einen zusätzlichen Parameter \(\alpha\), der das Verhältnis von \(\text{L}_1\) und \(\text{L}_2\) regularization angibt.

\sphinxAtStartPar
Weil Ridge Regression und Lasso die Koeffizienten sehr unterschiedlich regulieren, ist als Kompromiss die Kombination aus beiden Methoden sehr beliebt geworden.
\begin{equation*}
\lambda\sum_j (\alpha b_j^2 + (1-\alpha)|b_j|)
\end{equation*}
\sphinxAtStartPar
Die Interpretation der beiden paramter \(\lambda\) und \(\alpha\) ist wie folgt:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\lambda\) bestimmt das generelle Mass an regularisation

\item {} 
\sphinxAtStartPar
\(\alpha\) gibt das Verhältnis an, mit dem diese beiden Strafterme indie regularisation einfliessen sollen


\end{itemize}

\sphinxAtStartPar
Im Übungs\sphinxhyphen{}Notebook zu den Boston house\sphinxhyphen{}prices werden wir ElasticNet verwenden.


\chapter{Interaction}
\label{\detokenize{Regression_Techniques:interaction}}
\sphinxAtStartPar
Interaktionen sind ein weiteres wichtiges Konzept in der linearen Regression. Hier ist der Effekt einer Variablen auf die abhängige Variable \(y\) abhängig von dem Wert einer anderen Variable.

\sphinxAtStartPar
In unterem Beispiel versuchen wir die Wahrscheinlichkeit zu modellieren, dass eine Person ein Haus kauft. Natürlich ist das monatliche Einkommen eine wichtige Variable und desto höher dieses, desto wahrscheinlicher auch, dass besagte Person ein Haus kauft. Eine andere wichtige Variable ist der Zivilstand. Verheiratet Personen mit Kindern im Haushalt tendieren stark zu Hauskauf, besonders wenn das monatliche Einkommen hoch ist. Auf der anderen Seite werden Singles, auch wenn sie ein hohes Einkommen haben, eher nicht zum Hauskauf tendieren.
Wir sehen also, die Variable “monatliches Einkommen” \sphinxstylestrong{interagiert} mit der Variable “Zivilstand”:

\noindent\sphinxincludegraphics{{Regression_Techniques_48_0}.png}

\sphinxAtStartPar
Das obige Beispiel beinhaltete kategorielle Variablen. Beispiele wie diese trifft man oft im Bereich der Varianzanalysen (ANOVA) an.
Interaktions\sphinxhyphen{}Effekte bestehen aber auch für kontinuierliche Variablen. In diesem Fall ist es aber etwas komplizierter die Effekte zu visualisieren.
Wir werden jetzt unseren eigenen Datensatz so erzeugen, dass er einen deutlichen Interaktions\sphinxhyphen{}Effekt aufweist. Damit der Effekt zwischen 2 kontinuierlichen Variablen überhaut in 2D dargestellt werden kann, musss eine der beiden Variablen wieder diskretisiert werden, d.h. wir müssen für sie wieder Kategorien bilden.
Im nächsten Rechenbeispiel versuchen wir dann, die Parameter, die zur Generierung der Daten gedient haben mit einer Linearen\sphinxhyphen{}Regressions\sphinxhyphen{}Analyse wieder zu finden.
Die Daten wurden nach folgendem Modell generiert:
\begin{equation*}
y = 2\cdot x + -2\cdot m + -7\cdot (x\cdot m) + \text{np.random.normal(loc = 0, scale = 4, size = n)}
\end{equation*}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{np.random.normal(loc=0, scale=4, size=n)}} ist der Random\sphinxhyphen{}Error\sphinxhyphen{}Term, den wir hinzufügen, damit die Daten nicht alle auf einer Lienie liegen. \sphinxcode{\sphinxupquote{loc=0}} besagt, dass der Mittelwert unseres zufälligen Fehlers \(0\) ist, \sphinxcode{\sphinxupquote{scale=4}}, dass die Varianz der Werte \(4\) ist und \sphinxcode{\sphinxupquote{size=n}} gibt die Anzahl der zu generierenden zufälligen Werte an

\sphinxAtStartPar
Folgliche haben wir also die Koeffizienten:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(b_x = 2\)

\item {} 
\sphinxAtStartPar
\(b_m = -2\)

\item {} 
\sphinxAtStartPar
\(b_{x\cdot m} = -7\)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size} \PYG{o}{=} \PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}  lin effects + interaction + random error}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{x} \PYG{o}{+} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{m} \PYG{o}{+} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{7}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{o}{*}\PYG{n}{m}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{size} \PYG{o}{=} \PYG{n}{n}\PYG{p}{)}

\PYG{n}{newM} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{small}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{average}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{large}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}


\PYG{n}{toy} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{n}{x}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{n}{y}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{moderator}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{n}{newM}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{moderator}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{toy}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_50_0}.png}

\sphinxAtStartPar
Interaktions\sphinxhyphen{}Terme können gebildet werden, indem man zwei Variablen elemente\sphinxhyphen{}weise miteinander multipliziert.
Durch die Hinzuname weiterer Terme sollte die Modell\sphinxhyphen{}Anpassung eigentlich besser werden \sphinxhyphen{} besonders wenn ein starker Interaktionsterm in den Daten vorliegt, so wie wir ihn eingebaut haben.
Vergleichen wir die Koeffizienten, so wie sie im Linearen\sphinxhyphen{}Modell gefunden werden mit denen, die zur Erzeugung unseres Datensatzes gedient haben. Gar nicht schlecht, oder? Die zufälligen Fehler mit der grossen Varianz sorgen natürlich dafür, dass sie dennoch von den ‘generating parameters’ verschieden sind.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{m}\PYG{p}{]}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}  \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{without considering the interaction, the mse is: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{n}{y\PYGZus{}hat}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{x} \PYG{o}{*} \PYG{n}{m}\PYG{p}{]}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}  \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{considering the interaction, the mse drops to: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{n}{y\PYGZus{}hat}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{the coefficients are given by }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{; compare these values}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ to the values }\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
     \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{we used for generating the data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
without considering the interaction, the mse is: 21.027778912546122
considering the interaction, the mse drops to: 15.46963918621246

the coefficients are given by [ 2.30055754 \PYGZhy{}1.20686064 \PYGZhy{}7.89087321]; compare these values
 to the values we used for generating the data
\end{sphinxVerbatim}


\section{some considerations}
\label{\detokenize{Regression_Techniques:some-considerations}}
\sphinxAtStartPar
Die Überlegung hier veranschaulicht, dass es schon bei moderater Variablen\sphinxhyphen{}Anzahl sehr viele mögliche Interaktions\sphinxhyphen{}Terme gibt. Für die normale Lineare Regression würde die grosse Anzahl dieser Terme zum Verhängnis werden, weil dann wieder der Fall eintreten könnte indem wir die Daten overfitten oder gar mehr Variablen als Beobachtunge zur Verfügung stehen. Auch in diesem Fall kann auf die vorgestellten\textasciicircum{} Regularisierungs\sphinxhyphen{}Verfahren (ElasticNet, Ridge und Lasso) zurückgegriffen werden:

\sphinxAtStartPar
Nehmen wir an, wir haben ein data\sphinxhyphen{}set mit 70 verschiedenen Variablen. Weil wir nichts über die Beziehungen der Variablen zur abhängigen Variable \(y\) noch über die Beziehungen der Variablen untereinander wissen, sind wir geneigt eine Menge zusätzlicher ‘features’ für unser Modell zu erzeugen:
\begin{itemize}
\item {} 
\sphinxAtStartPar
wir können 70 quadratische Terme hinzufügen (\(x_j^2\))

\item {} 
\sphinxAtStartPar
wir können 70 kubische Terme aufnehmen (\(x_j^3\))

\item {} 
\sphinxAtStartPar
wir können auch \(\binom{70}{2} = 2415\) Interaktionen erster Ordnung zwischen den 70 Variablen annehmen

\item {} 
\sphinxAtStartPar
anstatt dessen könnte wir auch die Interaktions\sphinxhyphen{}Terme der 210 (70 Variablen + 70 quadratische Terme + 70 kubische Terme) Variablen mit aufnhemne: \(\binom{210}{2} = 21945\)

\item {} 
\sphinxAtStartPar
neben quadratisch und kubischen Termen gibt es auch viele andere linearisierende Transformation, die unter Umständen zu besseren ergebnissen führen wie beispielsweise die log\sphinxhyphen{}Transformation. Im praktischen Beipiel des Bosten house\sphinxhyphen{}prices data\sphinxhyphen{}Sets werden wir die \sphinxcode{\sphinxupquote{box\sphinxhyphen{}cox\sphinxhyphen{}Transformation}} kennen lernen.

\end{itemize}

\sphinxAtStartPar
Wie wir gesehen haben, kann die Anzahl möglicher Variablen sehr schnell wachsen, wenn man alle Effekte berücksichtigt, die ausschlaggebend sein könnten. Manchmal existieren sogar Interaktionseffekte zweiter Ordnund, d.h. drei Variablen sind dann daran beteiligt. 
Würden wir alle möglichen Variablen berücksichtigen, die sich derart bilden lassen, dann würde dies auch bei grossen Daten\sphinxhyphen{}Sets zu ausgeprägten ‘overfitting’ führen. \sphinxstylestrong{Aus diesem Grund wurden die regularization techniques wie das  ElasticNet und seine Komponenten, die Ridge Regression und die Lasso Regression eingeführt}.


\chapter{Wie zuversichtlich sind wir hinsichtlich unserer Modell\sphinxhyphen{}Vorhersagen}
\label{\detokenize{Regression_Techniques:wie-zuversichtlich-sind-wir-hinsichtlich-unserer-modell-vorhersagen}}
\sphinxAtStartPar
Selten werden wir mit unserem Modell genau die Koeffizienten schätzen können, die in der gesamten Population (alle Daten, die wir erheben könnten) anzutreffen sind. Viel öfter ist unsere Stichprobe nicht repräsentativ für die gesamte Population oder sie ist schlicht zu klein und zufällige, normalverteilte Fehler in unseren Daten beeinflussen die Schätzung der Koeffizienten. Dies umsomehr, desto mehr Variablen wir in user Modell aufnehmen.
Wie können wir nun die Güte unserer Schätzung beurteilen? Hier sind mindestens zwei verschieden Fragen denkbar:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Wie sicher sind wir mit Hinblick auf die geschätzen Koeffizienten \(\mathbf{b}\)?. Diese Frage ist besonders für Wissenschaftler wichtig, da die Antwort dafür ausschlaggebend ist, ob eine Hypothese beibehalten oder verworfen werden muss.

\item {} 
\sphinxAtStartPar
Wie sicher sind wir uns bezüglich einzelner Vorhersagen. Dies spielt die grösste Rolle im Machine Learning Umfeld, da wir das trainierte Modell gerne in unsere Business\sphinxhyphen{}Abläufe integrieren würden.

\end{itemize}

\sphinxAtStartPar
Diese beiden Fragestellungen lassen sich mit Hinblick auf die Regression auch wie folgt formulieren:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Wie sehr ist die ‘mean response’, unsere Regressions\sphinxhyphen{}Funktion von der Stichprobe abhängig. Variiert Erstere sehr stark und umfasst unter Umständen sogar den Wert \(0\), dann können diese Effekte (Koeffizienten) nicht interpretiert werden.

\item {} 
\sphinxAtStartPar
Wie sehr können Beobachtungen \(y\) für eine gegebene Kombination von Variablen\sphinxhyphen{}Werten in \(\mathbf{X}\) variieren? Ist diese Variation sehr gross, so werden wir auch grosse Fehler in unseren Business\sphinxhyphen{}Process einbauen

\end{itemize}


\section{Recap of assumptions underlying regression}
\label{\detokenize{Regression_Techniques:recap-of-assumptions-underlying-regression}}
\sphinxAtStartPar
Dies sind Linearität (der Zusammenhang einer Variablen und der abhängigen Variablen ist linear, d.h. der selbe Steigungsparamter gilt für alle Bereiche der Variablen), Homoskedastizität (die Fehler der Regression – die Residuen – sind in allen Bereichen von X normal verteilt mit gleicher Varianz) und Normalität der Residuen bei gegebenem Wert von X.
Diese Voraussetzungen sind in vielen Fällen nicht erfüllt und auch bekannterweise verletzt.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity}: Die Regression\sphinxhyphen{}Funktion ist eine gute Annäherung für die Beziehung zwischen \(\mathbf{X}\) and \(\mathbf{y}\), d.h. ist ein quadratischer Trend in den Daten und wir haben keine quadratischen Effekte in das Modell aufgenommen, so sind die Annahmen nicht erfüllt. Die Linearität besagt nämlich, dass für den Zusammenhang einer Variablen \(x\) und der abhängigen Variablen \(y\) der selbe Steigungs\sphinxhyphen{}Koeffizient \(b_x\) für all Bereich für \(x\) gelten muss. Ansonsten hat das Modell einen \sphinxstylestrong{bias}, es schätzt einen Koeffizienten systematisch falsch.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity}: Die Varianz unseres Vorhersagefehlers (Residuen) ist für alle Bereiche einer Variablen \(x\) identisch.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normality}: Die Werte der abhängigen Variablen \(\mathbf{y}\) sind für einen gegeben Wert von \(\mathbf{x}\) normal verteilt: \(\mathbf{y}|\mathbf{x} \sim N(\mu, \sigma)\)

\end{itemize}

\sphinxAtStartPar
In der nächsten Graphik werden die Voraussetzungen der linearen Regression veranschaulicht:
Image taken from \sphinxhref{https://janhove.github.io/analysis/2019/04/11/assumptions-relevance}{here}

\noindent\sphinxincludegraphics{{Regression_Techniques_56_0}.png}

\sphinxAtStartPar
Now, with respect to our confidence need:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Prediction interval}: The interval around our prediction, 95\% (97.5\%) of all observed values are supposed to fall in; This interval is symmetrical around the regression line. This fact follows from the assumptions discussed above. The standard error of prediction (or forecast) is given by: \$\(\hat{\sigma}_e = \sqrt{\frac{1}{N-(p+1)}\sum_i^N e_i^2},\)\(
with \)p\( being the number of parameters (the term \)+1\( is for the intercept); \)e\_i\( are the residuals, i.e., the differences between the observed data points \)y\_i\( and the prediction \)\textbackslash{}hat\{y\}\sphinxstyleemphasis{i\(. The confidence interval is given by:
\)\( CI_i = y_i \pm t_{1-\alpha/2, N-p} \cdot \hat{\sigma}_e.\)\(
Here, \)t}\{1\sphinxhyphen{}\textbackslash{}alpha/2, N\sphinxhyphen{}p\}\( is the value of the student-t-distribution for a confidence level of \)1\sphinxhyphen{}\textbackslash{}alpha/2\( and \)N\sphinxhyphen{}p\$ degrees of freedom.

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Confidence interval}: In a similar manner (a bit more involved) we could derive the confidence interval for the predicted mean \(\hat{y}_i\). Remember, that data is supposed to be normally distributed. The regression line we fit, is an estimate of the mean for a given configuration \(\mathbf{x}_i\). Of course, we do not fit the empirical values exactly; some may be lying above the regression line, some beneath. This confidence interval gives an upper and a lower bound for the mean estimate, i.e. the regression line. This confidence interval is not equidistant from the regression line for all values of \(\mathbf{x}\). In the regions where data is sparse, the regression line can not be estimated with high confidence. In contrast, near the mean of \(\mathbf{x}\) the estimate is supposed to be more accurate (normaly distributed \(\mathbf{x}\) assumed).

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{CI for regression coefficients}: Again, the derivation of the formulae for this CI is more involved than this for the prediction interval. This interval gives the upper and lower boundary for the coefficients \(\mathbf{b}\). These coefficients indicate how important the respectiv variable is in the regression equation. The interpretation of these coefficients is linked to \sphinxcode{\sphinxupquote{real}} science, where the epistemological caveat is the matter of interest. For example: “is closing schools and universities related to lower base reproduction numbers (\(R_0\))”. This is typically not the kind of questions a data scientist is trying to answer ;\sphinxhyphen{})

\end{enumerate}

\sphinxAtStartPar
In the following code examples, first, we display the classical summary statistics. In the middle of the printed output, you can find the confidence intervals for the regression coefficients ‘const’ (intercept) and \(x_1\), the \(b_1\) coefficient. The plots illustrate te he points 1 and 2.

\sphinxAtStartPar
If someone has a strong interest in these more statistical models, I can recommend this \sphinxhref{http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE\_Book/3-7-UnivarPredict.html}{source}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}

\PYG{c+c1}{\PYGZsh{} data example}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the x (small x) is just for plotting purpose}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x\PYGZus{}intercept} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{p}{]}


\PYG{n}{X\PYGZus{}intercept} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{p}{]}

\PYG{n}{ols\PYGZus{}result\PYGZus{}lin} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{X\PYGZus{}intercept}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat\PYGZus{}lin} \PYG{o}{=} \PYG{n}{ols\PYGZus{}result\PYGZus{}lin}\PYG{o}{.}\PYG{n}{get\PYGZus{}prediction}\PYG{p}{(}\PYG{n}{x\PYGZus{}intercept}\PYG{p}{)}



\PYG{n}{dt\PYGZus{}lin} \PYG{o}{=} \PYG{n}{y\PYGZus{}hat\PYGZus{}lin}\PYG{o}{.}\PYG{n}{summary\PYGZus{}frame}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}lin} \PYG{o}{=} \PYG{n}{dt\PYGZus{}lin}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{meanCIs\PYGZus{}lin} \PYG{o}{=} \PYG{n}{dt\PYGZus{}lin}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}ci\PYGZus{}lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}ci\PYGZus{}upper}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{obsCIs\PYGZus{}lin} \PYG{o}{=} \PYG{n}{dt\PYGZus{}lin}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{obs\PYGZus{}ci\PYGZus{}lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{obs\PYGZus{}ci\PYGZus{}upper}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R\PYGZhy{}squared:                       0.584
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.538
Method:                 Least Squares   F\PYGZhy{}statistic:                     12.64
Date:                Wed, 14 Jul 2021   Prob (F\PYGZhy{}statistic):            0.00616
Time:                        22:43:16   Log\PYGZhy{}Likelihood:                \PYGZhy{}49.385
No. Observations:                  11   AIC:                             102.8
Df Residuals:                       9   BIC:                             103.6
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const         \PYGZhy{}8.4903     15.410     \PYGZhy{}0.551      0.595     \PYGZhy{}43.351      26.370
x1             8.0791      2.272      3.556      0.006       2.939      13.219
==============================================================================
Omnibus:                        4.018   Durbin\PYGZhy{}Watson:                   1.670
Prob(Omnibus):                  0.134   Jarque\PYGZhy{}Bera (JB):                1.165
Skew:                           0.156   Prob(JB):                        0.559
Kurtosis:                       1.437   Cond. No.                         14.8
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 115.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_63_1}.png}

\sphinxAtStartPar
The same plot is derived for an equation including a quadratic term:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}intercept\PYGZus{}quad} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X\PYGZus{}intercept}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} for plotting:}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x\PYGZus{}intercept\PYGZus{}quad} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}

\PYG{n}{ols\PYGZus{}result\PYGZus{}quad} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{X\PYGZus{}intercept\PYGZus{}quad}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}


\PYG{n}{y\PYGZus{}hat\PYGZus{}quad} \PYG{o}{=} \PYG{n}{ols\PYGZus{}result\PYGZus{}quad}\PYG{o}{.}\PYG{n}{get\PYGZus{}prediction}\PYG{p}{(}\PYG{n}{x\PYGZus{}intercept\PYGZus{}quad}\PYG{p}{)}
\PYG{n}{dt\PYGZus{}quad} \PYG{o}{=} \PYG{n}{y\PYGZus{}hat\PYGZus{}quad}\PYG{o}{.}\PYG{n}{summary\PYGZus{}frame}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}quad} \PYG{o}{=} \PYG{n}{dt\PYGZus{}quad}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{meanCIs\PYGZus{}quad} \PYG{o}{=} \PYG{n}{dt\PYGZus{}quad}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}ci\PYGZus{}lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean\PYGZus{}ci\PYGZus{}upper}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{obsCIs\PYGZus{}quad} \PYG{o}{=} \PYG{n}{dt\PYGZus{}quad}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{obs\PYGZus{}ci\PYGZus{}lower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{obs\PYGZus{}ci\PYGZus{}upper}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ols\PYGZus{}result\PYGZus{}quad}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R\PYGZhy{}squared:                       0.783
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.728
Method:                 Least Squares   F\PYGZhy{}statistic:                     14.39
Date:                Wed, 14 Jul 2021   Prob (F\PYGZhy{}statistic):            0.00224
Time:                        22:43:17   Log\PYGZhy{}Likelihood:                \PYGZhy{}45.820
No. Observations:                  11   AIC:                             97.64
Df Residuals:                       8   BIC:                             98.83
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const         35.3379     20.074      1.760      0.116     \PYGZhy{}10.952      81.628
x1           \PYGZhy{}12.1493      7.688     \PYGZhy{}1.580      0.153     \PYGZhy{}29.879       5.580
x2             1.6857      0.624      2.701      0.027       0.247       3.125
==============================================================================
Omnibus:                        9.915   Durbin\PYGZhy{}Watson:                   2.828
Prob(Omnibus):                  0.007   Jarque\PYGZhy{}Bera (JB):                4.642
Skew:                           1.313   Prob(JB):                       0.0982
Kurtosis:                       4.798   Cond. No.                         234.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 115.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_67_1}.png}


\section{Bootstrap}
\label{\detokenize{Regression_Techniques:bootstrap}}
\sphinxAtStartPar
With real, messy data it is rather seldom to meet all the assumptions underlying the theory of confidence intervals. A robust alternative, without any assumptions is the bootstrap. We view our data sample as the population and draw samples from it, with replacement. We fit the model to each of these samples and gather the statistics of relevance. Then we report the 2.5\% quantile and the 97.5\% quantile as the boundaries of our confidence interval with confidence level of \(\alpha=5\%\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{choices}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{Lasso}
\PYG{k+kn}{import} \PYG{n+nn}{warnings}
\PYG{n}{warnings}\PYG{o}{.}\PYG{n}{filterwarnings}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}y.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{load}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/home/martin/python/fhnw\PYGZus{}lecture/scripts/regression\PYGZus{}X.pickle.npy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{}X = np.c\PYGZus{}[np.ones(X.shape[0]), X, X**2, X**3, X**4]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{]}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}x = np.c\PYGZus{}[np.ones(x.shape[0]), x, x**2, x**3, x**4]}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{x}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{4}\PYG{p}{]}
\PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{drew} \PYG{o}{=} \PYG{n}{choices}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{sampler} \PYG{o}{=} \PYG{p}{(}\PYG{n}{choices}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{,} \PYG{n}{k} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{indices}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{CIS} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Lasso}\PYG{p}{(}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{fit\PYGZus{}intercept}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{drew}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{p}{[}\PYG{n}{drew}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYGZbs{}
                              \PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
                              \PYG{k}{for} \PYG{n}{drew} \PYG{o+ow}{in} \PYG{n}{sampler}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} x is 220 long}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{Lasso}\PYG{p}{(}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{fit\PYGZus{}intercept}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{y\PYGZus{}hat} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7f305c0a5990\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_70_1}.png}


\chapter{Extension: logistic regression and the GLM}
\label{\detokenize{Regression_Techniques:extension-logistic-regression-and-the-glm}}
\sphinxAtStartPar
There are other models that are relatives of the linear model that we discussed in this notebook. One of the most prominent is the \sphinxstylestrong{logistic regression}. This model belongs to the “\sphinxstylestrong{generalized} linear model” (GLM). The GLM may not be confounded with the “\sphinxstylestrong{general} linear model”. The latter essentially expresses analysis of variance (ANOVA) in terms of linear regression.
The \sphinxstylestrong{GLM} extends the
linear regression beyond models with normal error distributions. This
remark in the corresponding wiki\sphinxhyphen{}article is enlightening:
\sphinxhref{https://en.wikipedia.org/wiki/Generalized\_linear\_model\#Confusion\_with\_general\_linear\_models}{read wikipedia for this}


\section{exponential family of distributions}
\label{\detokenize{Regression_Techniques:exponential-family-of-distributions}}
\sphinxAtStartPar
From the perspective of modern statistics the GLM comprises many
different linear models, among others the classical linear model. Every
distribution in the exponential family can be written in the following
form:
\$\(f(y| \theta) = \exp\left(\frac{y \theta + b(\theta)}{\Phi} + c(y, \Phi)\right),\)\(
where \)\textbackslash{}theta\( is called the canonical parameter that in turn is a
function of \)\textbackslash{}mu\(, the mean. This function is called the canonical link
function that links \)\textbackslash{}mu\( to a linear function of the regression
parameters. In short: it is this function that linearizes the relation
between the dependent and the independent variables. For the sake of
completeness: \)b(\textbackslash{}theta)\( is a function of the canonical parameter and
hence, also depends on \)\textbackslash{}mu\(. \)\textbackslash{}Phi\( is called the dispersion parameter
and \)c(y, \textbackslash{}Phi)\$ is a function depending on the observation and the
dispersion parameter.


\subsection{Normal distribution}
\label{\detokenize{Regression_Techniques:normal-distribution}}\begin{eqnarray*}
f(y| \mu, \sigma) =& (2\pi \sigma^2)^{-\frac{1}{2}} \exp\left(-\frac{1}{2}\frac{y^2 -2y\mu + \mu^2}{\sigma^2}\right) \\
 =&\quad \exp \left(\frac{y\mu -\frac{\mu^2}{2}}{\sigma^2} - \frac{1}{2}\left(\frac{y^2}{\sigma^2} + \log(2\pi\sigma^2\right)\right),\quad \text{with}
\end{eqnarray*}
\sphinxAtStartPar
\(\mu = \theta(\mu)\), i.e. \(\mu\) is the canonical parameter and the link
function is given by the identity function. Hence, the mean can be
modeled directly without any transformation. The dispersion parameter \(\Phi\) is given by \(\sigma^2\), the variance. This case is the classical
linear regression.


\subsection{Poisson distribution}
\label{\detokenize{Regression_Techniques:poisson-distribution}}
\sphinxAtStartPar
Now, for the Poisson distribution we have
\begin{eqnarray*}
f(y| \mu) =& \frac{\mu^{y} e^{-\mu}}{y!} = \mu^y e^{-\mu}\frac{1}{y!}\\
=& \quad\exp\left(y \log(\mu) - \mu - \log(y!)\right), \quad\text{where}
\end{eqnarray*}
\sphinxAtStartPar
the link function is given by \(\log(\mu)\). Note that the Poisson
distribution does not have any dispersion parameter.


\subsection{Bernoulli distribution \protect\(\Rightarrow\protect\) logistic regression}
\label{\detokenize{Regression_Techniques:bernoulli-distribution-rightarrow-logistic-regression}}
\sphinxAtStartPar
And finally the Bernoulli distribution from which we derive the logistic
regression. Using the Bernoulli distribution, we can calculate the probabilities of experiments consisting of binary events. The classical example is coin flipping. Here, \(\pi\) is the probability of the coin showing ‘head’; \((1-\pi)\) is the probability of the coin showing ‘tail’. We can now calculate the probability of getting exactly 7 times head for 10 tosses with a fair coin:
\$\(\pi^7 (1-\pi)^3 = 0.5^7 0.5^3 = 0.5^{10} = 0.0009765625\)\$

\sphinxAtStartPar
Next, I demonstrate how we can rewrite the Bernoulli distribution to fit into the framework of the exponential family:
\begin{eqnarray*}
f(y |\pi) =& \pi^y (1-\pi)^{1-y} = \exp\left(y \log(\pi) + (1-y) \log(1-\pi)\right)\\
= & \quad \exp\left(y \log(\pi) + \log(1-\pi) - y\log(1-\pi)\right)\\
=&\quad \exp\left(y\log(\frac{\pi}{1-\pi}) + \log(1-\pi)\right),\quad\text{where}
\end{eqnarray*}
\sphinxAtStartPar
the link function evaluates to \(\log(\frac{\pi}{1-\pi})\). This function
is also called the logit function whose reverse function is the logistic
function. Hence, it is the logit that is modeled by a lineare function
of the regressors:
\(\log(\frac{\pi}{1-\pi}) = a + b_{1}x_1 + \ldots + b_jx_j\). If we plug
the right hand term into the logistic function we get the estimated
probabilities:
\$\(P(y=1 |x) = \frac{\exp(a + b_{1}x_1 + \ldots + b_jx_j)}{1 + \exp(a + b_{1}x_1 + \ldots + b_jx_j)}.\)\$

\sphinxAtStartPar
Here, I showed that the classical linear regression with normal
error terms can be seen as a special case of a much wider family of
models comprising all distributions out of the exponential family. (For
a more complete treatment of other distributions see again
https://en.wikipedia.org/wiki/Generalized\_linear\_model.)


\chapter{GLMNET}
\label{\detokenize{Regression_Techniques:glmnet}}
\sphinxAtStartPar
In the statistical language R, there exists a library called ‘glmnet’. This package implements the elastic net as we discussed here but for the glm and not only for the classical linear regession.
https://web.stanford.edu/\textasciitilde{}hastie/glmnet/glmnet\_alpha.html

\sphinxAtStartPar
There exists also a python package implementing glmnet by using the exact same fortran code: \sphinxstylestrong{glmnet\sphinxhyphen{}python}.
There are some subtleties in the implementation that are different from the elastic net version as provided by sklearn.
https://pypi.org/project/glmnet\sphinxhyphen{}python/


\chapter{Neural Network}
\label{\detokenize{Regression_Techniques:neural-network}}
\sphinxAtStartPar
We can also cast linear regression into a neural network context. The network has no hidden layer. The activation function in the output neuron is either the identity function \(y=x\) for classical linear regression or the logistic function for logistic regression.

\noindent\sphinxincludegraphics{{Regression_Techniques_79_0}.png}


\section{classical linear regression}
\label{\detokenize{Regression_Techniques:classical-linear-regression}}


\sphinxAtStartPar
Remember, we included the intercept \(\alpha\) into the vector \(\mathbf{\beta}\) by including an all\sphinxhyphen{}ones vector into the matrix \(\mathbf{X}\). The equation is hence written:
\$\(\mathbf{y} = \mathbf{X} \mathbf{\beta}\)\(
In neurall network context, the vector \)\textbackslash{}mathbf\{\textbackslash{}beta\}\( is called the network weights and often is denotedn as \)\textbackslash{}mathbf\{W\}\$.

\sphinxAtStartPar
Why are the weight\sphinxhyphen{}vectors \(\mathbf{W}\) in upper\sphinxhyphen{}case?In a regression\sphinxhyphen{}context, we usually use lower\sphinxhyphen{}case letters like \(\mathbf{\beta}\) or \(\mathbf{b}\)?

\noindent\sphinxincludegraphics{{Regression_Techniques_82_0}.png}


\section{logistic regression}
\label{\detokenize{Regression_Techniques:logistic-regression}}


\sphinxAtStartPar
For logistic regression, the activation function is changed. Now, it is not the identity function, but the logistic function:
\$\(P(y=1 |x) = \frac{\exp(a + b_{1}x_1 + \ldots + b_jx_j)}{1 + \exp(a + b_{1}x_1 + \ldots + b_jx_j)}\)\$
This function approaches 0, 1 asymptotically.


\subsection{Weight decay}
\label{\detokenize{Regression_Techniques:weight-decay}}
\sphinxAtStartPar
In the neural network literature, the \(l_2\)\sphinxhyphen{}penalty term is called “weight decay”. It is not a parameter of the single layers or neurons, but of the optimizer. As with regularized regression, the weight decay is written:
\$\(L' = L + \lambda\sum_i w_i^2, \)\(
where \)L\( is the actual loss and \)w\_i\$ are the weights of the incoming connections of a neuron.







\renewcommand{\indexname}{Index}
\printindex
\end{document}