{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box cox transform\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data, descriptions of the variables and some examples can be found here:\n",
    "-> link to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:195: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "/home/martin/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py:1985: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/home/martin/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:910: RuntimeWarning: divide by zero encountered in log\n",
      "  return (lmb - 1) * np.sum(logdata, axis=0) - N/2 * np.log(variance)\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "train.drop('Id', axis = 1, inplace = True)\n",
    "test.drop('Id', axis = 1, inplace = True)\n",
    "\n",
    "SalePrice = train['SalePrice']\n",
    "train.drop('SalePrice', axis=1, inplace = True)\n",
    "\n",
    "data = pd.concat((train, test))\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# categorical and numericalvariables:\n",
    "categorical = [var for var in data.columns if data[var].dtype=='O']\n",
    "numerical = [var for var in data.columns if data[var].dtype!='O']\n",
    "\n",
    "# missing values:\n",
    "# for categorical data, missing values often is the absence of a feature\n",
    "# categorical data has no metric, so in a later step each level will be a variable on its own\n",
    "# with ones where the respective category and zeros otherwise. This is called one-hot encoding\n",
    "# or dummy-coding\n",
    "data[categorical] = data[categorical].fillna('None')\n",
    "\n",
    "## next, we substitute missing values with the mean of the variable and form new variables\n",
    "## indicating the missing values. Sometimes data is not missing at random and the fact that\n",
    "## data is missing might contain valuable information\n",
    "variables_na = []\n",
    "for val in numerical:\n",
    "    data[val + '_na'] = pd.isnull(data[val])\n",
    "    variables_na.append(val + '_na')\n",
    "    data[val].fillna(data[val].mean(), inplace = True)\n",
    "\n",
    "## box-cox transform is variance stabilizing. It is meant to make \n",
    "## the variable more normaly distributed    \n",
    "box_cox = []\n",
    "for val in numerical:\n",
    "    new_vals, lamb = boxcox(data[val] + 1)\n",
    "    if np.abs(lamb) < 8:\n",
    "        data[val + '_box_cox'] = new_vals\n",
    "        box_cox.append(val)\n",
    "\n",
    "# as already announced, categorical data is one-hot encoded (dummy-coded)        \n",
    "data_base = pd.get_dummies(data[[col for col in data.columns if col not in variables_na]])\n",
    "data_na = pd.get_dummies(data[variables_na])\n",
    "\n",
    "# we have to cast every variable's data type to float32 for our next 'trick' \n",
    "data_base = data_base.astype(np.float32)\n",
    "data_na = data_na.astype(np.float32)\n",
    "\n",
    "data = pd.concat([data_base, data_na], axis = 1)\n",
    "# the number of variables is quiet high. We want to add interaction terms for the most important\n",
    "# variables. Therefore, we want to compute some variable-importance measure. This is\n",
    "# done by the help of gradient boosted trees:\n",
    "gbm = GradientBoostingRegressor(n_estimators = 32, max_depth = 4)\n",
    "gbm.fit(data[: len(train_ID)].values, SalePrice.values)\n",
    "\n",
    "# we sort the variables (indizes) by variable importance\n",
    "indizes = np.argsort(gbm.feature_importances_)\n",
    "# import a tool for getting all possible n over 2 combinations of these variables\n",
    "from itertools import combinations\n",
    "# and add the interactions\n",
    "interactions = []\n",
    "for comb in list(combinations(data.columns[indizes[-55:]], 2)):\n",
    "    data[comb[0] + '_x_' + comb[1]] = data[comb[0]] * data[comb[1]]\n",
    "    interactions.append(comb[0] + '_x_' + comb[1])\n",
    "\n",
    "data_interactions = data[interactions]\n",
    "\n",
    "## 1.\n",
    "# now, we have different data sets\n",
    "# the base set with missing values imputed by the mean and no other feature engineering\n",
    "# box-cox transformed variables are removen\n",
    "base = data_base[[col for col in data_base.columns if not col.endswith('_box_cox')]]\n",
    "## 2.\n",
    "# box_cox is admitted; original variables removed\n",
    "with_box_cox = data_base[[col for col in data_base.columns if not col in box_cox]]\n",
    "## 3.\n",
    "# variables indicating formerly missing values are included\n",
    "with_na = pd.concat([with_box_cox, data_na], axis = 1)\n",
    "## 4.\n",
    "# all interaction terms of the 55 most important variables are added\n",
    "with_interactions = pd.concat([with_na, data_interactions], axis = 1)\n",
    "\n",
    "## the target variable is log-transformed\n",
    "y = np.log1p(SalePrice)\n",
    "\n",
    "## since we want to try elasticnet, we have to find the optimal parameter for \n",
    "# lambda (amount of regularization) and for alpha (ratio of lasso and ridge mixing)\n",
    "lamb = 10**(np.linspace(-1, 0.2, 15))\n",
    "# ratio\n",
    "ratio = np.linspace(0, 1, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least error is: 0.13773722408683256, best parameters are: [(0.3981071705534972, 0.0)]\n",
      "least error is: 0.1305111256473241, best parameters are: [(0.1, 0.1111111111111111)]\n",
      "least error is: 0.1304897562810484, best parameters are: [(0.2682695795279726, 0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1546: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least error is: 0.11982155965574919, best parameters are: [(0.4849693428528198, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "best_parameters = []\n",
    "# we iterate over list of data-sets\n",
    "for d in [base, with_box_cox, with_na, with_interactions]:\n",
    "    # scale variables\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(d) #  fit  the scale        \n",
    "\n",
    "    X_train = scaler.transform(d[:len(train_ID)])\n",
    "    \n",
    "    # the function cross_val_score computes the model passed to it for cv=5-fold \n",
    "    # cross validation; we compute the mean over the 5 folds\n",
    "    get_results = [(l, r, np.mean(np.sqrt(-cross_val_score(ElasticNet(alpha = l,\n",
    "                                                            l1_ratio = r),\n",
    "            X_train, y , scoring = 'neg_mean_squared_error',\n",
    "            cv = 5, n_jobs = -1))))\n",
    "                for l in lamb for r in ratio]\n",
    "    \n",
    "    # the least error is extracted\n",
    "    least_error = np.min([i[2] for i in get_results])\n",
    "    error.append(least_error)\n",
    "    # the parameters belonging to the best result\n",
    "    parameters = [i[0:2] for i in get_results if i[2] == least_error]\n",
    "    best_parameters.append(parameters)\n",
    "    print(f'least error is: {least_error}, best parameters are: {parameters}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.3981071705534972, 0.0)],\n",
       " [(0.1, 0.1111111111111111)],\n",
       " [(0.2682695795279726, 0.0)],\n",
       " [(0.4849693428528198, 0.0)]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude:\n",
    "1. The error for the base data set (only missing values imputed) is: {{np.round(error[0], 4)}}  (mse); The corresponding lambda is {{np.round(best_parameters[0][0][0], 4)}}, i.e. the amount of regularization; the l1_ratio = {{np.round(best_parameters[0][0][1], 4)}}; the kind of regularization was pure ridge (l2-penalty)\n",
    "2. The error with some of the numeric variables box-cox transformed is {{np.round(error[1], 4)}} (mse); the amount of regularization is far less than before ({{np.round(best_parameters[1][0][0], 4)}}); we have {{np.round(best_parameters[1][0][1] * 100)}}% l1-penalty and {{np.round(100 - (best_parameters[1][0][1] * 100))}}% l2-penalty\n",
    "3. Indicator variables for formerly missing values are included in the data-set; The error ({{np.round(error[2], 4)}}) shrinks by an insignificant amount. The lambda parameter is {{np.round(best_parameters[2][0][0], 4)}}; no l1-penalty is used\n",
    "4. adding the interaction terms has the most pronounced effect. The error drops to {{np.round(error[3], 4)}}; The best parameters are as before.\n",
    "\n",
    "One additional note: By including the interaction terms, we have __more variables (1831) than observations (1460)__ in the training set. This situation is not admissable in classical statistics. For machine learning algorithms with regularization, it does not mean any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have found the best parameters by cross-validation. Now, we try to solve a business problem with these results:<br>\n",
    "__The sales team needs all houses from the test set with estimated prices higher than 350'000\\$. Can you please deliver an estimate about h\n",
    "1. Compute the confidence intervals for the test-set.ow accurate your predictions are?__<br>\n",
    "We proceed as follows:\n",
    "2. Obtain estimates for the train-set by splitting the train-set in k=5 folds and always train on 4 folds and make predictions on the 5th fold. We obtain CIs in this manner.\n",
    "3. We take the lower-bounds of the confidence intervals. This ensures, that we do not include cases (houses) with very unstable estimates.\n",
    "4. Since we trained the CIs for the training-set with cross-validation, we can treat them as an estimate for the accuracy of the CIs of the test-set: This gives us an estimate of the error we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from random import choices\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "warnings.filterwarnings('ignore')\n",
    "# we get a lot of warnings:\n",
    "# LinAlgWarning: Ill-conditioned matrix (rcond=1.80167e-08): result may not be accurate.\n",
    "#  overwrite_a=False)\n",
    "# this is because we use more variables than observations and we get the already discussed\n",
    "# problems with matrix inversion\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(with_interactions) #  fit  the scale        \n",
    "\n",
    "\n",
    "X = scaler.transform(with_interactions[:len(train_ID)])\n",
    "test = scaler.transform(with_interactions[len(train_ID):])\n",
    "\n",
    "## 1.\n",
    "indices = np.arange(0, X.shape[0])\n",
    "# draw 200 samples with replacement from training data set\n",
    "sampler = (choices(indices, k = len(indices)) for i in range(200))\n",
    "# fit 200 models to the samples drawn and predict on test-set\n",
    "# \n",
    "CIS_test = np.percentile(\n",
    "            np.array(\n",
    "                [\n",
    "                 Ridge(alpha=best_parameters[-1][0][0], fit_intercept=True)\\\n",
    "                 .fit(X[drew,:], y.values[drew]).predict(test).tolist()\n",
    "                 for drew in sampler]\n",
    "                 ), [2.5, 97.5], axis = 0)\n",
    "\n",
    "\n",
    "## 2.\n",
    "kf = KFold(n_splits = 5, shuffle=True)\n",
    "CIS = np.empty((2, X.shape[0]))\n",
    "y_hat = np.empty((y.shape[0],))\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    indices = np.arange(0, X_train.shape[0])\n",
    "\n",
    "    sampler = (choices(indices, k = len(indices)) for i in range(200))\n",
    "    CIS[:, test_index] = np.percentile(\n",
    "        np.array(\n",
    "            [\n",
    "             Ridge(alpha=best_parameters[-1][0][0], fit_intercept=True)\\\n",
    "             .fit(X_train[drew,:], y_train.values[drew])\\\n",
    "                              .predict(X_test).tolist()\n",
    "             for drew in sampler]\n",
    "             ), [2.5, 97.5], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 416560.12710907526),\n",
       " (200, 390926.2441803477),\n",
       " (201, 357200.65165663866),\n",
       " (202, 350056.35784401966),\n",
       " (203, 444586.5422132871),\n",
       " (211, 367257.12035243073),\n",
       " (507, 355180.4511882404),\n",
       " (510, 376426.0282781632),\n",
       " (514, 431917.6201557001),\n",
       " (829, 396837.5338146626),\n",
       " (831, 391321.16178918397),\n",
       " (832, 430358.804773981),\n",
       " (834, 451524.5175652788),\n",
       " (871, 354472.4001314549),\n",
       " (879, 363590.88408265),\n",
       " (1167, 368303.80340578983),\n",
       " (1168, 378273.3607423235),\n",
       " (1170, 393803.3854470842),\n",
       " (1191, 387209.9676127946),\n",
       " (1194, 350179.24043305503),\n",
       " (1220, 360119.8795763919),\n",
       " (1222, 387306.76570398925)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.\n",
    "bool_indizes = np.exp(CIS_test[0, :]) + 1 > 350000\n",
    "sum(bool_indizes)\n",
    "for_sales_departement = list(\n",
    "    zip(np.arange(0, CIS_test.shape[1])[bool_indizes], np.exp(CIS_test[0, bool_indizes])+1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we estimate that 93.33% of our predictions are correct\n",
      "\n",
      "however, we only got 26.79% of all houses with prices higher than 350000\n"
     ]
    }
   ],
   "source": [
    "y_hat_lower = np.exp(CIS[0,:])+1\n",
    "estimates = y_hat_lower > 350000\n",
    "true = (np.exp(y) +1) > 350000\n",
    "y_hat_lower[estimates]\n",
    "print(f'we estimate that {np.round(np.mean(true[estimates]) * 100, 2)}% of our predictions are correct')\n",
    "print(f'\\nhowever, we only got {np.round(sum(estimates)/sum(true)*100, 2)}% of all houses with prices higher than 350000')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we hope that {{a}} will render"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
