{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box cox transform\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data, descriptions of the variables and some examples can be found here:\n",
    "[house-pricese-from-kaggle-competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data is from a kaggle-competition\n",
    "Attention, the test data-set has no target; This is the part you are supposed to upload to kaggle for estimation of your modelling performance;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', sep=\",\")\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "train.drop('Id', axis = 1, inplace = True)\n",
    "test.drop('Id', axis = 1, inplace = True)\n",
    "\n",
    "SalePrice = train['SalePrice']\n",
    "train.drop('SalePrice', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now, we want data-leakage\n",
    "We discussed at length, that train and test-set should be standardized and preprocessed independently; This is only true if we want a fair estimate about our algorithm's performance with new, unseen data.<br>\n",
    "In the current case we want our training-procedure to be biased towards the test-set, because the test-set performance is what counts in kaggle-competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat((train, test))\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "# categorical and numericalvariables:\n",
    "\n",
    "categorical = [var for var in train.columns if train[var].dtype=='O']\n",
    "numerical = [var for var in train.columns if train[var].dtype!='O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some preprocessing steps\n",
    "### we fill missing values with the mean and add an extra variable indicating the missing position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[categorical] = data[categorical].fillna('None')\n",
    "\n",
    "\n",
    "## next, we substitute missing values with the mean of the variable and form new variables\n",
    "## indicating the missing values. Sometimes data is not missing at random and the fact that\n",
    "## data is missing might contain valuable information\n",
    "variables_na = []\n",
    "for val in numerical:\n",
    "    data[val + '_na'] = pd.isnull(data[val])\n",
    "    variables_na.append(val + '_na')\n",
    "    data[val].fillna(data[val].mean(), inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we transform the continuous variables to be more normally distributed\n",
    "[box-cox transform in short](https://www.statisticshowto.com/box-cox-transformation/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## box-cox transform is variance stabilizing. It is meant to make \n",
    "## the variable more normaly distributed    \n",
    "box_cox = []\n",
    "for val in numerical:\n",
    "    new_vals, lamb = boxcox(data[val] + 1)\n",
    "    if np.abs(lamb) < 8:\n",
    "        data[val + '_box_cox'] = new_vals\n",
    "        box_cox.append(val)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we demonstrate the effect of the box-cox transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "var = 'LotArea'\n",
    "# 2 fitures\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "# non-transformed variable    \n",
    "ax.hist(data[var], bins = int(180/5),\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "# transformed_variable\n",
    "ax.hist(data[var + '_box_cox'], bins = int(180/5),\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we encode categorical data as dummy-variables (aka one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as already announced, categorical data is one-hot encoded (dummy-coded)  \n",
    "\n",
    "data_base = pd.get_dummies(data[[col for col in data.columns if col not in variables_na]])\n",
    "data_na = pd.get_dummies(data[variables_na])\n",
    "\n",
    "# we have to cast every variable's data type to float32 for our next 'trick' \n",
    "data_base = data_base.astype(np.float32)\n",
    "data_na = data_na.astype(np.float32)\n",
    "data_numerical = data[numerical]\n",
    "\n",
    "data = pd.concat([data_base, data_na], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we compute the feature importance in order to get the most relevant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of variables is quiet high. We want to add interaction terms for the most important\n",
    "# variables. Therefore, we want to compute some variable-importance measure. This is\n",
    "# done by the help of gradient boosted trees:\n",
    "gbm = GradientBoostingRegressor(n_estimators = 32, max_depth = 4)\n",
    "gbm.fit(data.iloc[0:len(train_ID)].values, SalePrice.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# we sort the variables (indizes) by variable importance\n",
    "indizes = np.argsort(gbm.feature_importances_)\n",
    "result = permutation_importance(gbm, data.iloc[0:len(train_ID)], SalePrice, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.boxplot(result.importances[sorted_idx[-15:]].T,\n",
    "            vert=False, labels=np.array(data.columns)[sorted_idx[-15:]])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for the most important features we compute the interactions\n",
    "### in the first step we include only the most important 15 variables to form interaction-terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a tool for getting all possible n over 2 combinations of these variables\n",
    "from itertools import combinations\n",
    "# and add the interactions\n",
    "interactions = []\n",
    "for comb in list(combinations(data.columns[sorted_idx[-15:]], 2)):\n",
    "    data[comb[0] + '_x_' + comb[1]] = data[comb[0]] * data[comb[1]]\n",
    "    interactions.append(comb[0] + '_x_' + comb[1])\n",
    "\n",
    "data_interactions = data[interactions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in the second step we include even 55 variables to form interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions2 = []\n",
    "for comb in list(combinations(data.columns[sorted_idx[-55:]], 2)):\n",
    "    data[comb[0] + '_x_' + comb[1]] = data[comb[0]] * data[comb[1]]\n",
    "    interactions2.append(comb[0] + '_x_' + comb[1])\n",
    "\n",
    "data_interactions2 = data[interactions2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.\n",
    "# now, we have different data sets\n",
    "# the base set with missing values imputed by the mean and no other feature engineering\n",
    "# box-cox transformed variables are removen\n",
    "base = data_base[[col for col in data_base.columns if not col.endswith('_box_cox')]]\n",
    "## 2.\n",
    "# box_cox is admitted; original variables removed\n",
    "with_box_cox = data_base[[col for col in data_base.columns if not col in box_cox]]\n",
    "## 3.\n",
    "# variables indicating formerly missing values are included\n",
    "with_na = pd.concat([with_box_cox, data_na], axis = 1)\n",
    "## 4.\n",
    "# all interaction terms of the 55 most important variables are added\n",
    "with_interactions = pd.concat([with_na, data_interactions], axis = 1)\n",
    "## 5.\n",
    "## we exagerate the number of interactions\n",
    "with_interactions2 = pd.concat([with_na, data_interactions2], axis=1)\n",
    "## the target variable is log-transformed\n",
    "y = np.log1p(SalePrice)\n",
    "\n",
    "## since we want to try elasticnet, we have to find the optimal parameter for \n",
    "# lambda (amount of regularization) and for alpha (ratio of lasso and ridge mixing)\n",
    "lamb = 10**(np.linspace(-1, 0.2, 15))\n",
    "# ratio\n",
    "ratio = np.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### excurs: parameter sampling on a logarithmic scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample some of the parameters on a logrithmic scale:\n",
    "https://www.coursera.org/lecture/deep-neural-network/using-an-appropriate-scale-to-pick-hyperparameters-3rdqN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.0001, 0.1, 5) # 5 equally spaced values from 0.0001 to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is no value between 0.0001 and 0.001 and no value between 0.001 and 0.01. But there are three values between 0.01 and 0.1<br>\n",
    "Now, compare to this solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**np.linspace(-4, -1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we transform the SalePrice (y) as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function np.log1p is well defined at 0\n",
    "display(np.log(0), np.log(1), np.log(2),\n",
    "        np.log1p(1), np.log1p(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'LotArea'\n",
    "# 2 fitures\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "# non-transformed variable    \n",
    "ax.hist(SalePrice, bins = int(180/5),\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "# transformed_variable\n",
    "ax.hist(np.log1p(SalePrice), bins = int(180/5),\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "best_parameters = []\n",
    "# we iterate over list of data-sets\n",
    "for d in [base, with_box_cox, with_na, with_interactions, with_interactions2]:\n",
    "    # scale variables\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(d[:len(train_ID)]) #  fit  the scale        \n",
    "\n",
    "    X_train = scaler.transform(d[:len(train_ID)])\n",
    "    X_test = scaler.transform(d[len(train_ID):])\n",
    "    \n",
    "    # the function cross_val_score computes the model passed to it for cv=5-fold \n",
    "    # cross validation; we compute the mean over the 5 folds\n",
    "    get_results = [(l, r, np.mean(np.sqrt(-cross_val_score(ElasticNet(alpha = l,\n",
    "                                                            l1_ratio = r),\n",
    "            X_train, y , scoring = 'neg_mean_squared_error',\n",
    "            cv = 5, n_jobs = -1))))\n",
    "                for l in lamb for r in ratio]\n",
    "    \n",
    "    # the least error is extracted\n",
    "    least_error = np.min([i[2] for i in get_results])\n",
    "    error.append(least_error)\n",
    "    # the parameters belonging to the best result\n",
    "    parameters = [i[0:2] for i in get_results if i[2] == least_error]\n",
    "    best_parameters.append(parameters)\n",
    "    print(f'least error is: {least_error}, best parameters are: {parameters}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(base.shape, with_box_cox.shape, with_na.shape, with_interactions.shape, with_interactions2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "np.round(100 - (best_parameters[1][0][1] * 100))": "89.0",
     "np.round(best_parameters[0][0][0], 4)": "0.3981",
     "np.round(best_parameters[0][0][1], 4)": "0.0",
     "np.round(best_parameters[1][0][0], 4)": "0.1",
     "np.round(best_parameters[1][0][1] * 100)": "11.0",
     "np.round(best_parameters[2][0][0], 4)": "0.1",
     "np.round(error[0], 4)": "0.1382",
     "np.round(error[1], 4)": "0.1307",
     "np.round(error[2], 4)": "0.1307",
     "np.round(error[3], 4)": "0.1282"
    }
   },
   "source": [
    "We conclude:\n",
    "1. The error for the base data set (only missing values imputed) is: {{np.round(error[0], 4)}}  (mse); The corresponding lambda is {{np.round(best_parameters[0][0][0], 4)}}, i.e. the amount of regularization; the l1_ratio = {{np.round(best_parameters[0][0][1], 4)}}; the kind of regularization was pure ridge (l2-penalty)\n",
    "2. The error with some of the numeric variables box-cox transformed is {{np.round(error[1], 4)}} (mse); the amount of regularization is far less than before ({{np.round(best_parameters[1][0][0], 4)}}); we have {{np.round(best_parameters[1][0][1] * 100)}}% l1-penalty and {{np.round(100 - (best_parameters[1][0][1] * 100))}}% l2-penalty\n",
    "3. Indicator variables for formerly missing values are included in the data-set; The error ({{np.round(error[2], 4)}}) shrinks by an insignificant amount. The lambda parameter is {{np.round(best_parameters[2][0][0], 4)}}; no l1-penalty is used\n",
    "4. adding the interaction terms has the most pronounced effect. The error drops to {{np.round(error[3], 4)}}; The best parameters are as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional note: By including the interaction2 terms, we have __more variables (1831) than observations (1460)__ in the training set. This situation is not admissable in classical statistics. For machine learning algorithms with regularization and/or iterative optimization, it does not mean any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have found the best parameters by cross-validation. Now, we try to solve a business problem with these results:<br>\n",
    "__The sales team needs all houses from the test set with estimated prices higher than 350'000\\$. Can you please deliver an estimate about how accurate your predictions are?__ (again we are allowed to be biased towards the test-set)<br>\n",
    "1. Compute the confidence intervals for the test-set.\n",
    "We proceed as follows:\n",
    "2. Obtain estimates for the train-set by splitting the train-set in k=5 folds and always train on 4 folds and make predictions on the 5th fold. We obtain CIs in this manner.\n",
    "3. We take the lower-bounds of the confidence intervals. This ensures, that we do not include cases (houses) with very unstable estimates.\n",
    "4. Since we trained the CIs for the training-set with cross-validation, we can treat them as an estimate for the accuracy of the CIs of the test-set: This gives us an estimate of the error we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from random import choices\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold # import KFold\n",
    "warnings.filterwarnings('ignore')\n",
    "# we get a lot of warnings:\n",
    "# LinAlgWarning: Ill-conditioned matrix (rcond=1.80167e-08): result may not be accurate.\n",
    "#  overwrite_a=False)\n",
    "# this is because we use more variables than observations and we get the already discussed\n",
    "# problems with matrix inversion\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(with_interactions) #  fit  the scale        \n",
    "\n",
    "\n",
    "X = scaler.transform(with_interactions[:len(train_ID)])\n",
    "test = scaler.transform(with_interactions[len(train_ID):])\n",
    "\n",
    "## 1.\n",
    "indices = np.arange(0, X.shape[0])\n",
    "# draw 200 samples with replacement from training data set\n",
    "sampler = (choices(indices, k = len(indices)) for i in range(200))\n",
    "# fit 200 models to the samples drawn and predict on test-set\n",
    "# \n",
    "CIS_test = np.percentile(\n",
    "            np.array(\n",
    "                [\n",
    "                 Ridge(alpha=best_parameters[-2][0][0], fit_intercept=True)\\\n",
    "                 .fit(X[drew,:], y.values[drew]).predict(test).tolist()\n",
    "                 for drew in sampler]\n",
    "                 ), [2.5, 97.5], axis = 0)\n",
    "\n",
    "\n",
    "## 2.\n",
    "kf = KFold(n_splits = 5, shuffle=True)\n",
    "CIS = np.empty((2, X.shape[0]))\n",
    "y_hat = np.empty((y.shape[0],))\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    indices = np.arange(0, X_train.shape[0])\n",
    "\n",
    "    sampler = (choices(indices, k = len(indices)) for i in range(200))\n",
    "    CIS[:, test_index] = np.percentile(\n",
    "        np.array(\n",
    "            [\n",
    "             Ridge(alpha=best_parameters[-1][0][0], fit_intercept=True)\\\n",
    "             .fit(X_train[drew,:], y_train.values[drew])\\\n",
    "                              .predict(X_test).tolist()\n",
    "             for drew in sampler]\n",
    "             ), [2.5, 97.5], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.\n",
    "bool_indizes = np.exp(CIS_test[0, :]) - 1 > 350000\n",
    "sum(bool_indizes)\n",
    "for_sales_departement = list(\n",
    "    zip(np.arange(0, CIS_test.shape[1])[bool_indizes], np.exp(CIS_test[0, bool_indizes])+1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lower = np.exp(CIS[0,:])-1\n",
    "estimates = y_hat_lower > 350000\n",
    "true = (np.exp(y) +1) > 350000\n",
    "y_hat_lower[estimates]\n",
    "print(f'we estimate that {np.round(np.mean(true[estimates]) * 100, 2)}% of our predictions are correct')\n",
    "print(f'\\nhowever, we only got {np.round(sum(true[estimates])/sum(true)*100, 2)}% of all houses with prices higher than 350000')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, suppose\n",
    "the sales team wants to use your algorith for new incoming houses. How good can we predict theses houses?\n",
    "\n",
    "__Question:__ What is the most accurate method to get a good estimate? In the example before, we fitted each statistic on the whole training-set. Is this a good idea to get an estimate about how the algorithm will perform on unseen data?\n",
    "\n",
    "__Exercise:__ get the estimates right by help of python Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding and adding variables for missing values is not critical: no statistics are fitted<br>\n",
    "\n",
    "we load the data (only train) once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', sep=\",\")\n",
    "train_ID = train['Id']\n",
    "train.drop('Id', axis = 1, inplace = True)\n",
    "SalePrice = train['SalePrice']\n",
    "train.drop('SalePrice', axis=1, inplace = True)\n",
    "categorical = [var for var in train.columns if train[var].dtype=='O']\n",
    "numerical = [var for var in train.columns if train[var].dtype!='O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add an indicator variable for every missing value: no statistics are fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[categorical] = train[categorical].fillna('None')\n",
    "for val in numerical:\n",
    "    train[val + '_na'] = pd.isnull(train[val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the follwing statistics however, depend on the data:<br>\n",
    "- filling NAs with mean-values\n",
    "- box-cox transform of variable\n",
    "- computing the best features and forming interaction variables\n",
    "- fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, please use the custom-function InteractionsTransformer provided in the file interactions_transformer.py; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "from interactions_transformer import InteractionsTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_features = numerical\n",
    "categorical_features = categorical\n",
    "\n",
    "# please use for these steps SimpleImputer and PowerTransformer -> look it up in the internet\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "                                     ])\n",
    "# one-hot encode (dummy encode) the categorical variables with OneHotEncoder\n",
    "categorical_transformer = \n",
    "\n",
    "# the ColumnTransformer combines the numeric_transformer and the categorical_transformer\n",
    "preprocessor = \n",
    "\n",
    "# use GridSearchCV, combine the preprocessor, InteractionsTransformer an ElasticNet within make_pipeline\n",
    "clf = GridSearchCV(\n",
    "            ,\n",
    "            param_grid={'elasticnet__alpha': 10**(np.linspace(-1, 0.2, 5)),\n",
    "                        'elasticnet__l1_ratio': np.linspace(0, 1, 6)},\n",
    "         cv=5, refit=False, scoring = 'neg_mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[numerical] += 1\n",
    "clf.fit(train, np.log1p(SalePrice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clf.best_params_, np.sqrt(-clf.best_score_))\n",
    "# clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to get the confidence intervalas CIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "from random import choices\n",
    "train[numerical] += 1\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle=True)\n",
    "CIS = np.empty((2, train.shape[0]))\n",
    "y_hat = np.empty((SalePrice.shape[0],))\n",
    "for train_index, test_index in kf.split(train):\n",
    "    X_train = train.iloc[train_index,:]\n",
    "    y_train = np.log1p(SalePrice)[train_index]\n",
    "    X_test = train.iloc[test_index, :]\n",
    "    y_test = np.log1p(SalePrice)[test_index]\n",
    "    indices = np.arange(0, X_train.shape[0])\n",
    "\n",
    "    custom_pipeline = make_pipeline(\n",
    "            preprocessor,\n",
    "            InteractionsTransformer(),\n",
    "            ElasticNet().set_params(alpha=clf.best_params_['elasticnet__alpha'],\n",
    "                                   l1_ratio=clf.best_params_['elasticnet__l1_ratio']))\n",
    "    sampler = (choices(indices, k = len(indices)) for i in range(200))\n",
    "    runs = []\n",
    "    for drew in sampler:\n",
    "        try:\n",
    "            runs.append(custom_pipeline.\\\n",
    "             fit(X_train.iloc[drew, :], y_train.iloc[drew]).predict(X_test).tolist()\n",
    "                       )\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    CIS[:, test_index] = np.percentile(np.array(runs), [2.5, 97.5], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lower = np.exp(CIS[0,:]) - 1\n",
    "estimates = y_hat_lower > 350000\n",
    "true = SalePrice > 350000\n",
    "y_hat_lower[estimates]\n",
    "print(f'we estimate that {np.round(np.mean(true[estimates]) * 100, 2)}% of our predictions are correct')\n",
    "print(f'\\nhowever, we only get {np.round(sum(estimates)/sum(true)*100, 2)}% of all houses with prices higher than 350000')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder\n",
    "We found the best parameters via extensive search over the whole data-set. As we have discussed, theoretically double-cross-validation would have been the better choice. However, it would also be computationally more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
