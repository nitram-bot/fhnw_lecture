{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b710f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1361b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext spacy-transformers spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import nmslib\n",
    "import spacy\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, download_if_missing=True)\n",
    "# http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "np.random.seed(123)\n",
    "texts = dataset.data # Extract text\n",
    "target = dataset.target # Extract target\n",
    "display(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d232ad",
   "metadata": {},
   "source": [
    "### Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca19812",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/glove.png', width=840, height=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bf10c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Word2Vec\n",
    "[Word2Vec paper](https://arxiv.org/pdf/1301.3781v3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e428fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/CBOW.png', width=320, height=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/SkipGram.png', width=420, height=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57da777",
   "metadata": {},
   "source": [
    "## FastText\n",
    "[FastText paper](https://arxiv.org/pdf/1607.04606.pdf). \n",
    "But are more approchable explanation can be found [here](https://amitness.com/2020/06/fasttext-embeddings/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab701442",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/FastText1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/FastText2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9963a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I skip this step because it takes too long\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumes too much RAM for me\n",
    "# ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d125848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you can decrease the embedding size\n",
    "# fasttext.util.reduce_model(ft, 100)\n",
    "# ft.get_dimension()\n",
    "# ft.save_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8107a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_word_vector('I am very smart but nobody knows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca730124",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset.data # Extract text\n",
    "target = dataset.target # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([ft.get_word_vector(t) for t in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "\n",
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3),\n",
    "         X, target, scoring = 'f1_micro', cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627bb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20719e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         X, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft=None\n",
    "del ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fae2ee",
   "metadata": {},
   "source": [
    "### BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/BERT1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(f'en_core_web_trf', disable=[\"tagger\", \"ner\", \"parser\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b051455",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [doc._.trf_data.tensors[-1] for doc in nlp.pipe(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('spacy_embeddings.pkl', 'wb') as sink:\n",
    "    pickle.dump(X, sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('spacy_embeddings.pkl', 'rb') as tap:\n",
    "    X = pickle.load(tap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.array([i[0] for i in X])\n",
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         X0, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea61ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([np.mean(i, axis=0) for i in X])\n",
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         X1, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e123a2a",
   "metadata": {},
   "source": [
    "### universal sentence encoder\n",
    "https://pypi.org/project/spacy-universal-sentence-encoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04456d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc941512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_universal_sentence_encoder\n",
    "nlp = spacy_universal_sentence_encoder.load_model('en_use_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('Hi there, how are you?')\n",
    "doc2 = nlp('Hello there, how are you doing today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b33297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.dot(doc1.vector, doc2.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d67e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc68aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_md = [nlp(t).vector for t in texts]\n",
    "import pickle\n",
    "with open('universal_sentence_encoder_embeddings.pkl', 'wb') as sink:\n",
    "    pickle.dump(X_md, sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         X_md, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm='l2' is default\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df = 0.3)\n",
    "X1 = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(f'{X1.shape[0]}, {X1.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         X1, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e7098",
   "metadata": {},
   "source": [
    "### Sentence-Transformers\n",
    "The initial [paper](https://arxiv.org/pdf/1908.10084.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101372af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/sentence-transformers1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/triplet_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9351800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e01e4",
   "metadata": {},
   "source": [
    "### multilingual sentence transformer\n",
    "This is the [publication](https://arxiv.org/abs/2004.09813) on this ingenious idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/multi_sbert.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483139ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/martin/python/fhnw_lecture/data'\n",
    "try:\n",
    "    model = SentenceTransformer(\\\n",
    "                    os.path.join(source, 'multi-qa-mpnet-base-dot-v1.pth'), device='cpu')\n",
    "except:\n",
    "    model = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device='cpu')\n",
    "    model.save(os.path.join(source, 'multi-qa-mpnet-base-dot-v1.pth'))\n",
    "\n",
    "model = model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71040d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model.encode(texts, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentence_transformer_embeddings.pkl', 'wb') as sink:\n",
    "    pickle.dump(embeddings, sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sentence_transformer_embeddings.pkl', 'rb') as tap:\n",
    "    embeddings = pickle.load(tap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dca94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         embeddings, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854363a",
   "metadata": {},
   "source": [
    "can we combine two different approaches? TF-IDF and the sentence embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "bigX = sparse.hstack([embeddings,X1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cross_val_score(SGDClassifier(max_iter=1000, tol=1e-3, loss=\"perceptron\"),\n",
    "         bigX, target, scoring = 'f1_micro', cv=3\n",
    ")\n",
    "np.mean(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb150176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response_de=requests.get('https://www.eda.admin.ch/aboutswitzerland/de/home/gesellschaft/sprachen/die-sprachen---fakten-und-zahlen.html')\n",
    "response_it = requests.get('https://www.eda.admin.ch/aboutswitzerland/it/home/gesellschaft/sprachen/die-sprachen---fakten-und-zahlen.html')\n",
    "response_fr = requests.get('https://www.eda.admin.ch/aboutswitzerland/fr/home/gesellschaft/sprachen/die-sprachen---fakten-und-zahlen.html')\n",
    "response_en = requests.get('https://www.eda.admin.ch/aboutswitzerland/en/home/gesellschaft/sprachen/die-sprachen---fakten-und-zahlen.html')\n",
    "response_es = requests.get('https://www.eda.admin.ch/aboutswitzerland/es/home/gesellschaft/sprachen/die-sprachen---fakten-und-zahlen.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bda9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup_de = BeautifulSoup(response_de.text, features=\"html.parser\")\n",
    "soup_it = BeautifulSoup(response_it.text, features=\"html.parser\")\n",
    "soup_fr = BeautifulSoup(response_fr.text, features=\"html.parser\")\n",
    "soup_en = BeautifulSoup(response_en.text, features=\"html.parser\")\n",
    "soup_es = BeautifulSoup(response_es.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77401ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "paragraphs_de = [re.sub(r'\\s{1,}', ' ', para.get_text().replace('\\n', ' ')).strip() for para in soup_de.find_all(\"p\")]\n",
    "paragraphs_it = [re.sub(r'\\s{1,}', ' ', para.get_text().replace('\\n', ' ')).strip() for para in soup_it.find_all(\"p\")]\n",
    "paragraphs_fr = [re.sub(r'\\s{1,}', ' ', para.get_text().replace('\\n', ' ')).strip() for para in soup_fr.find_all(\"p\")]\n",
    "paragraphs_en = [re.sub(r'\\s{1,}', ' ', para.get_text().replace('\\n', ' ')).strip() for para in soup_en.find_all(\"p\")]\n",
    "paragraphs_es = [re.sub(r'\\s{1,}', ' ', para.get_text().replace('\\n', ' ')).strip() for para in soup_es.find_all(\"p\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/martin/python/fhnw_lecture/data'\n",
    "try:\n",
    "    model = SentenceTransformer(\\\n",
    "                    os.path.join(source, 'paraphrase-multilingual-mpnet-base-v2.pth'), device='cpu')\n",
    "except:\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device='cpu')\n",
    "    model.save(os.path.join(source, 'paraphrase-multilingual-mpnet-base-v2.pth'))\n",
    "\n",
    "model = model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ed413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "all_paragraphs = paragraphs_de + paragraphs_it + paragraphs_fr + paragraphs_en + paragraphs_es\n",
    "sentences = [s for p in all_paragraphs for s in sent_tokenize(p if len(p.split(' ')) > 2 else '')]\n",
    "\n",
    "with torch.no_grad():\n",
    "    multi_lang_embeddings = model.encode(sentences, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05379bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lang_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70557afd",
   "metadata": {},
   "source": [
    "### visualization via u-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37147607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = multi_lang_embeddings.shape\n",
    "normalized = multi_lang_embeddings/\\\n",
    "      np.resize(np.linalg.norm(multi_lang_embeddings, axis=1), (h, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dc1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_transformed = reducer.fit_transform(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "\n",
    "embedding_df = pd.DataFrame(umap_transformed, columns=('x', 'y'))\n",
    "embedding_df['sentences'] = sentences\n",
    "# embedding_df['image'] = list(map(embeddable_image, digits.images))\n",
    "\n",
    "datasource = ColumnDataSource(embedding_df)\n",
    "color_mapping = CategoricalColorMapper(factors=[str(i) for i in np.arange(0,9)],\n",
    "                                       palette=Spectral10)\n",
    "output_file(filename='/home/martin/python/fhnw_lecture/images/multilang_umap.html', title = 'sentence similarity')\n",
    "plot_figure = figure(\n",
    "    title='UMAP projection of sentence embeddings',\n",
    "    plot_width=1000,\n",
    "    plot_height=800,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    ")\n",
    "\n",
    "plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 16px; color: #224499'>sentence:</span>\n",
    "        <span style='font-size: 18px'>@sentences</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plot_figure.circle(\n",
    "    'x',\n",
    "    'y',\n",
    "    source=datasource,\n",
    "    color=dict(field='sentences',transform=color_mapping),\n",
    "    line_alpha=0.6,\n",
    "    fill_alpha=0.6,\n",
    "    size=4\n",
    ")\n",
    "save(plot_figure)\n",
    "# show(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ebdbc",
   "metadata": {},
   "source": [
    "### Monolingual models can be found on 'the Huggingface':\n",
    "best [german model](https://huggingface.co/T-Systems-onsite/german-roberta-sentence-transformer-v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imbalanced]",
   "language": "python",
   "name": "conda-env-imbalanced-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
