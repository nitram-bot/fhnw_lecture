{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finished-counter",
   "metadata": {},
   "source": [
    "# Classification example demonstrating stacking and mean-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost.datasets import amazon\n",
    "employee_train, employee_test = amazon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-switzerland",
   "metadata": {},
   "source": [
    "The data is taken from a kaggle competition where catboost was shining:<br>\n",
    "[https://www.kaggle.com/c/amazon-employee-access-challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = employee_train['ACTION']\n",
    "X = employee_train.drop('ACTION', axis=1)\n",
    "\n",
    "# Split into train & validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-supplier",
   "metadata": {},
   "source": [
    "### catboost\n",
    "\n",
    "[2018: catboost the new kid on the block from russia](https://arxiv.org/pdf/1810.11363.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import ipywidgets\n",
    "cat_features = [*range(8)]\n",
    "model = CatBoostClassifier(custom_metric=['TotalF1'], early_stopping_rounds=100, eval_metric='AUC')\n",
    "\n",
    "model.fit(X_train, y_train, cat_features=cat_features,\n",
    "          eval_set=(X_val, y_val), plot=True, verbose=False, use_best_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-island",
   "metadata": {},
   "source": [
    "### lightgbm in comparison\n",
    "\n",
    "Remark, that we do no hyperparameter tuning at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "model = lightgbm.LGBMClassifier(metric='auc', n_estimators=5000, learning_rate=0.02, random_state=42)\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                      verbose=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-maximum",
   "metadata": {},
   "source": [
    "## Now, we add the mean-encoding manually as a preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from validatedstackedmeanencoder import ValidatedStackedMeanEncoder\n",
    "numerical = []\n",
    "categorical = X_train.columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "      \n",
    "    \n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        #('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical),\n",
    "        ('mean_enc', ValidatedStackedMeanEncoder(), categorical)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-battlefield",
   "metadata": {},
   "source": [
    "### without mean-encoding\n",
    "The setup is the same, except that we pass FunctionTransformer(None) to the categorical variables - everything else is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "model = lightgbm.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.02, \n",
    "                                n_estimators=5000, subsample_for_bin=20000, objective='binary', \n",
    "                                subsample=1.0, subsample_freq=0, colsample_bytree=1.0, \n",
    "                                n_jobs=- 1, silent=True, importance_type='split',\n",
    "                                is_unbalance = False, scale_pos_weight = 1.0, random_state=42, metric='auc',\n",
    "                                verbose=1\n",
    "                              )\n",
    "\n",
    "custom_pipeline1 = make_pipeline(\n",
    "            ColumnTransformer(transformers=[('num', numeric_transformer, numerical), \n",
    "                                            ('empty', FunctionTransformer(None), categorical)]),\n",
    "            model\n",
    "            )\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "custom_pipeline1.fit(X_train, y_train, lgbmclassifier__eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                     lgbmclassifier__early_stopping_rounds=100)\n",
    "#custom_pipeline.predict(test)\n",
    "print(\"model score: %.3f\" % custom_pipeline1.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(y_val, custom_pipeline1.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC score: \", roc_auc_score(y_val, custom_pipeline1.predict_proba(X_val)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-monaco",
   "metadata": {},
   "source": [
    "### with mean-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-exchange",
   "metadata": {},
   "source": [
    "we first transform the train and test set for the early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "new_X = preprocessor2.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_val = preprocessor2.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_pipeline = make_pipeline(\n",
    "            preprocessor2,\n",
    "            model\n",
    "            )\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "custom_pipeline.fit(X_train, y_train, lgbmclassifier__eval_set=[(new_X, y_train), (new_X_val, y_val)],\n",
    "                     lgbmclassifier__early_stopping_rounds=100)\n",
    "#custom_pipeline.predict(test)\n",
    "print(\"model score: %.3f\" % custom_pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, log_loss\n",
    "\n",
    "print(classification_report(y_val, custom_pipeline.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC score: \", roc_auc_score(y_val, custom_pipeline.predict_proba(X_val)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-diana",
   "metadata": {},
   "source": [
    "### can we tune the parameters? For example, the learning-rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for lr in 10**np.linspace(-2.5, -0.2, 6):\n",
    "    custom_pipeline.set_params(lgbmclassifier__learning_rate= lr)\n",
    "    custom_pipeline.fit(X_train, y_train, lgbmclassifier__eval_set=[(new_X, y_train), (new_X_val, y_val)],\n",
    "                     lgbmclassifier__early_stopping_rounds=100)\n",
    "#custom_pipeline.predict(test)\n",
    "    print(\"model score: %.3f\" % custom_pipeline.score(X_val, y_val))\n",
    "    results.append(roc_auc_score(y_val, custom_pipeline.predict_proba(X_val)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "removable-reunion",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b5de1523c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "list(zip(results, 10**np.linspace(-4, -0.2, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b575dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imbalanced]",
   "language": "python",
   "name": "conda-env-imbalanced-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
