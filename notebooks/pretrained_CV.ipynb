{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb1ef57",
   "metadata": {},
   "source": [
    "## Object Detection with the OpenImages v4 classes\n",
    "\n",
    "The paper to this project is [this](https://openaccess.thecvf.com/content/ICCV2021/papers/Narayan_Discriminative_Region-Based_Multi-Label_Zero-Shot_Learning_ICCV_2021_paper.pdf)<br>\n",
    "The code is slightly modified copy from the one in the corresponding [github-repository](https://github.com/akshitac8/BiAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a703ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../images/BiAM1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797280eb",
   "metadata": {},
   "source": [
    "### Understanding the Backbone\n",
    "$\\mathbf{x}_g$, the global or scene-context features as well as the regional features $\\mathbf{x}_r$ are extracted for the different layers of an old-school VVG19 net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Image('../images/vgg1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    computes regional features (backbone)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        \n",
    "        # all convolutions but the the last MaxPool2d layer\n",
    "        self.regional_features = nn.Sequential(*list(vgg19.features[:-1]))\n",
    "    def forward(self, x):\n",
    "        # image with 224 x 224 x 3 but has to be in pytorch order\n",
    "        x = x.view(-1, 3, 224, 224)\n",
    "        return self.regional_features(x)\n",
    "\n",
    "       \n",
    "    \n",
    "class vgg_net(nn.Module):\n",
    "    \"\"\"\n",
    "    applies the last MaxPool2d to the backbone 'regional' features\n",
    "    and passes them through part of the classifier to get a 4096\n",
    "    contextual feature-vector\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(vgg_net, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        # only MaxPool2d operation\n",
    "        self.features = nn.Sequential(*list(vgg19.features[-1:]))\n",
    "                        \n",
    "        # classifier without the last Dropout and linear-layer 4096 -> 1000\n",
    "        self.fc = nn.Sequential(*list(vgg19.classifier[0:-2]))\n",
    "                  \n",
    "        ## nnlist = []\n",
    "        ## nnlist.append(vgg19.features[36])\n",
    "        ## for i in list(vgg19.classifier[0:5]):\n",
    "        ##     nnlist.append(i)\n",
    "        ## self.fc = nn.Sequential(*nnlist)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view([-1,512,14,14])\n",
    "        x = self.features(x)\n",
    "        # x = x.view(x.size(0),-1)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class Vgg(nn.Module):\n",
    "    \"\"\"\n",
    "    takes contextual features and classifies in 1000 imagenet categories\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Vgg, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        self.final_classifier = nn.Sequential(*list(vgg19.classifier[-2:]))\n",
    "    def forward(self, x):\n",
    "        # input is batch-size, contextual-feature vector\n",
    "        x = x.view(-1, 4096)\n",
    "        return self.final_classifier(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = torchvision.models.vgg19(True)\n",
    "vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/vgg2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e5e0ae",
   "metadata": {},
   "source": [
    "### Joint Visual-Semantic Space\n",
    "\n",
    "The target of the BiAM-Network are the Glove-embeddings of the OpenImages-Labels. The Network is trained to output embedding-vectors that are as near as possible to the embeddings of the labels.<br>\n",
    "This is also how the zero-shot learning takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torchvision\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    computes regional features (backbone)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        \n",
    "        # all convolutions but the the last MaxPool2d layer\n",
    "        self.regional_features = nn.Sequential(*list(vgg19.features[:-1]))\n",
    "    def forward(self, x):\n",
    "        # image with 224 x 224 x 3 but has to be in pytorch order\n",
    "        x = x.view(-1, 3, 224, 224)\n",
    "        return self.regional_features(x)\n",
    "    \n",
    "class Vgg(nn.Module):\n",
    "    \"\"\"\n",
    "    takes contextual features and classifies in 1000 imagenet categories\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Vgg, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        self.final_classifier = nn.Sequential(*list(vgg19.classifier[-2:]))\n",
    "    def forward(self, x):\n",
    "        # input is batch-size, contextual-feature vector\n",
    "        x = x.view(-1, 4096)\n",
    "        return self.final_classifier(x)\n",
    "\n",
    "class vgg_net(nn.Module):\n",
    "    \"\"\"\n",
    "    applies the last MaxPool2d to the backbone 'regional' features\n",
    "    and passes them through part of the classifier to get a 4096\n",
    "    contextual feature-vector\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(vgg_net, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        # only MaxPool2d operation\n",
    "        self.features = nn.Sequential(*list(vgg19.features[-1:]))\n",
    "        # classifier without the last Dropout and linear-layer 4096 -> 1000\n",
    "        self.fc = nn.Sequential(*list(vgg19.classifier[0:-2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view([-1,512,14,14])\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "random.seed(3483)\n",
    "np.random.seed(3483)\n",
    "torch.manual_seed(3483)\n",
    "torch.cuda.manual_seed(3483)\n",
    "torch.cuda.manual_seed_all(3483)\n",
    "\n",
    "def tensordot(x,y):\n",
    "    return torch.matmul(x, y)\n",
    "    # return torch.einsum(\"abc,cd->abd\", (x, y))\n",
    "\n",
    "\n",
    "def matmul(x,y):\n",
    "    return torch.matmul(x, y)\n",
    "    # return torch.einsum(\"ab,bc->ac\", (x, y))\n",
    "\n",
    "class CONV3_3(nn.Module):\n",
    "    def __init__(self, num_in=512,num_out=512,kernel=3):\n",
    "        super(CONV3_3, self).__init__()\n",
    "        self.body = nn.Conv2d(num_in, num_out, kernel, padding=int((kernel-1)/2), dilation=1)\n",
    "        self.bn = nn.BatchNorm2d(num_out, affine=True, eps=0.001, momentum=0.99)\n",
    "        self.relu = nn.ReLU(True)\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x) \n",
    "        return x\n",
    "\n",
    "class CONV1_1(nn.Module):\n",
    "    def __init__(self, num_in=512,num_out=512,kernel=1):\n",
    "        super(CONV1_1, self).__init__()\n",
    "        self.body = nn.Conv2d(num_in, num_out, kernel, padding=int((kernel-1)/2), dilation=1)\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return x\n",
    "\n",
    "class vgg_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg_net, self).__init__()\n",
    "        vgg19 = torchvision.models.vgg19(True)\n",
    "        self.features = nn.Sequential(*list(vgg19.features[-1:]))\n",
    "        self.fc = nn.Sequential(*list(vgg19.classifier[0:-2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view([-1,512,14,14])\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class RCB(nn.Module):\n",
    "    \"\"\"\n",
    "    Region contextualized block\n",
    "    \"\"\"\n",
    "    def __init__(self, heads=8, d_model=512, d_ff=1024, dropout = 0.1):\n",
    "        super(RCB, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.w_q = nn.Conv2d(in_channels = d_model , out_channels = d_model , kernel_size=1, bias=True)\n",
    "        self.w_k = nn.Conv2d(in_channels = d_model , out_channels = d_model , kernel_size=1, bias=True)\n",
    "        self.w_v = nn.Conv2d(in_channels = d_model, out_channels = d_model, kernel_size=1, bias=True)\n",
    "        self.w_o = nn.Conv2d(in_channels = d_model , out_channels = d_model , kernel_size=1, bias=True)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.sub_network = C_R(d_model, d_ff)\n",
    "\n",
    "    def F_R(self, q, k, v, d_k, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "        #scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.Tensor([d_k])) #math.sqrt(d_k)\n",
    "        #scores = torch.div(torch.matmul(q, k.transpose(-2, -1)), torch.sqrt(torch.Tensor([d_k]))) #math.sqrt(d_k)\n",
    "        scores = scores.masked_fill(scores == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores) \n",
    "        return scores\n",
    "\n",
    "    def forward(self, q_feat, k_feat, v_feat):\n",
    "        if k_feat is None:\n",
    "            k_feat = q_feat\n",
    "        bs = q_feat.size(0)\n",
    "        spa = q_feat.size(-1)\n",
    "        residual = q_feat\n",
    "        k_h_r = self.w_k(k_feat).view(bs, self.h, self.d_k, spa*spa).transpose(3,2)\n",
    "        q_h_r = self.w_q(q_feat).view(bs, self.h, self.d_k, spa*spa).transpose(3,2)\n",
    "        v_h_r = self.w_v(v_feat).view(bs, self.h, self.d_k, spa*spa).transpose(3,2)\n",
    "        r_h = self.F_R(q_h_r, k_h_r, v_h_r, self.d_k, self.dropout_1)\n",
    "        alpha_h = torch.matmul(r_h, v_h_r)\n",
    "        o_r = alpha_h.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        o_r = o_r.permute(0,2,1)\n",
    "        o_r = o_r.view(-1,self.d_model,spa,spa)\n",
    "        o_r = self.dropout_2(self.w_o(o_r))\n",
    "        o_r += residual\n",
    "        input_o_r = o_r\n",
    "        e_r = self.sub_network(o_r)\n",
    "        e_r += input_o_r\n",
    "        return e_r\n",
    "\n",
    "class C_R(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = d_model , out_channels = d_ff , kernel_size= 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels = d_ff , out_channels = d_model , kernel_size= 1, bias=True)\n",
    "    def forward(self, x):\n",
    "        x_out = self.conv2(F.relu(self.conv1(x), True))\n",
    "        return x_out\n",
    "\n",
    "class SCB(nn.Module):\n",
    "    \"\"\"\n",
    "    scene contextualized block\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, D):\n",
    "        super(SCB, self).__init__()\n",
    "        self.channel_dim = opt.channel_dim\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.gcdropout = nn.Dropout(0.2)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, False)\n",
    "        self.w_g = nn.Conv2d(in_channels=4096,out_channels=self.channel_dim,kernel_size=1,bias=True) #nn.Linear(4096, self.channel_dim, bias=False) # \n",
    "        self.gcff = CONV3_3(num_in=self.channel_dim, num_out=self.channel_dim)\n",
    "        self.channel_conv = CONV1_1(num_in=self.channel_dim, num_out=self.channel_dim)\n",
    "\n",
    "    def F_G(self, q , k):\n",
    "        r_g = q * k\n",
    "        r_g = self.sigmoid(r_g)     \n",
    "        r_g = r_g.view(-1,self.channel_dim,1)\n",
    "        return r_g\n",
    "\n",
    "    def forward(self, h_r, vecs, x_g):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        q_g = self.lrelu(self.channel_conv(h_r))\n",
    "        v_g =  self.lrelu(self.channel_conv(h_r))\n",
    "        k_g = self.w_g(self.gcdropout(x_g).view(-1,4096,1,1))\n",
    "        # k_g = self.w_g(self.gcdropout(x_g))\n",
    "        q_g_value = q_g.view(-1,self.channel_dim,196).mean(-1).repeat(1,1,1).view(-1,self.channel_dim)\n",
    "        r_g = self.F_G(q_g_value,k_g.view(-1,self.channel_dim))\n",
    "        # r_g = self.F_G(q_g_value,k_g)\n",
    "        c_g = r_g.unsqueeze(3).unsqueeze(4) * v_g.unsqueeze(2)\n",
    "        c_g = c_g.view(-1,self.channel_dim,14,14)\n",
    "        e_g = c_g + self.gcff(c_g)\n",
    "        return e_g\n",
    "\n",
    "class BiAM(nn.Module):\n",
    "    def __init__(self, opt, dim_w2v=300, dim_feature=[196,512]):\n",
    "        super(BiAM, self).__init__()\n",
    "        D = dim_feature[1]     #### D is the feature dimension of attention windows\n",
    "        self.channel_dim = opt.channel_dim\n",
    "        self.conv_3X3 = CONV3_3(num_out=self.channel_dim)\n",
    "        self.region_context_block = RCB(heads=opt.heads, d_model=self.channel_dim, d_ff=self.channel_dim*2, dropout = 0.1)\n",
    "        self.scene_context_block = SCB(opt, D)\n",
    "        self.W = nn.Linear(dim_w2v,D, bias=True)\n",
    "        self.conv_1X1 = CONV1_1(num_in=self.channel_dim*2, num_out=D)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def predict(self, e_f, vecs, W):\n",
    "        classifiers = W(vecs)                                 \n",
    "        m = tensordot(e_f, classifiers.t())                                   \n",
    "        logits = torch.topk(m,k=6,dim=1)[0].mean(dim=1)\n",
    "        return logits\n",
    "        \n",
    "    def forward(self, features, vecs, x_g):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        x_r = features.view([-1,512,14,14])\n",
    "        h_r = self.conv_3X3(x_r)\n",
    "        e_r = self.region_context_block(h_r,h_r,h_r)\n",
    "        e_g = self.scene_context_block(h_r, vecs, x_g)\n",
    "        e_f = torch.cat([e_r, e_g], dim=1)\n",
    "        e_f = self.lrelu(self.conv_1X1(e_f))\n",
    "        e_f = e_f.permute(0,2,3,1)\n",
    "        e_f = e_f.view(-1,196,512)\n",
    "        logits = self.predict(e_f, vecs, self.W)\n",
    "        return logits\n",
    "\n",
    "def ranking_lossT(logitsT, labelsT):\n",
    "    eps = 1e-8\n",
    "    subset_idxT = torch.sum(torch.abs(labelsT),dim=0)\n",
    "    subset_idxT = (subset_idxT>0).nonzero().view(-1).long().cuda()\n",
    "    sub_labelsT = labelsT[:,subset_idxT]\n",
    "    sub_logitsT = logitsT[:,subset_idxT]    \n",
    "    positive_tagsT = torch.clamp(sub_labelsT,0.,1.)\n",
    "    negative_tagsT = torch.clamp(-sub_labelsT,0.,1.)\n",
    "    maskT = positive_tagsT.unsqueeze(1) * negative_tagsT.unsqueeze(-1)\n",
    "    pos_score_matT = sub_logitsT * positive_tagsT\n",
    "    neg_score_matT = sub_logitsT * negative_tagsT\n",
    "    IW_pos3T = pos_score_matT.unsqueeze(1)\n",
    "    IW_neg3T = neg_score_matT.unsqueeze(-1)\n",
    "    OT = 1 + IW_neg3T - IW_pos3T\n",
    "    O_maskT = maskT * OT\n",
    "    diffT = torch.clamp(O_maskT, 0)\n",
    "    violationT = torch.sign(diffT).sum(1).sum(1) \n",
    "    diffT = diffT.sum(1).sum(1) \n",
    "    lossT =  torch.mean(diffT / (violationT+eps))\n",
    "    return lossT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', type=float, default=0.001 ,help='initial learning rate')\n",
    "parser.add_argument('--lr_min', type=float, default=0.0002 ,help='minimum lr for scheduler drop')\n",
    "parser.add_argument('--train_full_lr', type=float, default=0.0002 ,help='lr for finetuning')\n",
    "parser.add_argument('--workers', type=int,help='number of data loading workers', default=0)\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('--cuda', action='store_true',default=True, help='enables cuda')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--nepoch', type=int, default=2000, help='number of epochs to train for')\n",
    "parser.add_argument('--train', action='store_true',default=False, help='enables cuda')\n",
    "parser.add_argument('--train_full_data', action='store_true',default=False, help='Only train a pretrained model')\n",
    "\n",
    "parser.add_argument('--eval_interval', type=int, default=2)\n",
    "parser.add_argument('--test_interval', type=int, default=10)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--val_batch_size', type=int, default=500)\n",
    "parser.add_argument('--test_batch_size', type=int, default=500)\n",
    "\n",
    "\n",
    "parser.add_argument('--save_path', type=str, default='test dataset type 2 split', help='details regarding the code')\n",
    "parser.add_argument('--SESSION', type=str, default='SA_LRANK', help='MODEL NAME')\n",
    "parser.add_argument('--job_id', type=str, default='14567', help='file job id')\n",
    "\n",
    "parser.add_argument('--heads', type=int, default=4, help='Heads for region Atn')\n",
    "\n",
    "parser.add_argument('--cosinelr_scheduler', action='store_true',default=False, help='Run with lr scheduler')\n",
    "parser.add_argument('--summary', type=str, default='Summary', help='Summary of Expt')\n",
    "parser.add_argument('--src', type=str,default=\"../../../../data\")\n",
    "\n",
    "parser.add_argument('--nseen_class', type=int, default=925,help='number of seen classes')\n",
    "parser.add_argument('--nclass_all', type=int, default=1006,help='number of all classes')\n",
    "\n",
    "parser.add_argument('--channel_dim', type=int, default=256,help='conv channel dim')\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "opt, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c26114",
   "metadata": {},
   "outputs": [],
   "source": [
    "biam_model_path = '/home/martin/python/fhnw_lecture/data'\n",
    "glove_vectors = 'OpenImage_w2v_context_window_10_glove-wiki-gigaword-300.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3373b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "biam = BiAM(opt, dim_feature=[196,512])\n",
    "biam.load_state_dict(torch.load(os.path.join(biam_model_path, 'model_BiAM.pth'),map_location=torch.device('cpu')))\n",
    "\n",
    "src_att = pickle.load(open(os.path.join(biam_model_path, glove_vectors), 'rb'))\n",
    "vecs_7186 = torch.from_numpy(normalize(src_att[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "all_classes = pd.read_csv(os.path.join(biam_model_path, 'OpenImages/class-descriptions.csv'), header=None)\n",
    "all_classes.columns = ['id', 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e441ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = pd.read_csv(os.path.join(biam_model_path, 'OpenImages/classes-trainable.txt'), header=None)\n",
    "trainable.columns = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befd916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = all_classes.merge(trainable, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = class_labels[['object']].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cc5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "with open('/home/martin/python/fhnw_lecture/data/imagenet1000_clsidx_to_labels.txt', 'r') as tap:\n",
    "    imagenet_cls = tap.read()\n",
    "    \n",
    "imagenet_cls = ast.literal_eval(imagenet_cls)    \n",
    "\n",
    "imagenet_cls = np.asarray(list(imagenet_cls.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual = vgg_net()\n",
    "backbone = Net()\n",
    "image_net_clf = Vgg()\n",
    "biam.eval()\n",
    "contextual.eval()\n",
    "backbone.eval()\n",
    "image_net_clf.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from PIL import Image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # bilinear interpolation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "#'https://unsplash.com/s/photos/random-objects'\n",
    "response = requests.get('https://unsplash.com/s/photos/random-objects')\n",
    "soup = BeautifulSoup(response.text)\n",
    "images = []\n",
    "for img in soup.findAll('img'):\n",
    "    images.append(img.get('src'))\n",
    "\n",
    "print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#for im in range(0, 10):\n",
    "for im in list(set(images)):\n",
    "    try:\n",
    "        # response = requests.get('https://picsum.photos/200/300?random=1')\n",
    "        response = requests.get(im)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    except:\n",
    "        next\n",
    "    \n",
    "    i = transform(img) if img.mode == 'RGB' else None\n",
    "    if type(i) == torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            regional_features = backbone(i)\n",
    "            vgg_4096 = contextual(regional_features)\n",
    "            biam_logits = biam(regional_features, vecs_7186, vgg_4096)\n",
    "            img_net_logits = image_net_clf(vgg_4096)\n",
    "            biam_probs = F.softmax(biam_logits, dim=1)\n",
    "            img_net_probs = F.softmax(img_net_logits, dim=1)\n",
    "\n",
    "        probabilities = biam_probs.cpu().numpy().reshape(-1)\n",
    "        img_net_probabilities = img_net_probs.cpu().numpy().reshape(-1)\n",
    "        order = np.flip(np.argsort(probabilities))\n",
    "        order_imgnet = np.flip(np.argsort(img_net_probabilities))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        print([(i, j) for i,j in zip(class_labels[order], probabilities[order])][0:10])\n",
    "        print([(i,j) for i,j in zip(imagenet_cls[order_imgnet], img_net_probabilities[order_imgnet])][0:5])\n",
    "        plt.show()\n",
    "                                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10667c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(im)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "i = transform(img) if img.mode == 'RGB' else None\n",
    "if type(img) == torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        regional_features = backbone(img)\n",
    "        vgg_4096 = contextual(regional_features)\n",
    "        biam_logits = biam(regional_features, vecs_7186, vgg_4096)\n",
    "        img_net_logits = image_net_clf(vgg_4096)\n",
    "        biam_probs = F.softmax(biam_logits, dim=1)\n",
    "        img_net_probs = F.softmax(img_net_logits, dim=1)\n",
    "\n",
    "    probabilities = biam_probs.cpu().numpy().reshape(-1)\n",
    "    order = np.flip(np.argsort(probabilities))\n",
    "    display(Image(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imbalanced]",
   "language": "python",
   "name": "conda-env-imbalanced-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
