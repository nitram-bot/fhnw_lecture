{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_data(path_to_ipynb):\n",
    "    with open(path_to_ipynb, 'r', encoding='UTF-8') as notebook:\n",
    "        notebook_data = json.load(notebook)\n",
    "    return notebook_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/martin/python/fhnw_lecture/notebooks/00_core.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/Performance_tips_and_tricks.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/Regression Techniques.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/Model_Selection_and_Validation_Schemes.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/index.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/house prices.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/01_Text_Clustering.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/Tree Methods.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/02_Segmentation.ipynb',\n",
       " '/home/martin/python/fhnw_lecture/notebooks/Translate_lecture_notebooks.ipynb']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_notebooks= [f'{os.getcwd()}/{file}' for file in os.listdir(os.getcwd()) if os.path.isfile(os.getcwd() + f'/{file}') and file.endswith('.ipynb')]\n",
    "all_notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nb = get_notebook_data(all_notebooks[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analytical vs. numerical approach\n",
      "\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "\n",
      "model.fit(X, y)\n",
      "\n",
      "# plt.plot(X, y, 'ro')\n",
      "\n",
      "# plt.show()\n",
      "\n",
      "\n",
      "\n",
      "for i in range(len(y)):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "\n",
      "\n",
      "#plt.close('all')\n",
      "\n",
      "## Derivation of parameters analytically\n",
      "\n",
      "\n",
      "\n",
      "We need the first derivative of the error-term with respect to the parameters. By setting the resulting equation equal to zero we assure that we get the parameter estimate at the minimum.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# we can easily verify these results\n",
      "\n",
      "## multivariate case: more than one x variable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "In matrix notation this is written:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We apply the same steps as for the derivation above:\n",
      "\n",
      "* we expand the error term:\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "## Polynomial regression as an example for more than one variable\n",
      "\n",
      "\n",
      "\n",
      "Some important points are:\n",
      "\n",
      "* we now have two variables, i.e. we can apply our formula for matrix notation\n",
      "\n",
      "* more variables will probably lead to a better fit\n",
      "\n",
      "# polynomial\n",
      "\n",
      "# underdetermined, ill-posed: infinitely many solutions\n",
      "\n",
      "# the x (small x) is just for plotting purpose\n",
      "\n",
      "# again we can compare the parameters of the model with those resulting from \n",
      "\n",
      "# our derived equation:\n",
      "\n",
      "### Overfitting\n",
      "\n",
      "We continue with adding variables and exagerate a little bit\n",
      "\n",
      "\n",
      "\n",
      "The important points to note here:\n",
      "\n",
      "* the fit to our empirical y-values gets better\n",
      "\n",
      "* at the same time, the regression line starts behaving strangly\n",
      "\n",
      "# underdetermined, ill-posed: infinitely many solutions\n",
      "\n",
      "### perfect fit: as many variables as data samples\n",
      "\n",
      "A perfect fit is possible as is demonstrated next. We have as many variables (terms derived from x) as observations (data points). So for each data point we have a variable to accommodate it.<br>\n",
      "\n",
      "# underdetermined, ill-posed: infinitely many solutions\n",
      "\n",
      "## What happens if we have more variables than data points?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "However, there are decompositions for matrix inversions that allow to invert singular matrices. Numpy is using such a decomposition, called LU-decomposition.\n",
      "\n",
      "\n",
      "\n",
      "# underdetermined, ill-posed: infinitely many solutions\n",
      "\n",
      "# this should give at least a warning, because matrix inversion as done above is not possible\n",
      "\n",
      "# any more, due to singular covariance matrix [X'X]\n",
      "\n",
      "# statistical package R\n",
      "\n",
      "The R statistical package is behaving still in another way. No warning is issued but coefficients are only computed for 11 variables (intercept included).\n",
      "\n",
      "# Dealing with overfitting\n",
      "\n",
      "As we could see, if there are many variables and only few observations, classical linear regression tends to overfit heavily.<br>\n",
      "\n",
      "\n",
      "\n",
      "## Ridge regression\n",
      "\n",
      "\n",
      "\n",
      "Remember this formula:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For two variables we can write:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "And in matrix notation for an arbitrary number of variables:\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "Following Hastie et al., originally, this was introduced to cope the rank deficency problems. When the algorithm was first proposed, it was not possible to invert a square matrix not of full rank. Hence, adding a small positive amount to its diagonal solved this problem. This can be demonstrated with our numerical example:\n",
      "## you can see how small this amount is, by having a glimpse on the diagonal elements:\n",
      "\n",
      "### example of ridge regression\n",
      "\n",
      "Next, we will apply ridge regression as implemented in the python sklearn library and compare the results to the analytical solution. Note, that we have to center the variables.\n",
      "\n",
      "# for plotting purpose\n",
      "\n",
      "# the result as obtained from the sklearn library\n",
      "\n",
      "# the analytical result as discussed above\n",
      "\n",
      "# here we add the mean of y to the predictions to display results in original coord. system\n",
      "\n",
      "# now the overfitted solution\n",
      "\n",
      "## Lasso\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For two variables we can write:\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "Unfortunately, and contrarily to ridge regression, there exists no closed form expression for computing the coefficients for the lasso.\n",
      "### lasso regression\n",
      "\n",
      "Next, we will apply lasso regression as implemented in the python sklearn library and compare the results to the unconstraint regression results.<br>\n",
      "\n",
      "As before, we have to center the variables (-> see discussion above)\n",
      "# for plotting purpose\n",
      "\n",
      "# the result as obtained from the sklearn library\n",
      "\n",
      "# now the overfitted solution\n",
      "\n",
      "# comparison of parameters ridge vs. lasso:\n",
      "\n",
      "## the difference between ridge and lasso\n",
      "\n",
      "To see why lasso shrinks parameters to zero, we will:\n",
      "\n",
      "* plot error contour\n",
      "\n",
      "* indicate the optimal point for the combination of penalty and MSE-error\n",
      "# generation of random data set:\n",
      "\n",
      "# test with linear regression \n",
      "\n",
      "# compute MSE-error\n",
      "\n",
      "# plot MSE-error contour\n",
      "\n",
      "# plot optimal solution\n",
      "\n",
      "# all ridge solutions with a penalty budget of 1\n",
      "\n",
      "# all lasso solutions with a penalty budget of 1\n",
      "\n",
      "# best ridge solution with penalty budget of 1\n",
      "\n",
      "# best lasso solution with penalty budget of 1\n",
      "\n",
      "The yellow ball like line and the red triangular line indicate parameter values corresponding to a penalty budget of 1 for the ridge and the lasso respectively. The best solution is given by the point on the line associated with the least mse-error. These points are also indicated.<br>\n",
      "\n",
      "# Elastic net\n",
      "\n",
      "The ridge regression and the lasso tend to shrink parameters quiet differently. A compromise that gaines huge popularity is a combination of the two:\n",
      "\n",
      "We will use elastic net in our data example.\n",
      "# Interaction\n",
      "\n",
      "If the effect of one variable on the outcome depends on the value of another variable. As an example we could try to model the probability that someone is going to buy a house. A very important variable for buying a house will be the monthly income. Another one could be the marital status (single, maried, maried with kids). Maried persons with kids will be very inclined to buy a house if the income situation is favorable. Singles, even with high income will not be considering buying a house. So, the effect of income is different for the levels of marital status:\n",
      "This introducing example comprised categorical variables. Interaction effects may also exist for continuous variables. In this case it is just harder to visualize. Again, we will construct our own data example and build a strong interaction into it. To properly visualize the data, we have to put one variable into bins. However, the scatter plot allows for an intuitive understanding of the interaction of two continuous variables.\n",
      "#  lin effects + interaction + random error\n",
      "\n",
      "As can be seen, the interaction effect is build into the example data, by simply multiplying two variables. Lets see, if we can recover the coefficients. We will also see that the model fit is better when the interaction term is included:\n",
      "### some considerations\n",
      "\n",
      "Let's assume, we have a data set with 70 different variables. Since we do not know anything about the relationsship of the variables to the dependent variable (y), nor of the variables among each other, we are inclined to construct a lot of new variables:\n",
      "\n",
      "* besides quadratic and cubic transformations, there my be other transformations leading to better results, like the log-transform.\n",
      "\n",
      "\n",
      "\n",
      "As you can see, the number of possible variables can grow very fast, when considering all possible effects that might be present in the data. Sometimes, there exists second-order interaction effects, that were not mentioned in the considerations above.<br>\n",
      "\n",
      "# How confident are we about our predictions\n",
      "\n",
      "Essentially, there are two questions that one could ask after fitting a linear model:\n",
      "\n",
      "* How confident are we about the predictions? This is asked in machine learning because we want to apply the model to unseen data and use the predictions in our business process.\n",
      "\n",
      "    Here, we have to deal with two different questions:\n",
      "\n",
      "    * How much will the mean response, our prediction - the regression line - vary?\n",
      "\n",
      "\n",
      "\n",
      "## Recap of assumptions underlying regression\n",
      "\n",
      "\n",
      "\n",
      "Now, with respect to our confidence need:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# data example\n",
      "\n",
      "# the x (small x) is just for plotting purpose\n",
      "\n",
      "### figure for linear plot\n",
      "\n",
      "The same plot is derived for an equation including a quadratic term:\n",
      "# for plotting:\n",
      "\n",
      "### figure for linear plot\n",
      "\n",
      "## Bootstrap\n",
      "\n",
      "# Extension: logistic regression and the GLM\n",
      "\n",
      "linear regression beyond models with normal error distributions. This\n",
      "\n",
      "remark in the corresponding wiki-article is enlightening:\n",
      "\n",
      "\n",
      "\n",
      "## exponential family of distributions\n",
      "\n",
      "From the perspective of modern statistics the GLM comprises many\n",
      "\n",
      "different linear models, among others the classical linear model. Every\n",
      "\n",
      "distribution in the exponential family can be written in the following\n",
      "\n",
      "form:\n",
      "\n",
      "parameters. In short: it is this function that linearizes the relation\n",
      "\n",
      "between the dependent and the independent variables. For the sake of\n",
      "\n",
      "dispersion parameter.\n",
      "\n",
      "\n",
      "\n",
      "### Normal distribution\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "function is given by the identity function. Hence, the mean can be\n",
      "\n",
      "linear regression.\n",
      "\n",
      "\n",
      "\n",
      "### Poisson distribution\n",
      "\n",
      "Now, for the Poisson distribution we have\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "distribution does not have any dispersion parameter.\n",
      "\n",
      "\n",
      "\n",
      "And finally the Bernoulli distribution from which we derive the logistic\n",
      "\n",
      "\n",
      "\n",
      "Next, I demonstrate how we can rewrite the Bernoulli distribution to fit into the framework of the exponential family:\n",
      "\n",
      "\n",
      "\n",
      "is also called the logit function whose reverse function is the logistic\n",
      "\n",
      "function. Hence, it is the logit that is modeled by a lineare function\n",
      "\n",
      "of the regressors:\n",
      "\n",
      "the right hand term into the logistic function we get the estimated\n",
      "\n",
      "probabilities:\n",
      "\n",
      "\n",
      "\n",
      "Here, I showed that the classical linear regression with normal\n",
      "\n",
      "error terms can be seen as a special case of a much wider family of\n",
      "\n",
      "models comprising all distributions out of the exponential family. (For\n",
      "\n",
      "a more complete treatment of other distributions see again\n",
      "\n",
      "\n",
      "\n",
      "# GLMNET\n",
      "\n",
      "In the statistical language R, there exists a library called 'glmnet'. This package implements the elastic net as we discussed here but for the glm and not only for the classical linear regession.<br>\n",
      "\n",
      "\n",
      "\n",
      "There are some subtleties in the implementation that are different from the elastic net version as provided by sklearn.\n",
      "\n",
      "https://pypi.org/project/glmnet-python/\n",
      "\n",
      "\n",
      "\n",
      "# Neural Network\n",
      "\n",
      "\n",
      "\n",
      "## classical linear regression\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## logistic regression\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For logistic regression, the activation function is changed. Now, it is not the identity function, but the logistic function:\n",
      "\n",
      "This function approaches 0, 1 asymptotically.\n",
      "\n",
      "\n",
      "\n",
      "### Weight decay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regexp = re.compile(r'\\\\|\\$|\\\\{|\\=|\\_|\\%')\n",
    "\n",
    "# for cell in [c for c in data_nb['cells'] if c['cell_type'] in ['code', 'markdown']]:\n",
    "for cell in data_nb['cells']:\n",
    "    if cell['cell_type'] in['code', 'markdown']:\n",
    "        for item in cell ['source']:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                if item.startswith('#') and not regexp.search(item):\n",
    "                    print(item) # translaltion is done here\n",
    "                else:\n",
    "                   continue # leave as is\n",
    "    \n",
    "            elif cell['cell_type'] == 'markdown':\n",
    "        \n",
    "                if not regexp.search(item):\n",
    "                     print(item)\n",
    "    else:\n",
    "        print(cell)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where $L$ is the actual loss and $w_i$ are the weights of the incoming connections of a neuron.\n"
     ]
    }
   ],
   "source": [
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
