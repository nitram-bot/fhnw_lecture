{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4de0930",
   "metadata": {},
   "source": [
    "# autogluon.tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad78305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23199/3159152723.py:11: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  X=X[set(X.columns.values.tolist())]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_csv('../data/train.csv', sep=\",\")\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "import sklearn\n",
    "y = train['SalePrice']\n",
    "X = train.drop('SalePrice', axis=1)\n",
    "X=X[set(X.columns.values.tolist())]\n",
    "categorical = [var for var in X.columns if X[var].dtype=='O']\n",
    "numerical = [var for var in X.columns if X[var].dtype!='O']\n",
    "X[categorical] = X[categorical].fillna('None')\n",
    "\n",
    "# auto-sklearn can not deal with categorical variables\n",
    "X= pd.concat([pd.get_dummies(X[categorical], dummy_na=True), X[numerical]], axis=1)\n",
    "\n",
    "y = np.log1p(y)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727a8596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230421_121610/\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230421_121610/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.9.16\n",
      "Operating System:   Linux\n",
      "Platform Machine:   aarch64\n",
      "Platform Version:   #1 SMP PREEMPT Tue Sep 13 07:51:32 UTC 2022\n",
      "Train Data Rows:    1168\n",
      "Train Data Columns: 347\n",
      "Label Column: SalePrice\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.521140839642674, 10.460270761075149, 12.03066, 0.39061)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12960.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.71 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 265 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 45): ['BsmtQual_nan', 'GarageCond_nan', 'GarageType_nan', 'HeatingQC_nan', 'KitchenQual_nan', 'BldgType_nan', 'Exterior2nd_nan', 'SaleType_nan', 'PoolQC_nan', 'GarageQual_nan', 'ExterQual_nan', 'Exterior1st_nan', 'ExterCond_nan', 'SaleCondition_nan', 'FireplaceQu_nan', 'BsmtFinType1_nan', 'Street_nan', 'MiscFeature_nan', 'Neighborhood_nan', 'MasVnrType_nan', 'Heating_nan', 'BsmtExposure_nan', 'Condition1_nan', 'HouseStyle_nan', 'RoofMatl_Membran', 'RoofMatl_nan', 'Alley_nan', 'GarageFinish_nan', 'LandSlope_nan', 'Utilities_nan', 'LotShape_nan', 'BsmtCond_nan', 'Electrical_Mix', 'Electrical_nan', 'Fence_nan', 'MSZoning_nan', 'CentralAir_nan', 'LotConfig_nan', 'RoofStyle_nan', 'Functional_nan', 'PavedDrive_nan', 'BsmtFinType2_nan', 'Condition2_nan', 'LandContour_nan', 'Foundation_nan']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) :   3 | ['MasVnrArea', 'LotFrontage', 'GarageYrBlt']\n",
      "\t\t('int', [])   : 299 | ['BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_None', 'BsmtQual_TA', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :   3 | ['MasVnrArea', 'LotFrontage', 'GarageYrBlt']\n",
      "\t\t('int', [])       :  34 | ['TotRmsAbvGrd', 'MoSold', 'TotalBsmtSF', 'ScreenPorch', 'HalfBath', ...]\n",
      "\t\t('int', ['bool']) : 265 | ['BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_None', 'BsmtQual_TA', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t302 features in original data used to generate 302 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.66 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 199.84s of the 299.83s of remaining time.\n",
      "\t-0.2314\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 199.8s of the 299.8s of remaining time.\n",
      "\t-0.2298\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 199.77s of the 299.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 199.73s of the 299.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 199.68s of the 299.68s of remaining time.\n",
      "\t-0.1452\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 198.36s of the 298.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==0.7.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 198.32s of the 298.31s of remaining time.\n",
      "\t-0.1455\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 197.26s of the 297.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.7.0`. \n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 197.22s of the 297.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==0.7.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 197.17s of the 297.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 1.6.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 197.13s of the 297.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Repeating k-fold bagging: 2/20\n",
      "Repeating k-fold bagging: 3/20\n",
      "Repeating k-fold bagging: 4/20\n",
      "Repeating k-fold bagging: 5/20\n",
      "Repeating k-fold bagging: 6/20\n",
      "Repeating k-fold bagging: 7/20\n",
      "Repeating k-fold bagging: 8/20\n",
      "Repeating k-fold bagging: 9/20\n",
      "Repeating k-fold bagging: 10/20\n",
      "Repeating k-fold bagging: 11/20\n",
      "Repeating k-fold bagging: 12/20\n",
      "Repeating k-fold bagging: 13/20\n",
      "Repeating k-fold bagging: 14/20\n",
      "Repeating k-fold bagging: 15/20\n",
      "Repeating k-fold bagging: 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repeating k-fold bagging: 17/20\n",
      "Repeating k-fold bagging: 18/20\n",
      "Repeating k-fold bagging: 19/20\n",
      "Repeating k-fold bagging: 20/20\n",
      "Completed 20/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.83s of the 297.06s of remaining time.\n",
      "\t-0.1412\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 297.01s of the 297.01s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 296.97s of the 296.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 296.93s of the 296.92s of remaining time.\n",
      "\t-0.1362\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 295.48s of the 295.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==0.7.0`.\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 295.43s of the 295.43s of remaining time.\n",
      "\t-0.1349\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 294.35s of the 294.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.7.0`. \n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 294.31s of the 294.3s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==0.7.0`.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 294.26s of the 294.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 1.6.\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 294.21s of the 294.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.7.0`.\n",
      "Repeating k-fold bagging: 2/20\n",
      "Repeating k-fold bagging: 3/20\n",
      "Repeating k-fold bagging: 4/20\n",
      "Repeating k-fold bagging: 5/20\n",
      "Repeating k-fold bagging: 6/20\n",
      "Repeating k-fold bagging: 7/20\n",
      "Repeating k-fold bagging: 8/20\n",
      "Repeating k-fold bagging: 9/20\n",
      "Repeating k-fold bagging: 10/20\n",
      "Repeating k-fold bagging: 11/20\n",
      "Repeating k-fold bagging: 12/20\n",
      "Repeating k-fold bagging: 13/20\n",
      "Repeating k-fold bagging: 14/20\n",
      "Repeating k-fold bagging: 15/20\n",
      "Repeating k-fold bagging: 16/20\n",
      "Repeating k-fold bagging: 17/20\n",
      "Repeating k-fold bagging: 18/20\n",
      "Repeating k-fold bagging: 19/20\n",
      "Repeating k-fold bagging: 20/20\n",
      "Completed 20/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 299.83s of the 294.15s of remaining time.\n",
      "\t-0.1337\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.9s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230421_121610/\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "X_train = pd.concat([X_train, y_train], axis=1)\n",
    "X_test = pd.concat([X_test, y_test], axis=1)\n",
    "train_data = TabularDataset(X_train)\n",
    "test_data = TabularDataset(X_test)\n",
    "predictor = TabularPredictor(label='SalePrice',eval_metric = 'root_mean_squared_error').fit(train_data, time_limit=300,\n",
    "                                                   presets='best_quality',auto_stack=True,)  # For a fair comparison with auto-sklearn\n",
    "# leaderboard = predictor.leaderboard(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c957a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.1363192172330769\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1363192172330769,\n",
      "    \"mean_squared_error\": -0.018582928987038814,\n",
      "    \"mean_absolute_error\": -0.09058484638885887,\n",
      "    \"r2\": 0.9004188921159217,\n",
      "    \"pearsonr\": 0.9499610277743544,\n",
      "    \"median_absolute_error\": -0.05549754284556929\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_test = test_data['SalePrice']  # values to predict\n",
    "test_data_nolab = test_data.drop(columns=['SalePrice'])  # delete label column to prove we're not cheating\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b632c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                    model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3  -0.133724       0.296334  2.999137                0.000138           0.022182            3       True          8\n",
      "1    ExtraTreesMSE_BAG_L2  -0.134864       0.223110  2.018322                0.071800           0.599221            2       True          7\n",
      "2  RandomForestMSE_BAG_L2  -0.136211       0.224396  2.377734                0.073086           0.958633            2       True          6\n",
      "3     WeightedEnsemble_L2  -0.141171       0.149018  1.451817                0.000154           0.036620            2       True          5\n",
      "4  RandomForestMSE_BAG_L1  -0.145216       0.072193  0.837244                0.072193           0.837244            1       True          3\n",
      "5    ExtraTreesMSE_BAG_L1  -0.145484       0.074117  0.574159                0.074117           0.574159            1       True          4\n",
      "6   KNeighborsDist_BAG_L1  -0.229760       0.002554  0.003794                0.002554           0.003794            1       True          2\n",
      "7   KNeighborsUnif_BAG_L1  -0.231418       0.002446  0.003904                0.002446           0.003904            1       True          1\n",
      "Number of models trained: 8\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_RF', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_XT', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     :   3 | ['MasVnrArea', 'LotFrontage', 'GarageYrBlt']\n",
      "('int', [])       :  34 | ['TotRmsAbvGrd', 'MoSold', 'TotalBsmtSF', 'ScreenPorch', 'HalfBath', ...]\n",
      "('int', ['bool']) : 265 | ['BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_None', 'BsmtQual_TA', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/automl/lib/python3.9/site-packages/autogluon/core/utils/plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary(show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51048f13",
   "metadata": {},
   "source": [
    "# Scikit-Optimize\n",
    "\n",
    "Probably you'll have to reload the notebook for the changes being in place. Scikit-Optimize works only with sklearn 0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188eab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-optimize) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-optimize) (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-optimize) (1.9.3)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-optimize) (1.24.2)\n",
      "Requirement already satisfied: PyYAML in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/martin/miniconda3/envs/automl/lib/python3.9/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fba16dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23199/1253800161.py:20: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  X_train.loc[:, numerical] = numeric_transformer.fit_transform(X_train.loc[:, numerical])\n",
      "/tmp/ipykernel_23199/1253800161.py:21: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  X_test.loc[:, numerical] = numeric_transformer.fit_transform(X_test.loc[:, numerical])\n",
      "/home/martin/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/searchcv.py:300: UserWarning: The `iid` parameter has been deprecated and will be ignored.\n",
      "  warnings.warn(\"The `iid` parameter has been deprecated \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingRegressor\n\u001b[1;32m     27\u001b[0m regressor \u001b[38;5;241m=\u001b[39m BayesSearchCV(\n\u001b[1;32m     28\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(),\n\u001b[1;32m     29\u001b[0m       search_spaces \u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m   )\n\u001b[0;32m---> 42\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     n_points_adjusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 512\u001b[0m     optim_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points_adjusted\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n_points\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/searchcv.py:400\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m\"\"\"Generate n_jobs parameters and evaluate them in parallel.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# get parameter values to evaluate\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# convert parameters to python native types\u001b[39;00m\n\u001b[1;32m    403\u001b[0m params \u001b[38;5;241m=\u001b[39m [[np\u001b[38;5;241m.\u001b[39marray(v)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:395\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[0;34m(self, n_points, strategy)\u001b[0m\n\u001b[1;32m    393\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_points):\n\u001b[0;32m--> 395\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    398\u001b[0m     ti_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macq_func \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(opt\u001b[38;5;241m.\u001b[39myi) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.ask\u001b[0;34m(self, n_points, strategy)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"Query point or multiple points at which objective should be evaluated.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03mn_points : int or None, default: None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m supported_strategies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_min\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_max\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(n_points, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m n_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:434\u001b[0m, in \u001b[0;36mOptimizer._ask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# this will not make a copy of `self.rng` and hence keep advancing\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# our random state.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;66;03m# The samples are evaluated starting form initial_samples[0]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples[\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_initial_points]\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/space.py:900\u001b[0m, in \u001b[0;36mSpace.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    897\u001b[0m columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensions:\n\u001b[0;32m--> 900\u001b[0m     columns\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# Transpose\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _transpose_list_array(columns)\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/space.py:698\u001b[0m, in \u001b[0;36mCategorical.rvs\u001b[0;34m(self, n_samples, random_state)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_transform([(choices)])\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices]\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/space.py:685\u001b[0m, in \u001b[0;36mCategorical.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m   original space.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;66;03m# The concatenation of all transformed dimensions makes Xt to be\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;66;03m# of type float, hence the required cast back to int.\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m inv_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inv_transform, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    687\u001b[0m     inv_transform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inv_transform)\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/space.py:168\u001b[0m, in \u001b[0;36mDimension.inverse_transform\u001b[0;34m(self, Xt)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, Xt):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;124;03m\"\"\"Inverse transform samples from the warped space back into the\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m       original space.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/transformers.py:309\u001b[0m, in \u001b[0;36mPipeline.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 309\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/skopt/space/transformers.py:275\u001b[0m, in \u001b[0;36mNormalize.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    273\u001b[0m X_orig \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_int:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mround(X_orig)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_orig\n",
      "File \u001b[0;32m~/miniconda3/envs/automl/lib/python3.9/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import skopt\n",
    "import importlib\n",
    "import sklearn\n",
    "display(skopt.__version__)\n",
    "importlib.reload(sklearn)\n",
    "display(sklearn.__version__)\n",
    "\n",
    "# Since BayesSearchCV can not deal with missing values, we have to impute them before:\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "numeric_transformer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_train.loc[:, numerical] = numeric_transformer.fit_transform(X_train.loc[:, numerical])\n",
    "X_test.loc[:, numerical] = numeric_transformer.fit_transform(X_test.loc[:, numerical])\n",
    "\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regressor = BayesSearchCV(\n",
    "    estimator = GradientBoostingRegressor(),\n",
    "      search_spaces ={\n",
    "         'learning_rate': Real(0.1,0.3),\n",
    "         'loss': Categorical(['ls']),\n",
    "         'max_depth': Integer(3,6),\n",
    "         'n_estimators': Integer(100, 600),\n",
    "         'subsample': Real(0.6, 1.0),\n",
    "         'max_features': Real(0.6, 1.0) \n",
    "      },\n",
    "    n_iter=64,\n",
    "    random_state=0,\n",
    "    verbose=1, iid=True,\n",
    "    cv=5, n_jobs=-1\n",
    "  )\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14149b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(X_test)\n",
    "print(\"mean-squared-error:\", sklearn.metrics.mean_squared_error(y_test, predictions, squared=False))\n",
    "\n",
    "# TPOT (Tree-based Pipeline Optimization Tool)\n",
    "\n",
    "!pip install tpot\n",
    "\n",
    "the steps below require a new-start of the notebook - if not the changes are not in place\n",
    "\n",
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import load_digits \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42, n_jobs=-1)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(np.sqrt(-tpot.score(X_test, y_test)))\n",
    "\n",
    "# FLAML\n",
    "\n",
    "!pip install flaml\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_csv('../data/train.csv', sep=\",\")\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "import sklearn\n",
    "y = train['SalePrice']\n",
    "X = train.drop('SalePrice', axis=1)\n",
    "y = np.log1p(y)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "from flaml import AutoML\n",
    "from flaml.default import LGBMRegressor\n",
    "automl = AutoML()\n",
    "# X_train, X_test, y_train, y_test\n",
    "automl.fit(X_train, y_train, task=\"regression\", estimator_list=[\"lgbm\", \"rf\", \"extra_tree\"],\n",
    "           train_time_limit=60,\n",
    "           time_budget=600,\n",
    "           n_jobs=-1,\n",
    "          ## ensemble=dict(final_estimator= LGBMRegressor(),\n",
    "          ## passthrough = True), \n",
    "           log_file_name='flaml_.log', log_type='all')\n",
    "\n",
    "print(automl.best_estimator)\n",
    "\n",
    "print(automl.best_config)\n",
    "\n",
    "print(automl.best_config_per_estimator)\n",
    "\n",
    "from flaml.data import get_output_from_log\n",
    "\n",
    "time_history, best_valid_loss_history, valid_loss_history, config_history, metric_history = \\\n",
    "    get_output_from_log(filename='flaml_logfile', time_budget=120)\n",
    "\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Wall Clock Time (s)\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.step(time_history, 1 - np.array(best_valid_loss_history), where=\"post\")\n",
    "plt.show()\n",
    "\n",
    "prediction = automl.predict(X_test)\n",
    "\n",
    "np.sqrt(np.mean((prediction - y_test)**2))\n",
    "\n",
    "# OPTUNA\n",
    "\n",
    "!pip install optuna\n",
    "\n",
    "import lightgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train = pd.read_csv('../data/train.csv', sep=\",\")\n",
    "categorical = [var for var in train.columns if train[var].dtype=='O']\n",
    "\n",
    "for cat_feat in categorical:\n",
    "    train[cat_feat] = train[cat_feat].astype('category')\n",
    "    \n",
    "y = train['SalePrice']\n",
    "X = train.drop('SalePrice', axis=1)\n",
    "y = np.log1p(y)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = \\\n",
    "        train_test_split(X_train, y_train, random_state=42, test_size=0.2)\n",
    "    \n",
    "    model = lightgbm.LGBMRegressor()\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"'neg_mean_squared_error'\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.1, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100, 400),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 4, 30),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\",0, 8),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "    }\n",
    "    model.set_params(**param)\n",
    "    clf = cross_val_score(\n",
    "    model,\n",
    "         x_tr, y_tr, scoring = 'neg_mean_squared_error'\n",
    "    )\n",
    "\n",
    "    return np.mean(np.sqrt(-clf))\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective , n_trials =70)\n",
    "trial = study.best_trial\n",
    "model = lightgbm.LGBMRegressor()\n",
    "model.set_params(**trial.params)\n",
    "for k,v in trial.params.items():\n",
    "    print(f'{k}: {v}')\n",
    "model.fit(X_train, y_train)\n",
    "print(f'result on hold-out set after HPO: {np.sqrt(mean_squared_error(y_test, model.predict(X_test)))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:automl]",
   "language": "python",
   "name": "conda-env-automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
