%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        

\title{5th Session}
\date{Dec 16, 2022}
\release{}
\author{Martin Biehler}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Der} \PYG{n}{Input} \PYG{n}{diese} \PYG{n}{Skripts} \PYG{n}{ist} \PYG{n}{anlässlich} \PYG{n}{eines} \PYG{n}{internen} \PYG{n}{workshops} \PYG{n}{mit} \PYG{n}{Finnova} \PYG{n}{entstanden}\PYG{o}{.} \PYG{n}{Im} \PYG{n}{wesentlichen} \PYG{n}{basiert} \PYG{n}{es} \PYG{n}{auf} \PYG{n}{Ausschnitten} \PYG{n}{eines} \PYG{n}{Seminars}\PYG{p}{,} \PYG{n}{das} \PYG{n}{ich} \PYG{n}{zuletzt} \PYG{n}{im} \PYG{n}{Sommersemester} \PYG{l+m+mi}{2022} \PYG{n}{an} \PYG{n}{der} \PYG{n}{FHNW} \PYG{n}{gegeben} \PYG{n}{habe}\PYG{o}{.}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data Modelling \& Cross Validation}

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Data Modelling for data with temporal context}
\label{\detokenize{dependent_data:data-modelling-for-data-with-temporal-context}}\label{\detokenize{dependent_data::doc}}
\sphinxAtStartPar
\sphinxincludegraphics{{banking_recommender1}.png}

\sphinxAtStartPar
\sphinxincludegraphics{{banking_recommender2}.png}


\chapter{Overfitting and Cross\sphinxhyphen{}Validation}
\label{\detokenize{dependent_data:overfitting-and-cross-validation}}
\sphinxAtStartPar
All classification and regression algorithms are prone to overfitting:
The algorithms learn pecularities of the train\sphinxhyphen{}data, that are not present in the real\sphinxhyphen{}world data.
When over\sphinxhyphen{}fitted, the algorithms are not generalizing to the real data.

\sphinxAtStartPar
\sphinxstylestrong{Capacity} refers to the ratio of free parameters and the amount of training data.

\noindent\sphinxincludegraphics{{dependent_data_5_0}.png}


\chapter{Cross\sphinxhyphen{}Validation}
\label{\detokenize{dependent_data:cross-validation}}
\sphinxAtStartPar
In most real\sphinxhyphen{}word applications we do not know the data universe, i.e. we do not know all possible data points that might be there. Our training data is possibly just a biased subsample of the population.
When we fit our algorithm to such a subsample its performance will degrade, when applied to new, unseen data points. In order to have an idea, how well our algorithm will perform in such cases, we can use a cross\sphinxhyphen{}validation scheme:
In the example below, a 5\sphinxhyphen{}fold cross\sphinxhyphen{}validation is illustrated.
\begin{itemize}
\item {} 
\sphinxAtStartPar
split the training data in 5 equal sized parts. In \sphinxstyleemphasis{sklearn} you can choose \sphinxstyleemphasis{StratifiedKFold}, that essentially tries to keep the percentages of all classes stable within each fold.

\item {} 
\sphinxAtStartPar
train your algorithmm on 4 folds and classify data in the 5th hold\sphinxhyphen{}out fold. Keep the performance on this fold.

\item {} 
\sphinxAtStartPar
repeat the last step 4 more times and use each time another fold as your hold\sphinxhyphen{}out fold.

\item {} 
\sphinxAtStartPar
at the end, you have 5 independent estimates of your algorithm’s performance

\item {} 
\sphinxAtStartPar
compute the mean of theses 5 estimates for an overall estimate

\end{itemize}

\noindent\sphinxincludegraphics{{dependent_data_7_0}.png}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{data leakage \& dependent data}

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Date Leakage and Dependent Data}
\label{\detokenize{dependent_data:date-leakage-and-dependent-data}}
\noindent\sphinxincludegraphics{{dependent_data_11_0}.png}


\chapter{Sources of data leakage}
\label{\detokenize{dependent_data:sources-of-data-leakage}}

\section{train data contains features that are not available in production}
\label{\detokenize{dependent_data:train-data-contains-features-that-are-not-available-in-production}}
\sphinxAtStartPar
e.g., the row\sphinxhyphen{}number contains information about the target: first come the negative examples, the positive cases were then simply inserted underneath.


\section{future data somehow slipped into the training set}
\label{\detokenize{dependent_data:future-data-somehow-slipped-into-the-training-set}}
\sphinxAtStartPar
e.g. Giba’s property:
\sphinxhref{https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61329}{taken from kaggle}

\noindent\sphinxincludegraphics{{dependent_data_14_0}.png}

\sphinxAtStartPar
and here is the mentioned data\sphinxhyphen{}structure:
\sphinxhref{https://www.kaggle.com/rebeccaysteboe/giba-s-property-extended-result}{this kernel exploits the leakage}

\noindent\sphinxincludegraphics{{dependent_data_16_0}.png}


\section{there is one feature that interacts with the target}
\label{\detokenize{dependent_data:there-is-one-feature-that-interacts-with-the-target}}
\sphinxAtStartPar
taken from \sphinxhref{http://kdd.org/exploration\_files/KDDCup08-P1.pdf}{Breast Cancer Identification: KDD CUP Winner’s Report}
Distribution of malignant (black) and benign (gray) candidates depending on patient ID on the X\sphinxhyphen{}axis in log scale.

\noindent\sphinxincludegraphics{{dependent_data_18_0}.png}


\section{Some more cases where we have data leakage:}
\label{\detokenize{dependent_data:some-more-cases-where-we-have-data-leakage}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Customer advisor has a long call with customer and finally sells the product that is shipped only two weeks later. Variables ‘last advisory contact’ and ‘length of call’ certainly anticipate the product sale. When an algorithm learns to predict product propensity based on ‘last advisor contact’ it will ultimately suggest customers to the advisors for whom the advisor has already closed the deal.

\item {} 
\sphinxAtStartPar
Train and test data is normalized with common sample statistics belonging to the whole data set
\begin{itemize}
\item {} 
\sphinxAtStartPar
target encoding is dangerous: we will talk about it later on

\item {} 
\sphinxAtStartPar
stacking is dangerous: we will discuss this topic as well

\end{itemize}

\end{itemize}


\section{Example: credit card applications}
\label{\detokenize{dependent_data:example-credit-card-applications}}
\sphinxAtStartPar
This data example was used in “Econometric Analysis” (William H. Greene) without the author noticing the problem:
\sphinxhref{https://www.kaggle.com/dansbecker/data-leakage}{example taken from here}
\begin{itemize}
\item {} 
\sphinxAtStartPar
card: Dummy variable, 1 if application for credit card accepted, 0 if not

\item {} 
\sphinxAtStartPar
reports: Number of major derogatory reports

\item {} 
\sphinxAtStartPar
age: Age n years plus twelfths of a year

\item {} 
\sphinxAtStartPar
income: Yearly income (divided by 10,000)

\item {} 
\sphinxAtStartPar
share: Ratio of monthly credit card expenditure to yearly income

\item {} 
\sphinxAtStartPar
expenditure: Average monthly credit card expenditure

\item {} 
\sphinxAtStartPar
owner: 1 if owns their home, 0 if rent

\item {} 
\sphinxAtStartPar
selfempl: 1 if self employed, 0 if not.

\item {} 
\sphinxAtStartPar
dependents: 1 + number of dependents

\item {} 
\sphinxAtStartPar
months: Months living at current address

\item {} 
\sphinxAtStartPar
majorcards: Number of major credit cards held

\item {} 
\sphinxAtStartPar
active: Number of active credit accounts

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{n}{url} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/YoshiKitaguchi/Credit\PYGZhy{}card\PYGZhy{}verification\PYGZhy{}project/master/AER\PYGZus{}credit\PYGZus{}card\PYGZus{}data.csv}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{url}\PYG{p}{,} \PYG{n}{error\PYGZus{}bad\PYGZus{}lines}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{true\PYGZus{}values} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{false\PYGZus{}values} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{no}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   card  reports       age  income     share  expenditure  owner  selfemp  \PYGZbs{}
0  True        0  37.66667  4.5200  0.033270   124.983300   True    False   
1  True        0  33.25000  2.4200  0.005217     9.854167  False    False   
2  True        0  33.66667  4.5000  0.004156    15.000000   True    False   
3  True        0  30.50000  2.5400  0.065214   137.869200  False    False   
4  True        0  32.16667  9.7867  0.067051   546.503300   True    False   

   dependents  months  majorcards  active  
0           3      54           1      12  
1           3      34           1      13  
2           4      58           1       5  
3           0      25           1       7  
4           2      64           1       5  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}pipeline}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{cross\PYGZus{}val\PYGZus{}score}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{lightgbm}

\PYG{n}{y} \PYG{o}{=} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{card}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{card}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{lightgbm}\PYG{o}{.}\PYG{n}{LGBMClassifier}\PYG{p}{(}\PYG{n}{boosting\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gbdt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}leaves}\PYG{o}{=}\PYG{l+m+mi}{31}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{subsample\PYGZus{}for\PYGZus{}bin}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,} \PYG{n}{objective}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                                \PYG{n}{subsample}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{subsample\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{colsample\PYGZus{}bytree}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{silent}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{importance\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{split}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{is\PYGZus{}unbalance} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{scale\PYGZus{}pos\PYGZus{}weight} \PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{)}
\PYG{n}{model\PYGZus{}pipe} \PYG{o}{=} \PYG{n}{make\PYGZus{}pipeline}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{cv\PYGZus{}scores} \PYG{o}{=} \PYG{n}{cross\PYGZus{}val\PYGZus{}score}\PYG{p}{(}\PYG{n}{model\PYGZus{}pipe}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{scoring}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{cv\PYGZus{}scores}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.9765065099665862
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{inspection} \PYG{k+kn}{import} \PYG{n}{permutation\PYGZus{}importance}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{permutation\PYGZus{}importance}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,}
        \PYG{n}{n\PYGZus{}repeats}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,}
        \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{sorted\PYGZus{}idx} \PYG{o}{=} \PYG{n}{result}\PYG{o}{.}\PYG{n}{importances\PYGZus{}mean}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{result}\PYG{o}{.}\PYG{n}{importances}\PYG{p}{[}\PYG{n}{sorted\PYGZus{}idx}\PYG{p}{]}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,}
           \PYG{n}{vert}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{X}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{n}{sorted\PYGZus{}idx}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Permutation Importances}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{dependent_data_25_0}.png}

\noindent\sphinxincludegraphics{{dependent_data_26_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{display}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{y} \PYG{o}{==} \PYG{k+kc}{True}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{expenditure}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{y} \PYG{o}{==} \PYG{k+kc}{False}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{expenditure}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
238.60242068103616
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{display}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{y} \PYG{o}{==} \PYG{k+kc}{True}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{share}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{y} \PYG{o}{==} \PYG{k+kc}{False}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{share}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.08848152972453567
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.0004767954841216091
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{tree}
\PYG{k+kn}{from} \PYG{n+nn}{dtreeviz}\PYG{n+nn}{.}\PYG{n+nn}{trees} \PYG{k+kn}{import} \PYG{o}{*}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{classifier} \PYG{o}{=} \PYG{n}{tree}\PYG{o}{.}\PYG{n}{DecisionTreeClassifier}\PYG{p}{(}\PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} limit depth of tree}
\PYG{n}{classifier}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}


\PYG{n}{viz} \PYG{o}{=} \PYG{n}{dtreeviz}\PYG{p}{(}\PYG{n}{classifier}\PYG{p}{,} 
               \PYG{n}{X}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,}
               \PYG{n}{y}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} 
               \PYG{n}{target\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{credit\PYGZhy{}card application}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{feature\PYGZus{}names}\PYG{o}{=}\PYG{n}{X}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
               \PYG{n}{class\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{not\PYGZus{}accepted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accepted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
              \PYG{p}{)}  

\PYG{n}{viz}\PYG{o}{.}\PYG{n}{save}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{decision\PYGZus{}tree.svg}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{dependent_data_32_0}.png}


\section{Solution:}
\label{\detokenize{dependent_data:solution}}
\sphinxAtStartPar
Obviously,
\begin{itemize}
\item {} 
\sphinxAtStartPar
share: Ratio of monthly credit card expenditure to yearly income

\item {} 
\sphinxAtStartPar
expenditure: Average monthly credit card expenditure

\end{itemize}

\sphinxAtStartPar
are features that suppose the applicant was granted a credit card.


\chapter{Dependency between data\sphinxhyphen{}samples}
\label{\detokenize{dependent_data:dependency-between-data-samples}}
\sphinxAtStartPar
Training Machine Learning Algorithms works best, when we have many independent data samples in the training data. Dependent data arises when:
\begin{itemize}
\item {} 
\sphinxAtStartPar
we take repeatedly measures from the same individual (the trained algorithms will not generalize to other individuals)

\item {} 
\sphinxAtStartPar
we take samples only from one bank (the socia\sphinxhyphen{}demographic structure of GKB’s customers might be differentfrom that of BCG’s customers \sphinxhyphen{} as a result the algorithm will badly generalize)

\end{itemize}


\section{Some more cases where we have dependent data}
\label{\detokenize{dependent_data:some-more-cases-where-we-have-dependent-data}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Repeatedly sampling data from the same individual:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Fraud: A fraudster commits many frauds that have a similiar pattern; For example for every fraud commited, the fraudster uses a different account of the same bank in Thailand. When doing cross\sphinxhyphen{}validation we have frauds related to the very bank in Thailand in the training set as well as in the test set. Hence, we will overestimate the capability of the trained classifier to generalize to new, unseen fraud cases. But it will be very efficient to detect this one fraudster with bank accounts in Thailand.

\item {} 
\sphinxAtStartPar
customer journey: to detect an event as soon as possible, data is sampled with different offsets before the event’s occurence. When trying to predict an event we could be tempted to sample data from different points in time before the event. This data will always be very similar and is hence dependent. For example, medical health records are not changing very fast and blood pressure two months ago will be similar to that measured one month ago. Most bank accounts have a similar balance in a one months distance.

\item {} 
\sphinxAtStartPar
classifying websites: social media websites belong all to facebook. There are just not enough social media websites to learn something about them in the training set and generalize to other social media websites in the test set.

\end{itemize}

\item {} 
\sphinxAtStartPar
Train and test data is normalized with common sample statistics belonging to the whole data set
\begin{itemize}
\item {} 
\sphinxAtStartPar
target encoding is dangerous: we will talk about it later on

\item {} 
\sphinxAtStartPar
stacking is dangerous: we will discuss this topic as well

\end{itemize}

\end{itemize}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sentence Classification: sentences belonging to the same document
\begin{itemize}
\item {} 
\sphinxAtStartPar
customer churn: An angry customer sends frequent e\sphinxhyphen{}mails. All e\sphinxhyphen{}mails happen to have the same characteristics, e.g. instead of the \sphinxstyleemphasis{Umlaut} ‘ü’, the customer uses ‘ue’. The algorithm might be tempted to learn that ‘ue’ is a special churn\sphinxhyphen{}characteristic. When half of the e\sphinxhyphen{}mails end up in the train set and the rest in the test set, we will overestimate the prediction accuracy of the learned algorithm.

\item {} 
\sphinxAtStartPar
When building a classifier to distinct medical publications from IT\sphinxhyphen{}related publications, it is important to have a representative sample of medical topics as well as tech\sphinxhyphen{}topics. When taking sentences from one document that is heavily Java related, the algorithm will struggle to generalize to the programming language Python. When the ‘Java\sphinxhyphen{}sentences’ are in the train set as well as in the test set, we will overestimate the algorithm’s performance on new documents.

\end{itemize}

\item {} 
\sphinxAtStartPar
Diagnosis: patient records coming from the same hospital
\begin{itemize}
\item {} 
\sphinxAtStartPar
Hospitals might have different specializations; When we want to predict diagnosis based on the doctors’ reports, cancer cases from a clinic specialized in cancer treatments might have higher similarity to each other than cancer cases coming from a orthopaedic hospital. When the reports of the specialized clinic end up in the train set as well as in the test set, we will overestimate the algorithm’s capability to correctly classifiy the diagnosis ‘cancer’.

\end{itemize}

\end{itemize}


\subsection{recent article summarizing errors when predicting COVID:}
\label{\detokenize{dependent_data:recent-article-summarizing-errors-when-predicting-covid}}
\sphinxAtStartPar
Excerpts of the article \sphinxhref{https://www.technologyreview.com/2021/07/30/1030329/machine-learning-ai-failed-covid-hospital-diagnosis-pandemic}{Hundreds of AI tools have been built to catch covid. None of them helped.}:
“They looked at 415 published tools and, like Wynants and her colleagues, concluded that none were fit for clinical use.”
“Both teams found that researchers repeated the same basic errors in the way they trained or tested their tools.”

\sphinxAtStartPar
“Many of the problems that were uncovered are linked to the poor quality of the data that researchers used to develop their tools.”
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{duplicates:} “Driggs highlights the problem of what he calls Frankenstein data sets, which are spliced together from multiple sources and can contain duplicates.”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{confounding variables:} 
\begin{itemize}
\item {} 
\sphinxAtStartPar
“Many unwittingly used a data set that contained chest scans of children who did not have covid as their examples of what non\sphinxhyphen{}covid cases looked like. But as a result, the AIs learned to identify kids, not covid.”

\item {} 
\sphinxAtStartPar
“Because patients scanned while lying down were more likely to be seriously ill, the AI learned wrongly to predict serious covid risk from a person’s position.”

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{different sources:} “In yet other cases, some AIs were found to be picking up on the text font that certain hospitals used to label the scans. As a result, fonts from hospitals with more serious caseloads became predictors of covid risk.”

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{human labeling error:} “It would be much better to label a medical scan with the result of a PCR test rather than one doctor’s opinion, says Driggs.”


\end{itemize}


\subsection{How to fix it?}
\label{\detokenize{dependent_data:how-to-fix-it}}\begin{itemize}
\item {} 
\sphinxAtStartPar
“Better data would help, but in times of crisis that’s a big ask.”

\item {} 
\sphinxAtStartPar
“’Until we buy into the idea that we need to sort out the unsexy problems before the sexy ones, we’re doomed to repeat the same mistakes,’ says Mateen.”


\end{itemize}

\sphinxAtStartPar
Original articles:
\sphinxhref{https://www.bmj.com/content/369/bmj.m1328}{Wynants et al., 2020. Prediction models for diagnosis and prognosis of covid\sphinxhyphen{}19: systematic review and critical appraisal}
\sphinxhref{https://www.nature.com/articles/s42256-021-00307-0}{Roberts et al., 2021. Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID\sphinxhyphen{}19 using chest radiographs and CT scans}


\subsection{Data leakage Literature}
\label{\detokenize{dependent_data:data-leakage-literature}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://static1.squarespace.com/static/5a4c161cfe54ef45b17aa18e/t/5ab4013b88251b7b684c6025/1521746286132/week2-part2.pdf}{examples of data leakage in competitions: start on page 19}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.coursera.org/lecture/competitive-data-science/basic-data-leaks-5w9Gy}{alternative to the text above here is the video}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.prem-melville.com/publications/medical-mining-dmkd09.pdf}{Medical data mining: insights from winning two competitions}

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{important}: if you’re not allowed to read any more ‘towardsdatascience’ article or ‘medium’ articles – just remove the cookies for the page (inspect \sphinxhyphen{}> applications \sphinxhyphen{}> cookies) and reload the page afterwards.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://towardsdatascience.com/data-leakage-with-hyper-parameter-tuning-c57ba2006046}{data leakage when tuning hyper\sphinxhyphen{}parameters}

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{imbalanced data (example in python)}

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Creditcard\sphinxhyphen{}Fraud: imbalanced data}
\label{\detokenize{imbalanced_data:creditcard-fraud-imbalanced-data}}\label{\detokenize{imbalanced_data::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}\PYG{p}{,} \PYG{n}{RandomForestClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{r2\PYGZus{}score}\PYG{p}{,} \PYG{n}{mean\PYGZus{}absolute\PYGZus{}error}\PYG{p}{,} \PYG{n}{mean\PYGZus{}squared\PYGZus{}error}\PYG{p}{,} \PYG{n}{f1\PYGZus{}score}\PYG{p}{,} \PYG{n}{accuracy\PYGZus{}score}\PYG{p}{,}\PYGZbs{}
                            \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{,} \PYG{n}{average\PYGZus{}precision\PYGZus{}score}\PYG{p}{,} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{,} \PYG{n}{auc}\PYG{p}{,}\PYGZbs{}
                            \PYG{n}{roc\PYGZus{}curve}\PYG{p}{,} \PYG{n}{precision\PYGZus{}score}\PYG{p}{,} \PYG{n}{recall\PYGZus{}score}

\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{KFold}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{cross\PYGZus{}val\PYGZus{}score}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{tree} \PYG{k+kn}{import} \PYG{n}{DecisionTreeClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{confusion\PYGZus{}matrix}
\PYG{k+kn}{from} \PYG{n+nn}{copy} \PYG{k+kn}{import} \PYG{n}{copy}
\PYG{k+kn}{from} \PYG{n+nn}{numpy} \PYG{k+kn}{import} \PYG{n}{random}

\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{config} InlineBackend.figure\PYGZus{}format = \PYGZsq{}png\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} https://www.kaggle.com/mlg\PYGZhy{}ulb/creditcardfraud}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(284807, 31)
\end{sphinxVerbatim}


\section{the data\sphinxhyphen{}set is too large \sphinxhyphen{} we subsample the majority class}
\label{\detokenize{imbalanced_data:the-data-set-is-too-large-we-subsample-the-majority-class}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{no\PYGZus{}fraud} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{fraud} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sampled} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{no\PYGZus{}fraud}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{fraud} \PYG{o}{+} \PYG{n}{sampled}\PYG{p}{,} \PYG{p}{]}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}subsampled.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{header}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}subsampled.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{num anomalies }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ = }\PYG{l+s+si}{\PYGZob{}}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
num anomalies 492/100492 = 0.49\PYGZpc{}
\end{sphinxVerbatim}


\chapter{what will our accuracy be ad hoc?}
\label{\detokenize{imbalanced_data:what-will-our-accuracy-be-ad-hoc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{crosstab}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{Class}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}true}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{Class}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}Actual}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
col\PYGZus{}0   0\PYGZus{}Actual
Class           
0\PYGZus{}true    100000
1\PYGZus{}true       492
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\text{accuracy} = \frac{\text{true positives + true negatives}}{\text{true negatives + false negatives + true positives + false positives}} = \frac{0 + 284315}{492 + 284315} = 0.998\)

\end{itemize}


\chapter{get a model}
\label{\detokenize{imbalanced_data:get-a-model}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{lightgbm}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{lightgbm}\PYG{o}{.}\PYG{n}{LGBMClassifier}\PYG{p}{(}\PYG{n}{boosting\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gbdt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{num\PYGZus{}leaves}\PYG{o}{=}\PYG{l+m+mi}{31}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{subsample\PYGZus{}for\PYGZus{}bin}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,} \PYG{n}{objective}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                                \PYG{n}{subsample}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{subsample\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{colsample\PYGZus{}bytree}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} 
                                \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{silent}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{importance\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{split}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{is\PYGZus{}unbalance} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{scale\PYGZus{}pos\PYGZus{}weight} \PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{train / test split}
\label{\detokenize{imbalanced_data:train-test-split}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}train} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}X\PYGZus{}train.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}X\PYGZus{}test.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}y\PYGZus{}train.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/creditcard\PYGZus{}y\PYGZus{}test.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} X, y = df.drop(\PYGZsq{}Class\PYGZsq{}, axis=1), df[\PYGZsq{}Class\PYGZsq{}]}
\PYG{c+c1}{\PYGZsh{} X\PYGZus{}train, X\PYGZus{}test, y\PYGZus{}train, y\PYGZus{}test = train\PYGZus{}test\PYGZus{}split(X, y, test\PYGZus{}size=0.20)}
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} X\PYGZus{}train.to\PYGZus{}csv(\PYGZdq{}../data/creditcard\PYGZus{}X\PYGZus{}train.csv\PYGZdq{}, header=True, index=False)}
\PYG{c+c1}{\PYGZsh{} X\PYGZus{}test.to\PYGZus{}csv(\PYGZdq{}../data/creditcard\PYGZus{}X\PYGZus{}test.csv\PYGZdq{}, header=True, index=False)}
\PYG{c+c1}{\PYGZsh{} y\PYGZus{}train.to\PYGZus{}csv(\PYGZdq{}../data/creditcard\PYGZus{}y\PYGZus{}train.csv\PYGZdq{}, header=True, index=False)}
\PYG{c+c1}{\PYGZsh{} y\PYGZus{}test.to\PYGZus{}csv(\PYGZdq{}../data/creditcard\PYGZus{}y\PYGZus{}test.csv\PYGZdq{}, header=True, index=False)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/home/martin/miniconda3/envs/imbalanced/lib/python3.7/site\PYGZhy{}packages/sklearn/utils/validation.py:63: DataConversionWarning: A column\PYGZhy{}vector y was passed when a 1d array was expected. Please change the shape of y to (n\PYGZus{}samples, ), for example using ravel().
  return f(*args, **kwargs)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LGBMClassifier(is\PYGZus{}unbalance=False, n\PYGZus{}estimators=500, objective=\PYGZsq{}binary\PYGZsq{},
               scale\PYGZus{}pos\PYGZus{}weight=1.0, subsample\PYGZus{}for\PYGZus{}bin=20000)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{display}\PYG{p}{(}\PYG{n}{y\PYGZus{}train}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
406
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
86
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[9.99999999e\PYGZhy{}01, 1.27675340e\PYGZhy{}09]])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n}{FPR}\PYG{p}{,} \PYG{n}{TPR}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{roc\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{non\PYGZhy{}Fraud}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fraud}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{non\PYGZhy{}Fraud}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fraud}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           non\PYGZhy{}Fraud  Fraud
non\PYGZhy{}Fraud      20012      1
Fraud             23     63
\end{sphinxVerbatim}

\sphinxAtStartPar
sensitivity, recall, hit rate, or true positive rate (TPR)
\label{equation:imbalanced_data:26634868-1db1-4f99-a3cf-b6a359b75b00}\begin{equation}
 \text{Precision}=\frac{\text{TP}}{\text{TP + FP}}
   \end{equation}\label{equation:imbalanced_data:ff0cda55-bfbb-4a85-bd64-2e68f9907604}\begin{equation} 
 \text{Recall}=\text{Sensitivity}=\text{TPR}=\frac{\text{TP}}{\text{P}} =\frac{\text{TP}}{\text{TP + FN}}
    \end{equation}\label{equation:imbalanced_data:d1b6ec80-1d0c-44c9-82c7-56714e45d90c}\begin{equation} 
 \text{F}_1\text{-Score} = 2\frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
     \end{equation}\label{equation:imbalanced_data:a75d568b-8fe7-4add-ad43-58cf0c2dd416}\begin{equation}   
  \text{FPR}=\frac{\text{FP}}{\text{N}} =\frac{\text{FP}}{\text{FP + TN}}
    \end{equation}
\sphinxAtStartPar
The normal ROC auc is computed for the True Positive Rate (TPR) and the False Positive Rate (FPR)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC AUC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.984, Recall 0.733
F1 0.84, Accuracy 0.9988
ROC AUC 0.96, AUC PR 0.82
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{FPR}\PYG{p}{,} \PYG{n}{TPR}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b.\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{markersize}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{FPR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TPR / Recall / Sensitivity}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{credit card \PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s2}{bf ROC\PYGZdl{} curve}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{savefig}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/tmp/ROC\PYGZhy{}curve.png}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{imbalanced_data_20_0}.png}

\sphinxAtStartPar
Because of the imbalance, the FPR has from the beginning on very low values. For imbalanced data, th ROC AUC of this curve is not ideal to compare different algorithms:
Suppose there are 100 positive cases in the data and 100’000 negative cases:
\label{equation:imbalanced_data:a703f845-1503-47c8-8aa5-20abac9d6af7}\begin{eqnarray}
\text{algorithm 1: 2000 false positives:  FPR } =\frac{\text{FP}}{\text{FP + TN}} = &\frac{2000}{2000 + 100000} = 0.0196\\
\text{algorithm 2:   200  false positives:  FPR } =\frac{\text{FP}}{\text{FP + TN}} = &\;\;\frac{200}{200+ 100000} \;=\; 0.001966
\end{eqnarray}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(array([2.0032e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 6.3000e+01]),
 array([1.84737102e\PYGZhy{}14, 1.00000000e\PYGZhy{}01, 2.00000000e\PYGZhy{}01, 3.00000000e\PYGZhy{}01,
        4.00000000e\PYGZhy{}01, 5.00000000e\PYGZhy{}01, 6.00000000e\PYGZhy{}01, 7.00000000e\PYGZhy{}01,
        8.00000000e\PYGZhy{}01, 9.00000000e\PYGZhy{}01, 1.00000000e+00]),
 \PYGZlt{}BarContainer object of 10 artists\PYGZgt{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{imbalanced_data_22_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b.\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{markersize}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{recall}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{precision}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{credit card \PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s2}{bf PR\PYGZdl{} curve}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{savefig}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/tmp/PR\PYGZhy{}curve.png}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{imbalanced_data_23_0}.png}

\sphinxAtStartPar
For imbalanced data, precision is the better measure:
Suppose there are 100 positive cases in the data and 100’000 negative cases:
Both algorithms classifiy 50 cases correctly:
\label{equation:imbalanced_data:da4de55a-ce2b-4286-8fa4-0ba0a1c808b4}\begin{eqnarray}
\text{algorithm 1: 2000 false positives:  }\; \text{Precision}=\frac{\text{TP}}{\text{TP + FP}} = &\frac{50}{50 + 2000} = 0.0243\\
\text{algorithm 2:   200  false positives:  }\; \text{Precision}=\frac{\text{TP}}{\text{TP + FP}} = &\;\;\frac{50}{50 + 200} \;=\; 0.2
\end{eqnarray}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} keep the workspace tidy}
\PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{confusion}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{confusion}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}
\end{sphinxVerbatim}


\chapter{proper test set?}
\label{\detokenize{imbalanced_data:proper-test-set}}
\sphinxAtStartPar
we can also do this with a sklearn data\sphinxhyphen{}set splitter:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{StratifiedShuffleSplit}
\PYG{n}{splitter} \PYG{o}{=} \PYG{n}{StratifiedShuffleSplit}\PYG{p}{(}\PYG{n}{n\PYGZus{}splits}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{train\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.8}\PYG{p}{)}
\PYG{n}{idx} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{splitter}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{df}\PYG{o}{.}\PYG{n}{Class}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{train\PYGZus{}idx} \PYG{o}{=} \PYG{n}{idx}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{test\PYGZus{}idx} \PYG{o}{=} \PYG{n}{idx}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{test\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
20099
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df\PYGZus{}train} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{train\PYGZus{}idx}\PYG{p}{]}
\PYG{n}{df\PYGZus{}test} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{test\PYGZus{}idx}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TRAIN num fraud }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ = }\PYG{l+s+si}{\PYGZob{}}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}train}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TEST num fraud }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{/}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ = }\PYG{l+s+si}{\PYGZob{}}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}test}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
TRAIN num fraud 394/80393 = 0.49\PYGZpc{}
TEST num fraud 98/20099 = 0.49\PYGZpc{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{df\PYGZus{}train}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{df\PYGZus{}test}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       F   T
F  19993   8
T     22  76
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC AUC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.905, Recall 0.776
F1 0.84, Accuracy 0.9985
ROC AUC 0.89, AUC PR 0.83
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} keep workspace tidy}
\PYG{n}{df}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{df\PYGZus{}train}\PYG{p}{,} \PYG{n}{df\PYGZus{}test}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,}\PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{df}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{df\PYGZus{}train}\PYG{p}{,} \PYG{n}{df\PYGZus{}test}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}
\end{sphinxVerbatim}


\chapter{Strategies for the imbalanced case:}
\label{\detokenize{imbalanced_data:strategies-for-the-imbalanced-case}}\begin{itemize}
\item {} 
\sphinxAtStartPar
oversample the minority class

\item {} 
\sphinxAtStartPar
undersample the majority class

\item {} 
\sphinxAtStartPar
do both of the former two suggestions

\item {} 
\sphinxAtStartPar
oversample only the cases that get missclassified

\item {} 
\sphinxAtStartPar
set \sphinxcode{\sphinxupquote{is\_unbalance}} and/or \sphinxcode{\sphinxupquote{scale\_pos\_weight}} parameters in lightgbm

\end{itemize}


\section{oversample fraud}
\label{\detokenize{imbalanced_data:oversample-fraud}}

\subsection{Synthetic Minority Oversampling Technique = SMOTE}
\label{\detokenize{imbalanced_data:synthetic-minority-oversampling-technique-smote}}
\sphinxAtStartPar
The \sphinxhref{https://arxiv.org/abs/1106.1813}{original paper is here}
The idea is to connect very close points in the feature space and generate new samples lying on the connecting line. This is done for the minority class only.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{imblearn}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{n}{SMOTE}\PYG{p}{,} \PYG{n}{ADASYN}
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov} \PYG{o}{=} \PYG{n}{SMOTE}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,}\PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       F   T
F  19999   2
T      0  98
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC AUC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.980, Recall 1.000
F1 0.99, Accuracy 0.9999
ROC AUC 1.00, AUC PR 1.00
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}
\end{sphinxVerbatim}


\subsection{Adaptive Synthetic Sampling = ADASYN}
\label{\detokenize{imbalanced_data:adaptive-synthetic-sampling-adasyn}}
\sphinxAtStartPar
The \sphinxhref{https://www.ele.uri.edu/faculty/he/PDFfiles/adasyn.pdf}{paper can be found here}.
More synthetic observations are generated in areas where the density of the minority class is very low. Those cases are hard to learn and upsampling them with SMOTE should make it easier to learn.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} needs too much RAM: }
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov} \PYG{o}{=} \PYG{n}{ADASYN}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,}\PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       F   T
F  20001   0
T      2  96
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC AUC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.919, Recall 0.806
F1 0.86, Accuracy 0.9987
ROC AUC 0.97, AUC PR 0.84
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}
\end{sphinxVerbatim}


\subsection{Borderline\sphinxhyphen{}SMOTE}
\label{\detokenize{imbalanced_data:borderline-smote}}
\sphinxAtStartPar
The \sphinxhref{https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf}{paper can be found here}
The principal idea is to upsample only those observations that are not classified correctly. The missclassified instances are found by k\sphinxhyphen{}nearest\sphinxhyphen{}neighbors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} needs to much RAM for me:}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{n}{BorderlineSMOTE}
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov} \PYG{o}{=} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,}\PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       F   T
F  19995   6
T     16  82
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC AUC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.932, Recall 0.837
F1 0.88, Accuracy 0.9989
ROC AUC 0.97, AUC PR 0.84
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}
\end{sphinxVerbatim}


\subsection{Support\sphinxhyphen{}Vector\sphinxhyphen{}Machine\sphinxhyphen{}SMOTE = SVMSMOTE}
\label{\detokenize{imbalanced_data:support-vector-machine-smote-svmsmote}}
\sphinxAtStartPar
The class\sphinxhyphen{}boundaries for the Borderline are learned as the support\sphinxhyphen{}vectors of a Support\sphinxhyphen{}Vector\sphinxhyphen{}Machine.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{n}{SVMSMOTE}
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ov} \PYG{o}{=} \PYG{n}{SVMSMOTE}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,}\PYG{n}{y\PYGZus{}train\PYGZus{}ov}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       F   T
F  19994   7
T     16  82
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Precision 0.921, Recall 0.837
F1 0.88, Accuracy 0.9989
ROC 0.97, AUC PR 0.86
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} keep the workspace tidy}
\PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ovm}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{clf} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{X\PYGZus{}train\PYGZus{}ov}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}ovm}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{clf}
\end{sphinxVerbatim}


\section{weigh minority class}
\label{\detokenize{imbalanced_data:weigh-minority-class}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{set\PYGZus{}params}\PYG{p}{(}\PYG{n}{is\PYGZus{}unbalance} \PYG{o}{=} \PYG{k+kc}{True}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}, scale\PYGZus{}pos\PYGZus{}weight}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{set\PYGZus{}params}\PYG{p}{(}\PYG{n}{is\PYGZus{}unbalance}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{scale\PYGZus{}pos\PYGZus{}weight}\PYG{o}{=}\PYG{l+m+mf}{200.0}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,}\PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} house\PYGZhy{}keeping}
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{k}{del} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}
\end{sphinxVerbatim}


\section{Undersample Fraud}
\label{\detokenize{imbalanced_data:undersample-fraud}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{under\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{n}{RandomUnderSampler}
\PYG{n}{X\PYGZus{}train\PYGZus{}u}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}u} \PYG{o}{=} \PYG{n}{RandomUnderSampler}\PYG{p}{(}\PYG{n}{sampling\PYGZus{}strategy}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,} \PYG{n}{X\PYGZus{}train\PYGZus{}u}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}u}\PYG{p}{,}\PYG{n}{y\PYGZus{}train\PYGZus{}u}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{X\PYGZus{}train\PYGZus{}u}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}u} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{,} \PYG{n}{X\PYGZus{}train\PYGZus{}u}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}u}
\end{sphinxVerbatim}


\section{do both: oversample minority class and undersample majority class}
\label{\detokenize{imbalanced_data:do-both-oversample-minority-class-and-undersample-majority-class}}

\subsection{SMOTE and Edited Nearest Neighbors = SMOTE\sphinxhyphen{}ENN}
\label{\detokenize{imbalanced_data:smote-and-edited-nearest-neighbors-smote-enn}}\begin{itemize}
\item {} 
\sphinxAtStartPar
SMOTE is used for upsampling

\item {} 
\sphinxAtStartPar
ENN removes an observation and its (k=3) nearest neighbors when the majority class of the nearest neighbors is different from the observation’s class

\end{itemize}


\subsection{SMOTETOMEK}
\label{\detokenize{imbalanced_data:smotetomek}}\begin{itemize}
\item {} 
\sphinxAtStartPar
this method finds reciprocal nearest\sphinxhyphen{}neighbors that are in different classes; These pairs are called Tomek\sphinxhyphen{}Links

\item {} 
\sphinxAtStartPar
the observation in such a pair, belonging to the majority class is deleted

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} not enough RAM}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{combine} \PYG{k+kn}{import} \PYG{n}{SMOTEENN}
\PYG{n}{X\PYGZus{}train\PYGZus{}uo}\PYG{p}{,} \PYG{n}{y\PYGZus{}train\PYGZus{}uo} \PYG{o}{=} \PYG{n}{SMOTEENN}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit\PYGZus{}resample}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} X\PYGZus{}train\PYGZus{}uo, y\PYGZus{}train\PYGZus{}uo = SMOTETomek(random\PYGZus{}state=0).fit\PYGZus{}resample(X\PYGZus{}train, y\PYGZus{}train)}
\PYG{c+c1}{\PYGZsh{} print(X\PYGZus{}train.shape, X\PYGZus{}train\PYGZus{}uo.shape)}
\end{sphinxVerbatim}


\section{get best combination: sampling and hyper\sphinxhyphen{}parameters}
\label{\detokenize{imbalanced_data:get-best-combination-sampling-and-hyper-parameters}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}pipeline}\PYG{p}{,} \PYG{n}{Pipeline}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{GridSearchCV}
\PYG{k+kn}{from} \PYG{n+nn}{imblearn}\PYG{n+nn}{.}\PYG{n+nn}{over\PYGZus{}sampling} \PYG{k+kn}{import} \PYG{n}{BorderlineSMOTE}
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\PYG{n}{clf}\PYG{o}{.}\PYG{n}{set\PYGZus{}params}\PYG{p}{(}\PYG{n}{num\PYGZus{}leaves}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{)}
\PYG{n}{borderline} \PYG{o}{=} \PYG{n}{BorderlineSMOTE}\PYG{p}{(}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{88}\PYG{p}{)}
\PYG{n}{grid} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class\PYGZus{}\PYGZus{}n\PYGZus{}estimators}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{l+m+mi}{500}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class\PYGZus{}\PYGZus{}neg\PYGZus{}bagging\PYGZus{}fraction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class\PYGZus{}\PYGZus{}max\PYGZus{}depth}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class\PYGZus{}\PYGZus{}learning\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:}\PYG{p}{[}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sampling\PYGZus{}\PYGZus{}sampling\PYGZus{}strategy}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{:}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\PYG{n}{pipeline} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sampling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{borderline}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{clf}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{grid\PYGZus{}cv} \PYG{o}{=} \PYG{n}{GridSearchCV}\PYG{p}{(}\PYG{n}{pipeline}\PYG{p}{,} \PYG{n}{grid}\PYG{p}{,} \PYG{n}{scoring} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{f1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cv} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{refit}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
   
\PYG{n}{grid\PYGZus{}cv}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{grid\PYGZus{}cv}\PYG{o}{.}\PYG{n}{best\PYGZus{}score\PYGZus{}}\PYG{p}{,} \PYG{n}{grid\PYGZus{}cv}\PYG{o}{.}\PYG{n}{best\PYGZus{}params\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pipeline}\PYG{o}{.}\PYG{n}{set\PYGZus{}params}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{grid\PYGZus{}cv}\PYG{o}{.}\PYG{n}{best\PYGZus{}params\PYGZus{}}\PYG{p}{)}
\PYG{n}{pipeline}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{pipeline}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{}[:, 1]}

\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred\PYGZus{}proba} \PYG{o}{=} \PYG{n}{pipeline}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} 2nd column is p(fraud)}
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}proba}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}


\section{One\sphinxhyphen{}Class SVM}
\label{\detokenize{imbalanced_data:one-class-svm}}
\sphinxAtStartPar
Train on majority class only and classifiy test\sphinxhyphen{}set in in\sphinxhyphen{}class examples and out\sphinxhyphen{}class examples:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{svm} \PYG{k+kn}{import} \PYG{n}{OneClassSVM} 

\PYG{n}{train\PYGZus{}normal} \PYG{o}{=} \PYG{n}{X\PYGZus{}train}\PYG{p}{[}\PYG{n}{y\PYGZus{}train} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]} 
\PYG{n}{train\PYGZus{}outliers} \PYG{o}{=} \PYG{n}{X\PYGZus{}train}\PYG{p}{[}\PYG{n}{y\PYGZus{}train} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{outlier\PYGZus{}prop} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{train\PYGZus{}outliers}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{train\PYGZus{}normal}\PYG{p}{)} 
\PYG{n}{svm} \PYG{o}{=} \PYG{n}{OneClassSVM}\PYG{p}{(}\PYG{n}{kernel}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{nu}\PYG{o}{=}\PYG{n}{outlier\PYGZus{}prop}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.000001}\PYG{p}{)} 
\PYG{n}{svm}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train\PYGZus{}normal}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{svm}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred\PYGZus{}corrected} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{[}\PYG{n}{y\PYGZus{}pred} \PYG{o}{==} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{y\PYGZus{}test}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{confusion} \PYG{o}{=} \PYG{n}{confusion\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}

\PYG{n}{df\PYGZus{}conf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{confusion}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}conf}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{AUC} \PYG{o}{=} \PYG{n}{roc\PYGZus{}auc\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}
\PYG{n}{precision}\PYG{p}{,} \PYG{n}{recall}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{precision\PYGZus{}recall\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{,} \PYG{n}{pos\PYGZus{}label}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{PR} \PYG{o}{=} \PYG{n}{auc}\PYG{p}{(}\PYG{n}{recall}\PYG{p}{,} \PYG{n}{precision}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Precision }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{precision\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Recall }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{recall\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{F1 }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f1\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Accuracy }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred\PYGZus{}corrected}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s2}{.4f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ROC }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{AUC}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, AUC PR }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{PR}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Pros:
\begin{itemize}
\item {} 
\sphinxAtStartPar
It works really well with a clear margin of separation

\item {} 
\sphinxAtStartPar
It is effective in high dimensional spaces.

\item {} 
\sphinxAtStartPar
It is effective in cases where the number of dimensions is greater than the number of samples.

\item {} 
\sphinxAtStartPar
It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.

\end{itemize}

\sphinxAtStartPar
Cons:
\begin{itemize}
\item {} 
\sphinxAtStartPar
It doesn’t perform well when we have large data set because the required training time is higher

\item {} 
\sphinxAtStartPar
It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping

\item {} 
\sphinxAtStartPar
SVM doesn’t directly provide probability estimates, these are calculated using an expensive five\sphinxhyphen{}fold cross\sphinxhyphen{}validation. It is included in the related SVC method of Python scikit\sphinxhyphen{}learn library.

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data Basics \& historical perspective}

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Titanic \sphinxhyphen{} Machine Learning from Disaster}
\label{\detokenize{Data_Basics:titanic-machine-learning-from-disaster}}\label{\detokenize{Data_Basics::doc}}

\section{Predict survival on the Titanic and get familiar with ML basics}
\label{\detokenize{Data_Basics:predict-survival-on-the-titanic-and-get-familiar-with-ml-basics}}
\sphinxAtStartPar
\sphinxhref{https://www.kaggle.com/c/titanic}{Home Page}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Pclass     Sex   Age Cabin
0       3    male  22.0   NaN
1       1  female  38.0   C85
2       3  female  26.0   NaN
3       1  female  35.0  C123
4       3    male  35.0   NaN
\end{sphinxVerbatim}


\section{continuous variables}
\label{\detokenize{Data_Basics:continuous-variables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
are always approximations, e.g. the age could be 22 years, 2 months, 1 week, 1 day, 10 hours, 2 minuts, 55 seconds, …

\item {} 
\sphinxAtStartPar
have an ordering, e.g. \(22\,\text{years} < 38\,\text{years}\)

\item {} 
\sphinxAtStartPar
you can interpret differences: \(38\,\text{years} - 22\,\text{years} = 16\,\text{years}\)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Age
0  22.0
1  38.0
2  26.0
3  35.0
4  35.0
\end{sphinxVerbatim}


\section{categorical or discrete variables}
\label{\detokenize{Data_Basics:categorical-or-discrete-variables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
are always finite, e.g. Sex is most of the time binary and it’s either female or male (I know that’s not the best exambple \sphinxhyphen{} please do not confuse sex with gender)

\item {} 
\sphinxAtStartPar
there is no ordering

\item {} 
\sphinxAtStartPar
you can not interpret differences

\item {} 
\sphinxAtStartPar
sex is a special case because it’s also a \sphinxstylestrong{binary variable}; another example for discrete variables is marital status that can be \sphinxstylestrong{single, married, widowed, divorced, separated}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Sex
0    male
1  female
2  female
3  female
4    male
\end{sphinxVerbatim}


\section{ordinal variables}
\label{\detokenize{Data_Basics:ordinal-variables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
are always finite, e.g the passenger\sphinxhyphen{}class is either 1, 2, or 3 but nothing in between

\item {} 
\sphinxAtStartPar
it’s still possible to have an ordering: \(\text{p-class 1} > \text{p-class 2} > \text{p-class 3}\)

\item {} 
\sphinxAtStartPar
you can not interpret the differences: \(\text{p-class 3} - \text{p-class 2} = \text{p-class 
1}\)?

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Pclass
0       3
1       1
2       3
3       1
4       3
\end{sphinxVerbatim}


\chapter{Processing of Variable}
\label{\detokenize{Data_Basics:processing-of-variable}}

\section{Continuous Variables}
\label{\detokenize{Data_Basics:id1}}
\sphinxAtStartPar
We can easily transform a continuous variable into a ordinal variable by setting cut\sphinxhyphen{}points. E.g., we could say that all passengers younger than 2 years are renamed as ‘Baby’, all passengers between 2 years and 17 years as ‘Child’, etc..

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{example}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age\PYGZus{}binned}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{cut}\PYG{p}{(}\PYG{n}{example}\PYG{o}{.}\PYG{n}{Age}\PYG{p}{,}\PYG{n}{bins}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{17}\PYG{p}{,}\PYG{l+m+mi}{65}\PYG{p}{,}\PYG{l+m+mi}{99}\PYG{p}{]}\PYG{p}{,}\PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Baby}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Child}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adult}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Elderly}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{c+c1}{\PYGZsh{} .iloc[30:40]}
\PYG{n}{example}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age\PYGZus{}binned}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{30}\PYG{p}{:}\PYG{l+m+mi}{40}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     Age Age\PYGZus{}binned
30  40.0      Adult
31   NaN        NaN
32   NaN        NaN
33  66.0    Elderly
34  28.0      Adult
35  42.0      Adult
36   NaN        NaN
37  21.0      Adult
38  18.0      Adult
39  14.0      Child
\end{sphinxVerbatim}


\section{Categorical or discrete variables}
\label{\detokenize{Data_Basics:id2}}
\sphinxAtStartPar
For mathematical models it’s hard to work with categories as for example \sphinxstyleemphasis{male}, \sphinxstyleemphasis{female} or \sphinxstyleemphasis{Adult}, \sphinxstyleemphasis{Baby}, \sphinxstyleemphasis{Child}, etc..
This is why we have to turn them into categorical variables. This is done by adding new columns to the data, one for each category\sphinxhyphen{}level:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nexample} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{example}\PYG{p}{,} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{example}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age\PYGZus{}binned}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{nexample}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age\PYGZus{}binned}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Baby}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Child}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adult}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Elderly}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{30}\PYG{p}{:}\PYG{l+m+mi}{40}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     Age Age\PYGZus{}binned  Baby  Child  Adult  Elderly
30  40.0      Adult     0      0      1        0
31   NaN        NaN     0      0      0        0
32   NaN        NaN     0      0      0        0
33  66.0    Elderly     0      0      0        1
34  28.0      Adult     0      0      1        0
35  42.0      Adult     0      0      1        0
36   NaN        NaN     0      0      0        0
37  21.0      Adult     0      0      1        0
38  18.0      Adult     0      0      1        0
39  14.0      Child     0      1      0        0
\end{sphinxVerbatim}

\sphinxAtStartPar
This is called:
\begin{itemize}
\item {} 
\sphinxAtStartPar
one\sphinxhyphen{}hot encoding

\item {} 
\sphinxAtStartPar
sometimes this is also incorrectly called dummy encoding

\item {} 
\sphinxAtStartPar
real \sphinxstylestrong{dummy encoding} has one column less than one\sphinxhyphen{}hot encoding: The idea is, if its not \sphinxstyleemphasis{Child}, nor \sphinxstyleemphasis{Adult} or \sphinxstyleemphasis{Elderly}, then is must be \sphinxstyleemphasis{Baby} \sphinxhyphen{} so we do not need an extra column for \sphinxstyleemphasis{Baby}

\end{itemize}

\sphinxAtStartPar
Most intuitively the \sphinxstylestrong{real} dummy\sphinxhyphen{}encoding can be seen with \sphinxstylestrong{sex}: even though there are two different categories, we just need one column

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Sex  Sex2
0    male     0
1  female     1
2  female     1
3  female     1
4    male     0
\end{sphinxVerbatim}


\section{ordinal data}
\label{\detokenize{Data_Basics:ordinal-data}}\begin{itemize}
\item {} 
\sphinxAtStartPar
there are methods for ordinal data, e.g. ordinal regression

\item {} 
\sphinxAtStartPar
most of the time ordinal variables are just treated as categorical variables

\end{itemize}


\chapter{Missing Data}
\label{\detokenize{Data_Basics:missing-data}}
\sphinxAtStartPar
Data can be missing:
\begin{itemize}
\item {} 
\sphinxAtStartPar
at random

\item {} 
\sphinxAtStartPar
systematically, i.e. the fact that the data is missing could bear some valuable information

\end{itemize}

\sphinxAtStartPar
For categorical and ordinal data, missing data is just another category.
For continuous variables there are several possibilities to deal with missing data. The most frequent ones are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
imputation by the mean (the mean\sphinxhyphen{}value of all non\sphinxhyphen{}missing values is taken)

\item {} 
\sphinxAtStartPar
imputation by the median (the value with half of all values larger and half of all values smaller is taken)

\item {} 
\sphinxAtStartPar
imputation by the mode (the most frequent value is taken)

\end{itemize}

\sphinxAtStartPar
However, since we do not know for sure, why data is missing it is often helpfull to keep track of it by creating a new indicator variable for missing values:

\sphinxAtStartPar
\sphinxstylestrong{First, we create the indicator variable:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     Age  missing\PYGZus{}Age
30  40.0            0
31   NaN            1
32   NaN            1
33  66.0            0
34  28.0            0
35  42.0            0
36   NaN            1
37  21.0            0
38  18.0            0
39  14.0            0
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{Second, we impute missing values with the average Age:}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          Age  missing\PYGZus{}Age
30  40.000000            0
31  29.699118            1
32  29.699118            1
33  66.000000            0
34  28.000000            0
35  42.000000            0
36  29.699118            1
37  21.000000            0
38  18.000000            0
39  14.000000            0
\end{sphinxVerbatim}


\section{Interactions}
\label{\detokenize{Data_Basics:interactions}}
\sphinxAtStartPar
Interactions are another important concept in linear modelling. Here, the effect of one variable on the dependent variable \(y\) depends on the value of another variable.
In the example below we try to model the probability that a person buys a house. Of course, monthly income is an important variable and the higher it is, the more likely that said person will buy a house. Another important variable is marital status. Married people with children in the household tend strongly to buy houses, especially if their monthly income is high. On the other hand, singles, even if they have a high income, will tend not to buy a house.
So we see, the variable “monthly income” \sphinxstylestrong{interacts} with the variable “marital status”: the effect of the two variables together is more than the sum of the effects of the individual variables.

\noindent\sphinxincludegraphics{{Data_Basics_22_0}.png}


\section{Standardization / Normalization}
\label{\detokenize{Data_Basics:standardization-normalization}}
\sphinxAtStartPar
Sometimes we have to bring different variables into the same range. This is very important for Neural Networks, but also for other algorithms it can sometimes be benefitial. 
Assume, we have the age of some passengers as in the following table:

\sphinxAtStartPar
We obtain normalized values by applying the following z\sphinxhyphen{}transform:
\label{equation:Data_Basics:35cbd6a8-a81e-4c16-8ab9-792e4d5bcdd9}\begin{eqnarray}
z_i=&\frac{x_i - \bar{x}}{\sigma}\\
\text{with:}&\\
\bar{x}=&\text{mean}\\
\sigma=&\text{standarddeviation}
\end{eqnarray}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{passenger\PYGZus{}age}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalized age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{passenger\PYGZus{}age}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{passenger\PYGZus{}age}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{passenger\PYGZus{}age}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{passenger\PYGZus{}age}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                 Name   Age  normalized age
0                             Braund, Mr. Owen Harris  22.0       \PYGZhy{}0.427976
1   Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0        0.481000
2                              Heikkinen, Miss. Laina  26.0       \PYGZhy{}0.200732
3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0        0.310567
4                            Allen, Mr. William Henry  35.0        0.310567
6                             McCarthy, Mr. Timothy J  54.0        1.389976
7                      Palsson, Master. Gosta Leonard   2.0       \PYGZhy{}1.564197
8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.0       \PYGZhy{}0.143921
9                 Nasser, Mrs. Nicholas (Adele Achem)  14.0       \PYGZhy{}0.882465
10                    Sandstrom, Miss. Marguerite Rut   4.0       \PYGZhy{}1.450575
11                           Bonnell, Miss. Elizabeth  58.0        1.617220
12                     Saundercock, Mr. William Henry  20.0       \PYGZhy{}0.541598
13                        Andersson, Mr. Anders Johan  39.0        0.537811
14               Vestrom, Miss. Hulda Amanda Adolfina  14.0       \PYGZhy{}0.882465
15                   Hewlett, Mrs. (Mary D Kingcome)   55.0        1.446787
\end{sphinxVerbatim}

\sphinxAtStartPar
Now, let’s do the same for another variable: Fare \sphinxhyphen{} the price payed for the passage

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                 Name   Age  normalized age  \PYGZbs{}
0                             Braund, Mr. Owen Harris  22.0       \PYGZhy{}0.427976   
1   Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0        0.481000   
2                              Heikkinen, Miss. Laina  26.0       \PYGZhy{}0.200732   
3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0        0.310567   
4                            Allen, Mr. William Henry  35.0        0.310567   
6                             McCarthy, Mr. Timothy J  54.0        1.389976   
7                      Palsson, Master. Gosta Leonard   2.0       \PYGZhy{}1.564197   
8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.0       \PYGZhy{}0.143921   
9                 Nasser, Mrs. Nicholas (Adele Achem)  14.0       \PYGZhy{}0.882465   
10                    Sandstrom, Miss. Marguerite Rut   4.0       \PYGZhy{}1.450575   
11                           Bonnell, Miss. Elizabeth  58.0        1.617220   
12                     Saundercock, Mr. William Henry  20.0       \PYGZhy{}0.541598   
13                        Andersson, Mr. Anders Johan  39.0        0.537811   
14               Vestrom, Miss. Hulda Amanda Adolfina  14.0       \PYGZhy{}0.882465   
15                   Hewlett, Mrs. (Mary D Kingcome)   55.0        1.446787   

       Fare  normalized\PYGZus{}Fare  
0    72.500        \PYGZhy{}0.868610  
1   712.833         2.347294  
2    79.250        \PYGZhy{}0.834709  
3   531.000         1.434086  
4    80.500        \PYGZhy{}0.828432  
6   518.625         1.371936  
7   210.750        \PYGZhy{}0.174285  
8   111.333        \PYGZhy{}0.673581  
9   300.708         0.277505  
10  167.000        \PYGZhy{}0.394008  
11  265.500         0.100682  
12   80.500        \PYGZhy{}0.828432  
13  312.750         0.337983  
14   78.542        \PYGZhy{}0.838265  
15  160.000        \PYGZhy{}0.429164  
\end{sphinxVerbatim}

\sphinxAtStartPar
The un\sphinxhyphen{}standardized and standardized variables accross passengers look like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Data_Basics_30_1}.png}

\noindent\sphinxincludegraphics{{Data_Basics_31_0}.png}


\chapter{not covered here}
\label{\detokenize{Data_Basics:not-covered-here}}
\sphinxAtStartPar
The following topics are more advanced and do not apply to tree\sphinxhyphen{}methods. In a possible follow\sphinxhyphen{}up we can discuss them as well:
\begin{itemize}
\item {} 
\sphinxAtStartPar
power\sphinxhyphen{}transforms

\item {} 
\sphinxAtStartPar
mean\sphinxhyphen{}encoding

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linear Regression}

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Knowledge Discovery \sphinxhyphen{}> Data Mining \sphinxhyphen{}> Data Science \sphinxhyphen{}> Machine Learning}
\label{\detokenize{Regression_Techniques:knowledge-discovery-data-mining-data-science-machine-learning}}\label{\detokenize{Regression_Techniques::doc}}
\noindent\sphinxincludegraphics{{Regression_Techniques_3_0}.png}


\chapter{Lineare Regression}
\label{\detokenize{Regression_Techniques:lineare-regression}}
\sphinxAtStartPar
In der nachfolgenden Zelle werden zuerst Daten geladen, die zur Veranschaulichung der linearen Regression dienen.
Anschliessend wird ein lineares Modell mit Hilfe der der Klasse Lineare Regression aus \sphinxcode{\sphinxupquote{sklearn.linear\_model}} gerechnet. Die Vorhersage (d.h. die Geradengleichung) ergibt sich aus den Koeffizienten durch \(y = a + bX\).

\noindent\sphinxincludegraphics{{Regression_Techniques_5_0}.png}

\sphinxAtStartPar
Der Plot zeigt die berechnete Regressionsgerade, sowie die Abweichungen (die Fehler) der wirklichen Messwerte von dieser Geraden. Diese Abweichungen werden als \sphinxstylestrong{Residuen} bezeichnet, weil es der Anteil der gemessenen Werte ist, der “übrig bleibt”, d.h. nicht durch das Modell erklärt werden kann. Vorhergesagte Variablen werden meist mit einem Dach (Hut) bezeichnet, sowie \(\hat{y}\).


\chapter{multivariate case: more than one x variable}
\label{\detokenize{Regression_Techniques:multivariate-case-more-than-one-x-variable}}
\sphinxAtStartPar
Für Multivariate Lineare Regression kann die Schreibweise mit Matrizen zusammengefasst werden. Dafür kann es lohnend sein, sich die Matrizen\sphinxhyphen{}Multiplikation noch einmal kurz anzusehen.
\begin{align*}
    y_1&=a+b_1\cdot x_{11}+b_2\cdot x_{21}+\cdots + b_p\cdot x_{p1}\\
    y_2&=a+b_1\cdot x_{12}+b_2\cdot x_{22}+\cdots + b_p\cdot x_{p2}\\
    \ldots& \ldots\\
    y_i&=a+b_1\cdot x_{1i}+b_2\cdot x_{2i}+\cdots + b_p\cdot x_{pi}\\
\end{align*}\begin{equation*}
    \begin{bmatrix}
      y_1\\
      y_2\\
      .  \\
      .  \\
      .  \\
      y_i
    \end{bmatrix}
    = a+
    \begin{bmatrix}
      x_{11} & x_{21} & x_{31} & \ldots & x_{p1}\\
      x_{12} & x_{22} & x_{32} & \ldots & x_{p2}\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      \ldots&\ldots&\ldots&\ldots&\ldots\\
      x_{1i} & x_{2i} & x_{3i} & \ldots & x_{pi}\\
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
 b_1\\
      b_2\\
      .\\
      .\\
      .\\
      b_p
    \end{bmatrix}
\end{equation*}
\sphinxAtStartPar
Den konstanten inercept Term (\(a\)) können wir mit in den Vektor der Parameter \(\mathbf{b}\) aufnehmen, indem wir in \(\mathbf{X}\) eine Einser\sphinxhyphen{}Spalte hinzufügen. Somit wird die Schreibweise sehr kompakt und der intercept \(a\) wird nicht mehr explizit aufgeführt:
\begin{equation*}
     \begin{bmatrix}
      y_1\\
      y_2\\
      .  \\
      .  \\
      .  \\
      y_i
    \end{bmatrix}
    =
    \begin{bmatrix}
      1& x_{11} & x_{21} & x_{31} & \ldots & x_{p1}\\
      1 &  x_{12} & x_{22} & x_{32} & \ldots & x_{p2}\\
      &\ldots&\ldots&\ldots&\ldots&\ldots\\
      &\ldots&\ldots&\ldots&\ldots&\ldots\\
      1& x_{1i} & x_{2i} & x_{3i} & \ldots & x_{pi}
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
      a\\
      b_1\\
      b_2\\
      .\\
      .\\
      b_p
    \end{bmatrix}
  \end{equation*}
\sphinxAtStartPar
In Matrizen\sphinxhyphen{}Schreibweise können wir jetzt einfach schreiben:
\(\mathbf{y} = \mathbf{X}\mathbf{b}\)


\chapter{Polynomial regression as an example for more than one variable}
\label{\detokenize{Regression_Techniques:polynomial-regression-as-an-example-for-more-than-one-variable}}
\sphinxAtStartPar
Um einfach Multivariate Lineare Regression an einem Beispiel zeigen zu können wird die quadratische Regression (ein Spezial\sphinxhyphen{}Fall der Multivariaten Regression) eingeführt. Eine neue Variable entsteht durch das Quadrieren der bisherigen univiaraten Variable x. Das Praktische ist, dass sich der Sachverhalt der Multivariaten Regression noch immer sehr schön 2\sphinxhyphen{}dimensional darstellen lässt.
\(y = a + b_1 x + b_2 x^2\)

\sphinxAtStartPar
Hier ist zu beachten:
\begin{itemize}
\item {} 
\sphinxAtStartPar
wir haben jetzt zwei Variablen und können folglich unsere Formel in Matrizen\sphinxhyphen{}Schreibweise anwenden

\item {} 
\sphinxAtStartPar
mehr Variablen führen hoffentlich zu einem besseren Modell

\item {} 
\sphinxAtStartPar
durch den quadratischen Term ist die resultierende Regressions\sphinxhyphen{}Funktion keine Gerade mehr.
\sphinxstylestrong{Der Ausdruck “linear” in Linearer Regression bedeutet dass die Funktion linear in den Parametern
\(a, \mathbf{b}_\mathbf{1}, \mathbf{b}_\mathbf{2}\) ist. Für alle Werte einer Variablen \(\mathbf{x_1}\) gilt der gleiche Parameter \(\mathbf{b_1}\).
Es bedeutet nicht, dass die Regressions\sphinxhyphen{}Funktion durch eine gerade Linie gegeben ist!}

\item {} 
\sphinxAtStartPar
ausserdem bedienen wir uns hier eines Tricks: Die Variable \(x^2\) müsste eigentlich eine eigene Achse bekommen. Dann wäre die Regressions\sphinxhyphen{}Gerade wieder eine gerade Linie \sphinxhyphen{} nur lässt sich das leider nicht mehr schön darstellen.

\end{itemize}

\sphinxAtStartPar
Nachfolgend fügen wir die weitere Variable durch Quadrieren der bisherigen Variable hinzu und berechnen abermals das Lineare Modell aus \sphinxcode{\sphinxupquote{sklearn.linear\_model}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}5.0, 110.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_10_1}.png}


\section{Overfitting}
\label{\detokenize{Regression_Techniques:overfitting}}
\sphinxAtStartPar
Nun wird diese Vorgehensweise für weitere Terme höherer Ordnung angewendet. Graphisch lässt sich zeigen, dass die Anpassung des Modells an die Daten immer besser wird, die Vorhersage für \sphinxstylestrong{neue Datenpunkte} aber sehr schlecht sein dürfte. Man sagt dann, das Model \sphinxstylestrong{“generalisiert”} sehr schlecht. Das Polynom hat an vielen Stellen Schlenker und absurde Kurven eingebaut. Dies ist ein erstes Beispiel für \sphinxstylestrong{“overfitting”}.Einen ‘perfekten’ fit erhält man, wenn man genau so viele Paramter (10 Steigunskoeffizienten + intercept) hat wie Daten\sphinxhyphen{}Messpunkte.

\sphinxAtStartPar
The important points to note here:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the fit to our empirical y\sphinxhyphen{}values gets better

\item {} 
\sphinxAtStartPar
at the same time, the regression line starts behaving strangly

\item {} 
\sphinxAtStartPar
the predictions made by the regression line in between the empirical y\sphinxhyphen{}values are grossly wrong: this is an example of \sphinxstylestrong{overfitting}

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 115.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_12_1}.png}


\section{perfect fit: as many variables as data samples}
\label{\detokenize{Regression_Techniques:perfect-fit-as-many-variables-as-data-samples}}
\sphinxAtStartPar
A perfect fit is possible as is demonstrated next. We have as many variables (terms derived from x) as observations (data points). So for each data point we have a variable to accommodate it.
\sphinxstylestrong{Note}, that a perfect fit is achieved with 10 variables + intercept. The intercept is also a parameter and in this case the number of observations \(n\) equals the number of variables \(p\), i.e. \(p=n\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0, 125.77315979942053)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Regression_Techniques_14_1}.png}


\section{Bias\sphinxhyphen{}Variance Tradeoff}
\label{\detokenize{Regression_Techniques:bias-variance-tradeoff}}
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Bias\%E2\%80\%93variance\_tradeoff}{Wiki}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Bias}}: Underfitting

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Variance}}: Overfitting \sphinxhyphen{} the model overfits pecularities of the data sample

\end{itemize}

\noindent\sphinxincludegraphics{{Regression_Techniques_16_0}.png}

\sphinxAtStartPar
This is the perspectiv of classical statistics:
\begin{itemize}
\item {} 
\sphinxAtStartPar
more parameters lead to overfitting

\item {} 
\sphinxAtStartPar
the results of models with many parameters are not reliable (due to the high variance)

\item {} 
\sphinxAtStartPar
with more parameters it’s harder for single parameters to reach the significance threshold (statistical testing)

\item {} 
\sphinxAtStartPar
smaller models are better (epistemology: prefer simpler solutions if they are as good as the more complex ones)

\end{itemize}

\sphinxAtStartPar
But neural networks are heavily over\sphinxhyphen{}parameterized with far more weight\sphinxhyphen{}parameters than independent samples in the training data. How comes they generalize quite well?

\sphinxAtStartPar
Following \sphinxhref{https://arxiv.org/abs/1812.11118}{Belkin et al., 2019} and \sphinxhref{https://arxiv.org/abs/2109.02355}{Dar et al., 2021}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
When we have as many parameters as data samples, the number of solutions is very constrained. The model has to “stretch” to reach the interpolation threshold with a limited capacity. This explains the weird loops the polynomial makes.

\item {} 
\sphinxAtStartPar
When we have more parameters than data points the space of interpolating solutions opens\sphinxhyphen{}up, actually allowing optimization to reach lower\sphinxhyphen{}norm interpolating solutions. These tend to generalize better, and that’s why you get the second descent on test data.

\end{itemize}

\noindent\sphinxincludegraphics{{Regression_Techniques_18_0}.png}

\sphinxAtStartPar
For the interested reader:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update}{On the Bias\sphinxhyphen{}Variance Tradeoff: Textbooks Need an Update}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://medium.com/mlearning-ai/double-descent-8f92dfdc442f}{Double Descent}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://openai.com/blog/deep-double-descent/}{Deep Double Descent}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html\#modern-risk-curve-for-deep-learning}{Are Deep Neural Networks Dramatically Overfitted?}

\end{itemize}


\chapter{Dealing with overfitting}
\label{\detokenize{Regression_Techniques:dealing-with-overfitting}}
\sphinxAtStartPar
Wie wir gesehen haben tendiert klassische Lineare Regression zu ‘overfitting’ sobald es wenige Datenpunkte gibt und mehrere Koeffizienten berechnet werden. 
Eine Lösung für dieses Problem ist, die Koeffizienten \(b_1, b_2, b_3, \ldots\) kleiner zu machen. Dies kann erreicht werden, wenn der Fehler der Regression mit grösseren Koeffizienten auch grösser wird. Um nun das Minimum der Fehlerfunktion zu finden ist ein probates Mittel, die Koeffizienten kleiner zu machen und somit implizit ‘overfitting’ zu verhindern.
Parameter können jetzt nur noch sehr gross werden, wenn dadurch gleichzeitig der Fehler stark reduziert werden kann.

\sphinxAtStartPar
Nachfolgend wird ein Strafterm (‘penalty’) für grosse Parameter eingeführt. Im Falle der Ridge\sphinxhyphen{}Regression gehen die Koeffizienten quadriert in die Fehlerfunktion mit ein. Der Gewichtungsfaktor \(\lambda\) bestimmt die Höhe des Strafterms und ist ein zusätzlicher Parameter für den – je nach Datensatz – ein optimaler Wert gefunden werden muss.


\section{Ridge regression}
\label{\detokenize{Regression_Techniques:ridge-regression}}
\sphinxAtStartPar
Remember this formula:
\label{equation:Regression_Techniques:3f934782-64f7-4d0c-8d63-25226a80cb14}\begin{equation}
\label{eq:1}
\sum_i^{n}(y_i - \hat{y_i})^2 = \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}
\end{equation}
\sphinxAtStartPar
To make the error term larger for extrem values of \(b\), we could simply add \(\lambda\cdot b^2\) to the error:
\label{equation:Regression_Techniques:5a1cfae2-2a72-423d-b04a-4e38b261db6e}\begin{equation}
\label{eq:2}
\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda b^2= \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}+ \lambda b^2
\end{equation}
\sphinxAtStartPar
The parameter \(\lambda\) is for scaling the amount of shrinkage.
Die beiden Ausdrücke
\label{equation:Regression_Techniques:83b706b3-6e7c-4cdc-b6c8-78a64ef4307b}\begin{equation}
\label{eq:fehler}
\sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}
\end{equation}\label{equation:Regression_Techniques:57f9289f-9dde-455e-93df-788940b66ab6}\begin{equation}
\label{eq:ridge_error}
\lambda b^2
\end{equation}
\sphinxAtStartPar
sind wie Antagonisten. Der Koeffizient \(b\) darf nur gross werden, wenn er es vermag \(\eqref{eq:fehler}\) stark zu verkleinern, so dass der Zugewinn in \(\eqref{eq:fehler}\) den Strafterm in \(\eqref{eq:ridge_error}\) überwiegt.

\sphinxAtStartPar
For two variables we can write:
\begin{equation*}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda b_1^2 + \lambda b_2^2= \sum_i^{n}[y_i - (a + b_1\cdot x_{i1} + b_2\cdot x_{i2})]^{2}+ \lambda b_1^2 + \lambda b_2^2\end{equation*}
\sphinxAtStartPar
And in matrix notation for an arbitrary number of variables:
\begin{align*}
    \text{min}=&(\mathbf{y}-\hat{\mathbf{y}})^2 + \lambda \mathbf{b}^2=(\mathbf{y}-\mathbf{X}\mathbf{b})'(\mathbf{y}-\mathbf{X}\mathbf{b}) + \lambda \mathbf{b}'\mathbf{b}
\end{align*}

\subsection{example of ridge regression}
\label{\detokenize{Regression_Techniques:example-of-ridge-regression}}
\sphinxAtStartPar
Next, we will apply ridge regression as implemented in the python \sphinxcode{\sphinxupquote{sklearn}} library and compare the results to the linear algebra solution. Note, that we have to center the variables.
\begin{itemize}
\item {} 
\sphinxAtStartPar
we can center \(\mathbf{X}\) and \(\mathbf{y}\) and display the result in the centered coordinate system

\item {} 
\sphinxAtStartPar
or we can center \(\mathbf{X}\) and add the mean of \(\mathbf{y}\) to the predicted values to display the result in the original coordinate system. This approaches allows for an easy comparison to the overfitted result

\end{itemize}

\sphinxAtStartPar
Die Zeile \sphinxcode{\sphinxupquote{Xc = X \sphinxhyphen{} np.mean(X, axis=0)}} standardisiert die Variablen auf den Mittelwert von 0

\noindent\sphinxincludegraphics{{Regression_Techniques_23_0}.png}

\sphinxAtStartPar
Now, it becomes clear why Ridge Regression was invented before Lasso Regression. We have a analytical solution. Ridge is nearer to ‘old school statistics’ than Lasso is.


\section{Lasso}
\label{\detokenize{Regression_Techniques:lasso}}
\sphinxAtStartPar
Alternativ zu einem quadratischen Strafterm \(b^2\) könnte man auch den absoluten Wert nehmen \(|b|\). In diesem Fall erhält man die sog.\textasciitilde{}Lasso Regression; \(\lambda\cdot |b|\) wird zum Vorhersage\sphinxhyphen{}Fehler addiert:
\begin{equation*}
\begin{split}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda |b|= \sum_i^{n}[y_i - (a + b\cdot x_i)]^{2}+ \lambda |b|\end{split}
\end{equation*}
\sphinxAtStartPar
Für zwei Variablen würde man folglich schreiben:
\begin{equation*}
\begin{split}\sum_i^{n}(y_i - \hat{y_i})^2 + \lambda |b_1| + \lambda |b_2|= \sum_i^{n}[y_i - (a + b_1\cdot x_{i1} + b_2\cdot x_{i2})]^{2}+ \lambda |b_1| + \lambda |b_2|\end{split}
\end{equation*}
\sphinxAtStartPar
Leider gibt es im Gegesatz zur Ridge Regression keine eindeutige analytische Lösung um die Koeffizienten der Lasso Regression zu erhalten. Hier kommen iterative Verfahren zum Einsatz, wie wir sie in Session 2 kennen lernen werden.
Iterative Verfahren haben sich erst sehr spät durchgesetzt \sphinxhyphen{} nicht zuletzt wegen der Rechenleistung die sie benötigen.


\subsection{kurzer Einschub: klassische Statistik vs. Machine Learning}
\label{\detokenize{Regression_Techniques:kurzer-einschub-klassische-statistik-vs-machine-learning}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Mathematisch liess sich lange Zeit nur ein lineares Gleichungssystem zuverlässig lösen (Rechenpower). Deshalb wurde Ridge\sphinxhyphen{}Regression auch vor Lasso\sphinxhyphen{}Regression erfunden. Für ersteres Verfahren gibt es eine analytische Lösung.

\item {} 
\sphinxAtStartPar
Das lineare Modell setzt voraus, dass alle Variablen darin voneinander unabhängig und normal verteilt sind. Dies trifft auf fast keinen Umstand in unserer Welt zu.

\item {} 
\sphinxAtStartPar
Konfidenzintervalle und Signifikanzen sind das direkte Resultat dieser Annahmen und der damit verbundenen mathmatischen Lösung \sphinxhyphen{} der Inversion der Kreuzprodukt\sphinxhyphen{}Matrix \sphinxhyphen{} so wie wir das besprochen haben.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{“Overfitting”} ist der Begriff, der verwendet wurde, wenn das verwendete mathematische Verfahren die Daten der Stichprobe zu genau repräsentiert und auf neue Daten schlecht generalisiert.

\item {} 
\sphinxAtStartPar
Leider wurde \sphinxstylestrong{“overfitting”} oft gleichbedeutend mit zu vielen Variablen verwendet.

\item {} 
\sphinxAtStartPar
Wird die Grösse der Parameter (die Norm) klein gehalten (Ridge, Lasso) so tritt \sphinxstylestrong{“overfitting”} nicht auf.

\item {} 
\sphinxAtStartPar
Mittlerweile gibt es zuverlässige Verfahren, die overfitting zu verhindern wissen. Da die Modellannahmen in den Wissenschaften oft nur getroffen wurden, weil es für diese eine analytische Lösung gibt, müssten eigentlich viele Lehrbücher umgeschrieben werden.

\end{itemize}


\subsection{kurzer Einschub: klassische Statistik vs. Machine Learning}
\label{\detokenize{Regression_Techniques:id1}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Die Verfahren mit vielen Variablen finden in vielen Anwendungen sehr gute Lösungen und haben einige Anwedungsfelder geradezu revolutioniert (Sprach\sphinxhyphen{} und Bilderverarbeitung).

\item {} 
\sphinxAtStartPar
Wissenschaftstheoretisch sind die neuen Verfahren nicht hinreichend, aber auch die alten Verfahren sind von geringem Nutzen, wenn die Annahmen falsch sind.

\item {} 
\sphinxAtStartPar
Es wird Zeit, bisherige klassische Statistik und Verfahren des maschinellen Lernen miteinander zu versöhnen. Eine neuere, umfassende Theorie muss entwickelt werden.

\end{itemize}

\noindent\sphinxincludegraphics{{Regression_Techniques_28_0}.png}


\chapter{Extension: logistic regression and the GLM}
\label{\detokenize{Regression_Techniques:extension-logistic-regression-and-the-glm}}
\sphinxAtStartPar
Es gibt andere Modelle, die eng verwandt mit der hier besprochenen Linearen Regression sind. Das Prominenteste unter ihnen ist die \sphinxstylestrong{Logistische Regression}. Diese Modell gehört zu dem “\sphinxstylestrong{Verallgemeinerten Linearen Modell}” (im engl. \sphinxstylestrong{generalized lineare model} (GLM)). Diese Modelle dürfen nicht mit dem “\sphinxstylestrong{Allgemeinen Linearen Modell}” (im engl. \sphinxstylestrong{general linear model}) verwechselt werden. Letzteres parametrisiert eine Varianzanalyse als ein lineares Modell mit Dummy\sphinxhyphen{}Variablen.
Das Verallgemeinerte Lineare Modell erweitert die Lineare Regression um Modelle, deren Fehler nicht normalverteilt sind.
\sphinxhref{https://en.wikipedia.org/wiki/Generalized\_linear\_model\#Confusion\_with\_general\_linear\_models}{Dieser Artikel} in der Wikipedia gibt weitere Auskunft.


\section{exponential family of distributions}
\label{\detokenize{Regression_Techniques:exponential-family-of-distributions}}
\sphinxAtStartPar
Aus der Perspektive der Modernen Statistik beinhaltet das Verallgemeinerte Lineare Modell verschiedene Lineare Modelle, unter anderem das der klassischen linearen Regression. Eine Verteilung, die in der “exponential family” von Verteilungen ist, kann immer folgendermassen geschrieben werden:
\label{equation:Regression_Techniques:bda7f799-b700-4a32-803f-0cab5b7efd6d}\begin{equation}
f(y| \theta) = \exp\left(\frac{y \theta + b(\theta)}{\Phi} + c(y, \Phi)\right),
\end{equation}
\sphinxAtStartPar
wobei \(\theta\) als Kanonischer Parameter bezeichnet wird, welcher eine Funktion von \(\mu\) ist dem Mittel. Diese Funktion wird als Kanonische Link\sphinxhyphen{}Funktion bezeichnet. Wie wir später an einem Beispiel sehen werden, ist es genau diese Funktion welche die Beziehung zwischen der abhängigen Variablen und den unabhängigen Variablen linearisiert.
Der Vollständigkeit halber: \(b(\theta)\) ist eine Funktion des Kanonischen Parameters und ist somit ebenfalls von \(\mu\) abhängig. \(\Phi\) wird als Streuungsparameter bezeichnet und \(c(y, \Phi)\) ist eine Funktion, die sowohl von beobachteten Daten wie auch dem Streuungsparameter abhängig ist.


\subsection{Normalverteilung}
\label{\detokenize{Regression_Techniques:normalverteilung}}\begin{eqnarray*}
f(y| \mu, \sigma) =& (2\pi \sigma^2)^{-\frac{1}{2}} \exp\left(-\frac{1}{2}\frac{y^2 -2y\mu + \mu^2}{\sigma^2}\right) \\
 =&\quad \exp \left(\frac{y\mu -\frac{\mu^2}{2}}{\sigma^2} - \frac{1}{2}\left(\frac{y^2}{\sigma^2} + \log(2\pi\sigma^2\right)\right),\quad \text{wobei}
\end{eqnarray*}
\sphinxAtStartPar
\(\mu = \theta(\mu)\), d.h. \(\mu\) ist der Kanonische Parameter und die Link\sphinxhyphen{}Funktion ist die Identitäts\sphinxhyphen{}Funktion. Der Mittelwert kann also ohne weitere Transformation direkt modelliert werden, so wie wir es in der klassischen Linearen Regression machen.
Der Streuungsparameter \(\Phi\) ist durch \(\sigma^2\), die Varianz gegeben. Dies ist die klassische Lineare Regression normalverteilter Variablen


\subsection{Poisson distribution}
\label{\detokenize{Regression_Techniques:poisson-distribution}}
\sphinxAtStartPar
Die Poisson\sphinxhyphen{}Verteilung gehört ebenfalls der exponential family von Verteilungen an:
\begin{eqnarray*}
f(y| \mu) =& \frac{\mu^{y} e^{-\mu}}{y!} = \mu^y e^{-\mu}\frac{1}{y!}\\
=& \quad\exp\left(y \log(\mu) - \mu - \log(y!)\right)
\end{eqnarray*}
\sphinxAtStartPar
Die Link\sphinxhyphen{}Funktion ist hier \(\log(\mu)\). Beachte bitte, dass die Poisson\sphinxhyphen{}Verteilung keinen Streuungsparameter besitzt.


\subsection{Bernoulli distribution \protect\(\Rightarrow\protect\) logistic regression}
\label{\detokenize{Regression_Techniques:bernoulli-distribution-rightarrow-logistic-regression}}
\sphinxAtStartPar
Zuguter Letzte, die Bernoulli Verteilung, von der wir die Logistische Regression ableiten können.
Die Bernoulli Verteilung eignet sich um binäre Ereignisse zu modellieren, die sich gegenseitig ausschliessen. Ein klassisches Beispiel ist der wiederholte Münzwurf. Die Wahrscheinlichkeit für ‘Kopf’ wird mit \(\pi\) bezeichnet, dir für ‘Zahl’ mit \((1-\pi)\). Hiermit lässt sich die Wahrscheinlichkeit berechnen, mit einer fairen Münze bei 10 Würfen eine bestimmte Sequenz mit genau 7 Mal ‘Kopf’ zu erhalten:
\label{equation:Regression_Techniques:f1356535-f83c-4660-9465-9207a7ace703}\begin{equation}
\pi^7 (1-\pi)^3 = 0.5^7 0.5^3 = 0.5^{10} = 0.0009765625
\end{equation}
\sphinxAtStartPar
\sphinxstylestrong{Vorsicht}, wenn wir die Wahrscheinlichkeit für Sequenzen mit genau 7 Mal Kopf berechnen wollen, benötigen wir noch den Binomial\sphinxhyphen{}Koeffizienten, der uns die Anzahl an möglichen Sequenzen mit 7 Mal ‘Kopf’ angibt.

\sphinxAtStartPar
Jetzt zeige ich, wie wir die Bernoulli Verteilung so umschreiben können, dass man ihre Zugehörigkeit zur exponential family von Verteilungen erkennt:
\begin{eqnarray*}
f(y |\pi) =& \pi^y (1-\pi)^{1-y} = \exp\left(y \log(\pi) + (1-y) \log(1-\pi)\right)\\
= & \quad \exp\left(y \log(\pi) + \log(1-\pi) - y\log(1-\pi)\right)\\
=&\quad \exp\left(y\log(\frac{\pi}{1-\pi}) + \log(1-\pi)\right),\quad\text{wobei}
\end{eqnarray*}
\sphinxAtStartPar
sich die Link\sphinxhyphen{}Funktion zu \(\log(\frac{\pi}{1-\pi})\) ergibt. Diese Funktion wird auch als Logit\sphinxhyphen{}Funktion bezeichnet. Die Umkehrfunktion der Logit\sphinxhyphen{}Funktion ist die \sphinxstylestrong{Logistische Funktion}. Es ist also die Logit\sphinxhyphen{}Funktion, die als lineare Kombination der unabhängigen Variablen modelliert wird.
\(\log(\frac{\pi}{1-\pi}) = a + b_{1}x_1 + \ldots + b_jx_j\). Wenn wir den rechten Teil dieser Gleichung in die Logistische Funktion einsetzen erhalten wir die geschätzten Wahrscheinlichkeiten:
\label{equation:Regression_Techniques:dae27466-6566-4874-bb3b-cdca3adf7841}\begin{equation}
P(y=1 |x) = \frac{\exp(a + b_{1}x_1 + \ldots + b_jx_j)}{1 + \exp(a + b_{1}x_1 + \ldots + b_jx_j)}.
\end{equation}

\chapter{Neural Network}
\label{\detokenize{Regression_Techniques:neural-network}}
\sphinxAtStartPar
Es ist auch möglich Neuronale Netzwerke unter dem Blickwinkel der Linearen Regression zu betrachten. Ein Netzwerk mit nur einer Eingabe\sphinxhyphen{}Schicht und einem Neuron wird als Perceptron bezeichnet. Die Aktivierungs\sphinxhyphen{}Funktion dieses Neurons ist entweder die Identitäts\sphinxhyphen{}Funktion, so wie in der klassischen Linearen Regression oder die Logistische Funktion wie in der Logistischen Regression. In letzterem Fall soll das Perceptron Wahrscheinlichkeiten für binäre Ereignisse bestimmen.

\noindent\sphinxincludegraphics{{Regression_Techniques_37_0}.png}


\section{classical linear regression}
\label{\detokenize{Regression_Techniques:classical-linear-regression}}
\sphinxAtStartPar
Im Jargon der neural network community werden unsere \(b\)\sphinxhyphen{}Koeffizienten als \sphinxstylestrong{Gewichte} bezeichnet. Der intercept \(\alpha\) heisst \sphinxstylestrong{bias}.
Erinnert Euch, dass wir den intercept \(\alpha\) in den Vektor \(\pmb{\beta}\) der \(b\)\sphinxhyphen{}Koeffizienten aufgenomen haben, indem wir eine Einser\sphinxhyphen{}Spalte in die Variablen\sphinxhyphen{}Matrix \(\mathbf{X}\) eingefügt haben. Wir konnten also Schreiben:
\begin{equation*}\mathbf{y} = \mathbf{X} \pmb{\beta}\end{equation*}
\sphinxAtStartPar
In der obigen Graphik könnt ihr sehen, dass im Perceptron die Input\sphinxhyphen{}Variablen mit den Gewichten der Verbindungen multipliziert werden und dass der konstante Wert \(\alpha\) hinzu addiert wird. Wie in der Linearen Regression werden diese Produkte dann aufsummiert.

\sphinxAtStartPar
Im Kontext Neuronaler Netzwerke wird der Vektor \(\pmb{\beta}\) als Netzwerk\sphinxhyphen{}Gewichte bezeichnet und wird mit \(\mathbf{W}\) angegeben. Wir hatten gelernt, dass Vektoren mit kleinen Buchstaben bezeichnet werden. In einem richtigen Neuronalen Netz haben wir in einer Schicht viel Perceptrons nebeneinander. Alle erhalten aber den Input aus der darunter liegenden Schicht. Fügt man die Gewichts\sphinxhyphen{}Vektoren der einzelnen Neurone in eine Matrix zusammen, erhält man \(\mathbf{W}\).
Neuronale Netzwerke sind also eigentlich nur viele parallele und hintereinander geschaltete Regressionen, die sehr effizient mit Matrizen\sphinxhyphen{}Multiplikation gerechnet werden können.

\noindent\sphinxincludegraphics{{Regression_Techniques_39_0}.png}


\section{logistic regression}
\label{\detokenize{Regression_Techniques:logistic-regression}}
\sphinxAtStartPar
Für die logistische Aktivierungs\sphinxhyphen{}Funktion schreiben wir:
\begin{equation*}
P(y=1 |x) = \frac{\exp(a + b_{1}x_1 + \ldots + b_jx_j)}{1 + \exp(a + b_{1}x_1 + \ldots + b_jx_j)}
\end{equation*}
\sphinxAtStartPar
Diese Funktion nähert sich asymptotisch der \(0\) für sehr kleinen Werte und der \(1\) für sehr grosse Werte.


\subsection{Weight decay}
\label{\detokenize{Regression_Techniques:weight-decay}}
\sphinxAtStartPar
In der Literatur zu Neuronalen Netzwerken wird der \(l_2\) Strafterm als “weight decay” bezeichnet.
Dieser Strafterm ist Teil des optimizers und nicht der einzelnen Neurone. Wie auch für Ridge Regression wird weight in die Fehler\sphinxhyphen{}Funktion mit aufgenommen:
\begin{equation*}
L' = L + \lambda\sum_i w_i^2, 
\end{equation*}
\sphinxAtStartPar
mit \(L\) als Loss (oder Fehler) und den \(w_i\) als die Gewichte der eingehenden Verbindungen der Neurone.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Trees}

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
Clustering

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Why trees?}
\label{\detokenize{Tree_Methods:why-trees}}\label{\detokenize{Tree_Methods::doc}}
\noindent\sphinxincludegraphics{{Tree_Methods_3_0}.png}

\sphinxAtStartPar
image taken form \sphinxhref{https://web.stanford.edu/~hastie/Papers/ESLII.pdf}{p.370}

\sphinxAtStartPar
In nachfolgender Graphik wird demonstriert, wie ein decision\sphinxhyphen{}tree classifier nach und nach den Input\sphinxhyphen{}Variablen\sphinxhyphen{}Raum unterteilt um möglichst reine Unterräume zu erhalten. Diese Unterräume entsprechen den jeweiligen Knoten im Baum (nodes), bzw. den Blättern.
Wichtig ist, dass diese splits auf einer Variablen immer \sphinxstylestrong{orthogonal} sind.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
findfont: Font family [\PYGZsq{}Arial\PYGZsq{}] not found. Falling back to DejaVu Sans.
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Tree_Methods_6_1}.png}

\sphinxAtStartPar
Dies ist der decision\sphinxhyphen{}trees mit den splits, wie sie oben dargestellt sind.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
findfont: Font family [\PYGZsq{}Arial\PYGZsq{}] not found. Falling back to DejaVu Sans.
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Tree_Methods_9_1}.png}


\section{Compare to linear Regression (logistic regression in this case)}
\label{\detokenize{Tree_Methods:compare-to-linear-regression-logistic-regression-in-this-case}}
\sphinxAtStartPar
Logistic Regression only works for binary classes. But we can always classify \_\_o\_\_ne class \_\_v\_\_ersus the \_\_r\_\_est (ovr) of the other classes \sphinxhyphen{} this allows multiclass classification with logistic regression.
We can see, whereas the classification tree can approximate \sphinxstylestrong{non\sphinxhyphen{}linear seperating lines} with many rectangular splits, the logistic regression can only do linear splits.

\noindent\sphinxincludegraphics{{Tree_Methods_11_0}.png}


\section{Splitting criteria}
\label{\detokenize{Tree_Methods:splitting-criteria}}
\sphinxAtStartPar
For most variants of classification trees, there are basically two important splitting statistics:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Gini Impurity

\item {} 
\sphinxAtStartPar
Entropy

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Gini Impurity}
\label{equation:Tree_Methods:4ada277c-c74a-4ef0-a040-17abfa01fea7}\begin{equation}
\text{Gini} = \sum_i^n p_i (1-p_i)
\end{equation}
\sphinxAtStartPar
For a binary classification problem (either 0 or 1) the Gini\sphinxhyphen{}Index is:
\label{equation:Tree_Methods:0548df43-c4d3-4091-8e5f-0f20eb280f03}\begin{equation}
\text{Gini} = p_1 (1-p_1) + p_2 (1-p_2) 
\end{equation}
\sphinxAtStartPar
Here, \(p_1\) is the purity of the respective node in the tree.

\noindent\sphinxincludegraphics{{Tree_Methods_13_0}.png}

\sphinxAtStartPar
At the root of the tree, we have the following impurities for class 1 and class 0:
\begin{eqnarray*}
p_{01} =& \frac{40}{120} = 0.3333\\
p_{00} =& \frac{80}{120} = 0.6666
\end{eqnarray*}
\sphinxAtStartPar
The Gini\sphinxhyphen{}Impurity at the root is given by:
\label{equation:Tree_Methods:6200a050-dc38-4cd0-9e5e-a17bdfd8ee63}\begin{equation}
\text{Gini}_{0} = p_{01}\cdot (1-p_{01}) + p_{00}\cdot (1-p_{00}) = 0.4444
\end{equation}


\sphinxAtStartPar
Next we compute the Gini\sphinxhyphen{}Impurities \(\text{Gini}_1\) and \(\text{Gini}_2\) for the child\sphinxhyphen{}nodes after the first split:
\begin{eqnarray*}
p_{11} =& \frac{28}{70} = 0.4\\
p_{10} =& \frac{42}{70} = 0.6
\end{eqnarray*}
\sphinxAtStartPar
The Gini\sphinxhyphen{}Impurity of the first child node is given by:
\label{equation:Tree_Methods:fd6c44de-8439-4796-9e19-79ac3d755d44}\begin{equation}
\text{Gini}_{1} = p_{11}\cdot (1-p_{11}) + p_{10}\cdot (1-p_{10}) = 0.48
\end{equation}
\sphinxAtStartPar
For the second child node, we get:
\begin{eqnarray*}
p_{21} =& \frac{12}{50} = 0.24\\
p_{20} =& \frac{38}{50} = 0.76
\end{eqnarray*}
\sphinxAtStartPar
The Gini\sphinxhyphen{}Impurity of the second child node is given by:
\label{equation:Tree_Methods:ebd986fa-75a4-481b-9216-483ff272c057}\begin{equation}
\text{Gini}_{2} = p_{21}\cdot (1-p_{21}) + p_{20}\cdot (1-p_{20}) = 0.3648
\end{equation}
\sphinxAtStartPar
In the left child node, there are 70 observations, whereas in the right child node, we only have 50 observations. To compute the overall Gini\sphinxhyphen{}impurity after the first split, we have the weight the Gini\sphinxhyphen{}Impurities of the two child\sphinxhyphen{}nodes with the fraction of observations they represent:
\label{equation:Tree_Methods:7a93fe22-5164-436c-8a89-43a24eb7d1d0}\begin{equation}
\text{Gini}_{\text{split1}} = \frac{70}{120} \cdot 0.48 + \frac{50}{120} \cdot 0.3658 = 0.4320
\end{equation}


\sphinxAtStartPar
Instead of Gini\sphinxhyphen{}Impurity, we could just take classification error as a criterion \sphinxhyphen{} this seems most intuitive:
The classification error in the root node is given by:
\label{equation:Tree_Methods:f5347641-3775-4b8f-97af-516f30a657c6}\begin{equation}
p_0 = \frac{40}{120} = \mathbf{0.333}
\end{equation}
\sphinxAtStartPar
The classification error in the first child\sphinxhyphen{}node is:
\label{equation:Tree_Methods:510fa1fa-cc82-41bb-99db-a54431f8ad5f}\begin{equation}
p_1 = \frac{28}{70} = 0.4
\end{equation}
\sphinxAtStartPar
And the classification error in the second child\sphinxhyphen{}node is given by: 
\label{equation:Tree_Methods:f3dee4ec-c287-474e-bf67-333339dc1c82}\begin{equation}
p_2 = \frac{12}{50} = 0.24
\end{equation}
\sphinxAtStartPar
Now, to compute the reduction in classifiication\sphinxhyphen{}error, we have again to weigh the two nodes by the number of observations the contain:

\sphinxAtStartPar
Classification\sphinxhyphen{}error after the first split is:
\label{equation:Tree_Methods:2153ebbd-f4ca-4668-9d4b-d6dde9099e74}\begin{equation}
\frac{70}{120} \cdot 0.4 + \frac{50}{120} \cdot 0.24 = \mathbf{0.333}
\end{equation}



\chapter{Genetic Algorithms}
\label{\detokenize{Tree_Methods:genetic-algorithms}}
\sphinxAtStartPar
\sphinxincludegraphics{{John_Holland}.png}

\noindent\sphinxincludegraphics{{Tree_Methods_18_0}.png}

\sphinxAtStartPar
\sphinxincludegraphics{{genetic_algorithm_trend}.png}


\section{Evolutionary Decision Trees:}
\label{\detokenize{Tree_Methods:evolutionary-decision-trees}}
\sphinxAtStartPar
Genetic Algorithms try to mimic the genetic recombination happening in sexual reproduction:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Crossing Over: random recombination between the paired chromosomes inherited from each of one’s parents, generally occurring during meiosis (wiki);

\item {} 
\sphinxAtStartPar
fertilization: haploid chromosomes from a random mother and a random father form a new diploid set of chromosomes

\item {} 
\sphinxAtStartPar
mutation: randomly, some genes may change accidentally

\end{itemize}

\sphinxAtStartPar
Survival of the Fittest:
\begin{itemize}
\item {} 
\sphinxAtStartPar
only a certain number of the offspring passes the evolutionary bottleneck (the best adapted ones + some randomness)

\item {} 
\sphinxAtStartPar
the survivors form the next parent generation with probability proportional to their fitness

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Applied to decision trees on can:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
start growing a generation of decision trees, and recombine the fittest trees

\item {} 
\sphinxAtStartPar
grow and prune decision trees with evolutionary principles

\end{itemize}


\section{my opinion about evolutionary algorithms:}
\label{\detokenize{Tree_Methods:my-opinion-about-evolutionary-algorithms}}
\sphinxAtStartPar
Before the advent of modern machine\sphinxhyphen{}learning algorithms, most algorithms (classification\sphinxhyphen{}trees) where optimized in a hill\sphinxhyphen{}climbing fashion: \sphinxstylestrong{straight to the top}
But as we all know, the seemingly shortest path is not necessarily the best one.
The straight path may end, for example, on a steep cliff, i.e the algorithm finds a local optimum that is not identical with the global optimum.
Evolutionary optimization methods are a way to explore the search space in a more random fashion \sphinxhyphen{} avoiding getting stuck in local optima and hopefully finding the local optimum.

\sphinxAtStartPar
\sphinxstylestrong{BUT}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
There is no guarantee that these algorithms will succeed.

\item {} 
\sphinxAtStartPar
The search can be very long\sphinxhyphen{}lasting.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{AND}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Modern Algorithms \sphinxhyphen{} as for example Random Forest or Gradient Boosting Trees \sphinxhyphen{} have randomness build in.

\end{itemize}


\section{Random Forest}
\label{\detokenize{Tree_Methods:random-forest}}
\sphinxAtStartPar
Random Forest is an example of classifier Bootstrap Aggregation or bagging.
\begin{itemize}
\item {} 
\sphinxAtStartPar
trees are not very deep (only stumps)

\item {} 
\sphinxAtStartPar
each tree is build on a subsample of data and/or columns – choosen randomly

\item {} 
\sphinxAtStartPar
results of the individual trees are aggregated (mean)

\end{itemize}


\subsection{Pros of Random Forest:}
\label{\detokenize{Tree_Methods:pros-of-random-forest}}\begin{itemize}
\item {} 
\sphinxAtStartPar
trees can be trained independently: easy to parallelize

\item {} 
\sphinxAtStartPar
classification and regression possible

\item {} 
\sphinxAtStartPar
all other pros of trees like: handling of missing values, insensitive to outliers, numerical and categorical data, etc..

\item {} 
\sphinxAtStartPar
can give a variance estimate (confidence intervals): mean prediction and variance of prediction (see SMAC in AutoML)

\item {} 
\sphinxAtStartPar
averaging allows for arbitrary non\sphinxhyphen{}linear relationships

\end{itemize}


\subsection{Cons of Random Forest:}
\label{\detokenize{Tree_Methods:cons-of-random-forest}}\begin{itemize}
\item {} 
\sphinxAtStartPar
black box algorithm: hard to interpret; (see feature importance)

\end{itemize}

\noindent\sphinxincludegraphics{{Tree_Methods_23_0}.png}

\noindent\sphinxincludegraphics{{Tree_Methods_24_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}IPython.core.display.HTML object\PYGZgt{}
\end{sphinxVerbatim}


\chapter{Gradient Boosted Trees}
\label{\detokenize{Tree_Methods:gradient-boosted-trees}}
\noindent\sphinxincludegraphics{{Tree_Methods_27_0}.png}


\section{Explanation of Gradient Boosted Trees:}
\label{\detokenize{Tree_Methods:explanation-of-gradient-boosted-trees}}\begin{itemize}
\item {} 
\sphinxAtStartPar
we start with a prediction \(f_0(x)\), i.e. the mean of \(y\) (\(\bar{y}\)) for regression or the most frequent class in case of classification

\item {} 
\sphinxAtStartPar
the difference between the actual values \(y_i\) and our initial start value \(f_0(x)\) (residuals) is to be predicted by the first tree \(T_1\); the tree uses the variables \(\pmb{X}\) to find a rule to group similar residuals in common nodes.

\item {} 
\sphinxAtStartPar
the new prediction of the tree \(T_1\) is added to our initial start value and weighted by the learning parameter; now we get our prediction at iteration \(1\): \(f_1(x) = f_0 + \alpha T_1(\pmb{x}, y - f_0(x))\), i.e. we train a tree \(T_1\) to correctly classify the residuals \(y - f_0(x)\) with a rule induced on \(\pmb{x}\) – our variables. The result of this tree is weighted with leraning\sphinxhyphen{}rate \(\alpha\) and added to the current estimate \(f_0(x)\).

\item {} 
\sphinxAtStartPar
this procedure is repeated until we can not find any more trees that substantially reduce our error or until the maximum number of iterations is reached

\end{itemize}

\sphinxAtStartPar
Since there are many different loss\sphinxhyphen{}functions possible for Gradient Boosting Trees (not only regression), we are looking for a more general formula that defines the best update to our current prediction made by the next tree.


\section{most important parameters for stochastic gradient\sphinxhyphen{}boosting:}
\label{\detokenize{Tree_Methods:most-important-parameters-for-stochastic-gradient-boosting}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{learning\_rate}: the factor \(\alpha\) in the above graphic

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{subsample}: takes part of the data without replacement; prevents overfitting

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{feature\_fraction}: select a subset of the features for the next tree; prevents overfitting

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{num\_leaves}: number of leaves; lightgbm fits level\sphinxhyphen{}wise and leaf\sphinxhyphen{}wise; prevents overfitting

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{max\_depth}: number of levels to grow the tree; prevents overfitting

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{num\_iterations}: number of trees to grow

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{max\_bin}: in lightgbm all features are discretized by binning them; the number of bins for a feature is given by \sphinxstylestrong{max\_bin} (this is what makes lightgbm super\sphinxhyphen{}fast)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{lambda\_l1/lambda\_l2}: regularizes the leaf\sphinxhyphen{}weights; \sphinxstylestrong{excurs}: the result assigned to all cases that end up in one leaf is called the weight; for regression this is a continuous value and also for classification since the result is ultimately passed through a sigmoid\sphinxhyphen{}function that assigns then values between 0 and 1; 

\end{itemize}

\sphinxAtStartPar
For a speed comparison between lightgbm and xgboost see e.g. \sphinxhref{https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031}{results from 2017} Meanwhile xgboost catched up with lightgbm.
A good paper, describing how the features of lightgbm have beed added to xgboost \sphinxhref{https://drive.google.com/file/d/0B0c0MbnP6Nn-eUNRRkVOOGpkbFk/view}{is this one.}
Less mathematical however, is this report here:
https://everdark.github.io/k9/notebooks/ml/gradient\_boosting/gbt.nb.html\#5\_lightgbm
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Morning
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Modelling \& Cross Validation

\item {} 
\sphinxAtStartPar
data leakage \& dependent data

\item {} 
\sphinxAtStartPar
imbalanced data (example in python)

\item {} 
\sphinxAtStartPar
study: Ebanking Fraud

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}



\item {} 
\sphinxAtStartPar
Afternoon
\begin{itemize}
\item {} 
\sphinxAtStartPar
Data Basics \& historical perspective

\item {} 
\sphinxAtStartPar
Linear Regression

\item {} 
\sphinxAtStartPar
Trees

\item {} 
\sphinxAtStartPar
house prices (regression example in python)

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Clustering}

\item {} 
\sphinxAtStartPar
bonus: Hyperparameter Optimization and AutoML

\item {} 
\sphinxAtStartPar
Q\&A

\end{itemize}

\end{enumerate}


\chapter{Clustering}
\label{\detokenize{Text_Clustering:clustering}}\label{\detokenize{Text_Clustering::doc}}
\sphinxAtStartPar
Clustering is one of the hardest problems in ML, because:
\begin{itemize}
\item {} 
\sphinxAtStartPar
you do not know how many clusters there are

\item {} 
\sphinxAtStartPar
it is unclear, which variables to use

\item {} 
\sphinxAtStartPar
the scale of the variables is important for similarity

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Automatically created module for IPython interactive environment
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_6_1}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_7_0}.png}


\section{SOM}
\label{\detokenize{Text_Clustering:som}}
\sphinxAtStartPar
https://www.scribd.com/document/380399478/Teuvo\sphinxhyphen{}Kohonen\sphinxhyphen{}Self\sphinxhyphen{}Organizing\sphinxhyphen{}Maps

\sphinxAtStartPar
Also mention: Vector Quantization

\sphinxAtStartPar
\sphinxstylestrong{Teuvo Kohonen, 1995:} Self\sphinxhyphen{}Organizing Maps

\noindent\sphinxincludegraphics{{Text_Clustering_10_0}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_11_0}.png}

\sphinxAtStartPar
\sphinxincludegraphics{{SOM_Google_trend}.png}



\sphinxAtStartPar
\sphinxincludegraphics{{U_matrix1}.png}\sphinxincludegraphics{{U_matrix2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_15_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_15_2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_16_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_16_2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_17_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_17_2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_18_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_18_2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_19_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_19_2}.png}

\noindent\sphinxincludegraphics{{Text_Clustering_20_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
([], [])
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_20_2}.png}


\section{Some considerations regarding the scale of used variables}
\label{\detokenize{Text_Clustering:some-considerations-regarding-the-scale-of-used-variables}}
\sphinxAtStartPar
In clustering we try to find observations that are near to each other and further away from dissimilar observations. The Notion of ‘similar’ only makes sense, when we pick out some variables that seem to be salient for the comparison of observations.
Most often we use similarity measures as for example Euclidean Distance
\label{equation:Text_Clustering:843e3f50-f2f4-4095-b9ba-9f83cc89b9a4}\begin{equation}
D_{m, n} = \sqrt{\sum_{i=0}^n (m_i - n_i)^2}
\end{equation}
\sphinxAtStartPar
Here, \(m\) and \(n\) are two observations with measures on \(n\) different variables. The Euclidean Distance is well known from the \sphinxstylestrong{Pythagorean theorem}: \(a^2 + b^2 = c^2\); it is just its extensions to more than two dimensions.

\sphinxAtStartPar
What happens when the Variables are scaled differently? Consider for example: \sphinxstylestrong{absolute spending in different categories vs. relative (percentage of) spending in different categories}

\sphinxAtStartPar
\sphinxstylestrong{absolute spendings}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_23_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
              a             b             c
a      0.000000  22219.023381  21100.947846
b  22219.023381      0.000000  28928.100525
c  21100.947846  28928.100525      0.000000
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxstylestrong{relative spendings}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_26_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          a         b         c
a  0.000000  0.227983  0.252631
b  0.227983  0.000000  0.410489
c  0.252631  0.410489  0.000000
\end{sphinxVerbatim}

\sphinxAtStartPar
As long as all variables have the same meaning and the same scale, clustering is still straight forward. But what are we going to do, if we have different variables with different scales? Consider adding the Age of the customers. A Difference in Age of 40 years adds not much to the distance of absolute spendings (in thousands of CHF), But will dominate the distance measure for relative spendings (percentages between 0 and 1).


\section{Spectral clustering}
\label{\detokenize{Text_Clustering:spectral-clustering}}
\sphinxAtStartPar
Spectral clustering originates from Graph Theory (Spectral Analysis)
The algorithms works as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
a connectivity matrix / affinity matrix W is formed. This matrix is symmetric and is n x n, where n is the number of observations. If there is a link between observation i and observation j there is a 1 in the matrix at positions \(W_{ij}\) and \(W_{ji}\). Alternatively, for the affinity matrix, there is a similarity measure indicating the closeness of two observations.

\item {} 
\sphinxAtStartPar
the degree matrix \(D\) is formed. It is a diagonal matrix that contains for each observation the number of links (or sum of similarities) to other observations.

\end{enumerate}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
the laplacian matrix \(L\) is formed by \(L = D -W\). This matrix has several interesting properties:

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
the smallest eigenvalue is always 0. If all observations are connected (fully connected), there is exactly one eigenvalue that is 0.

\item {} 
\sphinxAtStartPar
the corresponding eigenvector is constant.

\item {} 
\sphinxAtStartPar
are there \(r\) connected components (cluster) in the data and if observations are ordered accordingly, the laplacian has block diagonal form:

\end{itemize}
\begin{equation*}   
 \begin{bmatrix}
   L_{1} & & \\
   & \ddots & \\
   & & L_{r}
 \end{bmatrix}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
the blocks \(L_i\) are proper laplacian matrices on their own. Since each component (cluster) is connected within, \(L_i\) has exactly one eigenvalue that is 0. The corresponding eigenvector is constant and is zero for all other components. These eigenvectors hence are indicator vectors for their component.

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} this simple example is taken from https://towardsdatascience.com/unsupervised\PYGZhy{}machine\PYGZhy{}learning\PYGZhy{}spectral\PYGZhy{}clustering\PYGZhy{}algorithm\PYGZhy{}implemented\PYGZhy{}from\PYGZhy{}scratch\PYGZhy{}in\PYGZhy{}python\PYGZhy{}205c87271045}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{13}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{[}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{]}
\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{n}{edgecolors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Height\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_31_1}.png}

\sphinxAtStartPar
Next we buid the connectivity matrix \(W\) (also called adjacency matrix). We see, it has three connected components (clusters):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{pairwise\PYGZus{}distances}
\PYG{n}{W} \PYG{o}{=} \PYG{n}{pairwise\PYGZus{}distances}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{euclidean}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{vectorizer} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vectorize}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{l+m+mi}{1} \PYG{k}{if} \PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{5} \PYG{k}{else} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{W} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vectorize}\PYG{p}{(}\PYG{n}{vectorizer}\PYG{p}{)}\PYG{p}{(}\PYG{n}{W}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{W}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 1 1 1 0 0 0 0 0 0 0 0]
 [1 1 1 1 0 0 0 0 0 0 0 0]
 [1 1 1 1 0 0 0 0 0 0 0 0]
 [1 1 1 1 0 0 0 0 0 0 0 0]
 [0 0 0 0 1 1 1 1 0 0 0 0]
 [0 0 0 0 1 1 1 1 0 0 0 0]
 [0 0 0 0 1 1 1 1 0 0 0 0]
 [0 0 0 0 1 1 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 1 1 1]
 [0 0 0 0 0 0 0 0 1 1 1 1]
 [0 0 0 0 0 0 0 0 1 1 1 1]
 [0 0 0 0 0 0 0 0 1 1 1 1]]
\end{sphinxVerbatim}

\sphinxAtStartPar
Next we buid the degree matrix \(D\) and finally the laplacian as \(L = D - W\)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}D = np.diag(np.reshape(W.dot(np.ones((W.shape[1], 1))), (W.shape[1],)))}
\PYG{n}{D} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{W}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{D}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[4 0 0 0 0 0 0 0 0 0 0 0]
 [0 4 0 0 0 0 0 0 0 0 0 0]
 [0 0 4 0 0 0 0 0 0 0 0 0]
 [0 0 0 4 0 0 0 0 0 0 0 0]
 [0 0 0 0 4 0 0 0 0 0 0 0]
 [0 0 0 0 0 4 0 0 0 0 0 0]
 [0 0 0 0 0 0 4 0 0 0 0 0]
 [0 0 0 0 0 0 0 4 0 0 0 0]
 [0 0 0 0 0 0 0 0 4 0 0 0]
 [0 0 0 0 0 0 0 0 0 4 0 0]
 [0 0 0 0 0 0 0 0 0 0 4 0]
 [0 0 0 0 0 0 0 0 0 0 0 4]]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{L} \PYG{o}{=} \PYG{n}{D} \PYG{o}{\PYGZhy{}} \PYG{n}{W}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{L}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 3 \PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1  0  0  0  0  0  0  0  0]
 [\PYGZhy{}1  3 \PYGZhy{}1 \PYGZhy{}1  0  0  0  0  0  0  0  0]
 [\PYGZhy{}1 \PYGZhy{}1  3 \PYGZhy{}1  0  0  0  0  0  0  0  0]
 [\PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1  3  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  3 \PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1  0  0  0  0]
 [ 0  0  0  0 \PYGZhy{}1  3 \PYGZhy{}1 \PYGZhy{}1  0  0  0  0]
 [ 0  0  0  0 \PYGZhy{}1 \PYGZhy{}1  3 \PYGZhy{}1  0  0  0  0]
 [ 0  0  0  0 \PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1  3  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  3 \PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1]
 [ 0  0  0  0  0  0  0  0 \PYGZhy{}1  3 \PYGZhy{}1 \PYGZhy{}1]
 [ 0  0  0  0  0  0  0  0 \PYGZhy{}1 \PYGZhy{}1  3 \PYGZhy{}1]
 [ 0  0  0  0  0  0  0  0 \PYGZhy{}1 \PYGZhy{}1 \PYGZhy{}1  3]]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} eigenvalues and eigenvectors}
\PYG{n}{eigenvalues}\PYG{p}{,} \PYG{n}{eigenvectors} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{eig}\PYG{p}{(}\PYG{n}{L}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} sort these based on the eigenvalues}
\PYG{n}{eigenvectors} \PYG{o}{=} \PYG{n}{eigenvectors}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{eigenvalues} \PYG{o}{=} \PYG{n}{eigenvalues}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{W}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{eigenvalues}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{n}{edgecolors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sorted eigenvalues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{eigenvalues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}eigenvalues\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_37_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{corresponding eigenvectors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{eigenvectors}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
corresponding eigenvectors
[[\PYGZhy{}0.5  0.   0. ]
 [\PYGZhy{}0.5  0.   0. ]
 [\PYGZhy{}0.5  0.   0. ]
 [\PYGZhy{}0.5  0.   0. ]
 [ 0.  \PYGZhy{}0.5  0. ]
 [ 0.  \PYGZhy{}0.5  0. ]
 [ 0.  \PYGZhy{}0.5  0. ]
 [ 0.  \PYGZhy{}0.5  0. ]
 [ 0.   0.  \PYGZhy{}0.5]
 [ 0.   0.  \PYGZhy{}0.5]
 [ 0.   0.  \PYGZhy{}0.5]
 [ 0.   0.  \PYGZhy{}0.5]]
\end{sphinxVerbatim}

\sphinxAtStartPar
This is a somewhat contrieved example but it illustrates the principal idea.
For normal data that is not as clear as in our example, the spectral embedding matrix (eigenvectors belonging to the \(s\) smallest eigenvalues) can be clustered by k\sphinxhyphen{}means. Another possibility is to start with the embedding matrix as a indicator matrix and optimize it as to maximize the in\sphinxhyphen{}component connectivity (association) and minimize the between component connectivity (cuts)(see https://www1.icsi.berkeley.edu/\textasciitilde{}stellayu/publication/doc/2003kwayICCV.pdf). In the python sklearn library the last method is called ‘discretize’.


\section{Example: 20 newsgroups}
\label{\detokenize{Text_Clustering:example-20-newsgroups}}
\sphinxAtStartPar
We are going to demonstrate the virtues of spectral clustering on a text\sphinxhyphen{}clustering example. The data is a common dataset with nearly balanced classes. Hence, k\sphinxhyphen{}means should also be appropriate.

\sphinxAtStartPar
Here is one example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
From: sigma@rahul.net (Kevin Martin)
Subject: Re: Stay Away from MAG Innovision!!!
Nntp\PYGZhy{}Posting\PYGZhy{}Host: bolero
Organization: a2i network
Lines: 10

In \PYGZlt{}16BB58B33.D1SAR@VM1.CC.UAKRON.EDU\PYGZgt{} D1SAR@VM1.CC.UAKRON.EDU (Steve Rimar) writes:
\PYGZgt{}My Mag MX15F works fine....................

Mine was beautiful for a year and a half.  Then it went \PYGZlt{}foomp\PYGZgt{}.  I bought
a ViewSonic 6FS instead.  Another great monitor, IMHO.

\PYGZhy{}\PYGZhy{} 
Kevin Martin
sigma@rahul.net
\PYGZdq{}I gotta get me another hat.\PYGZdq{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{Text_Clustering_41_0}.png}


\section{Organization}
\label{\detokenize{Text_Clustering:organization}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
We build tf\sphinxhyphen{}idf vectors since we are following the bag\sphinxhyphen{}of\sphinxhyphen{}words approach

\item {} 
\sphinxAtStartPar
For building the adjacency/connectivity/affinity matrix we have several possibilities. We will discuss the most relevant ones

\item {} 
\sphinxAtStartPar
we compare different cluster solution and evaluate their results on the 20 different target labels of the newsgroup posts. For doing this, we introduce the the \sphinxstyleemphasis{adusted rand index}.

\item {} 
\sphinxAtStartPar
Since we are always curious we verify if we could have guessed the right number of clusters from the eigenvalue criterion as discussed above

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}\PYGZpc{}reload\PYGZus{}ext autoreload}
\PYG{c+c1}{\PYGZsh{}\PYGZpc{}autoreload 2}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{fetch\PYGZus{}20newsgroups}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{TfidfVectorizer}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{SpectralClustering}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{metrics}
\PYG{k+kn}{import} \PYG{n+nn}{nmslib}
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{fetch\PYGZus{}20newsgroups}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{download\PYGZus{}if\PYGZus{}missing}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} http://qwone.com/\PYGZti{}jason/20Newsgroups/}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{123}\PYG{p}{)}
\PYG{n}{texts} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{data} \PYG{c+c1}{\PYGZsh{} Extract text}
\PYG{n}{target} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{target} \PYG{c+c1}{\PYGZsh{} Extract target}
\PYG{n}{display}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{texts}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
18846
\end{sphinxVerbatim}


\chapter{Tf\sphinxhyphen{}idf}
\label{\detokenize{Text_Clustering:tf-idf}}
\sphinxAtStartPar
refresher:

\sphinxAtStartPar
\(w_{i,j} = \text{tf}_{i,j} \cdot \log\frac{N}{\text{df}_i}\)with\(\text{tf}_{i,j}\) is the frequency of term i in document j\(\text{df}_i\) is the number of documents containing term i

\sphinxAtStartPar
Since we want to cluster newsgroup posts we are more interested in words that are nearly unique to certain groups. By setting max\_df = 0.3 we ensure that only words are considered that are not too common, i.e. only in 30\% of all posts. By contrast, words that are very seldom but are concerned with special topics discussed in the groups are most important for our endeavor. Hence, there is no limit for min\_df.We tell the vectorizer to remove common english stop words.By default the vectors returned are normed, i.e. they all have length 1. This is important when we compute the cosine similarity between the vectors.


\section{Rand Index}
\label{\detokenize{Text_Clustering:rand-index}}
\sphinxAtStartPar
The rand index is a measure of how well two partitions of a set of objects coincide:
\begin{equation*}
R=\frac{a + b}{a + b + c + d} = \frac{a + b}{{n \choose2}},
\end{equation*}
\sphinxAtStartPar
where\(a\) is the number of pairs of elements that are in the same subset for both partitions\(b\) is the number of pairs of elements that are in different subsets for both partitions\(c\) is the number of pairs of elements that are in the same subset for the first partition but not for the second\(d\) is the number of pairs of elements that are in the different subsets for the first partition but in the same subset for the second partition

\sphinxAtStartPar
The Rand index is the percentage of consistent/congruent decisions for the two partitions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} norm=\PYGZsq{}l2\PYGZsq{} is default}
\PYG{n}{vectorizer} \PYG{o}{=} \PYG{n}{TfidfVectorizer}\PYG{p}{(}\PYG{n}{stop\PYGZus{}words}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{english}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{max\PYGZus{}df} \PYG{o}{=} \PYG{l+m+mf}{0.3}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{vectorizer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{texts}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
18846, 173438
\end{sphinxVerbatim}


\section{Connectivity / adjacency / affinity matrix}
\label{\detokenize{Text_Clustering:connectivity-adjacency-affinity-matrix}}
\sphinxAtStartPar
We have several possibilities to express the similarity / connectedness between observations. Some of common ones:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The \(\epsilon\) neighborhood graph: all observations whose pairwise distance is less than \(\epsilon\) are connected. (This was used in our contrieved example)

\item {} 
\sphinxAtStartPar
k\sphinxhyphen{}nearest neighborhood graph: for every observation, the k\sphinxhyphen{}nearest neighbors get connected. Since the nearest neighbors relationsship is not symmetric, we have to adjust the resulting adjacency matrix: \(W_{symm} = 0.5 \cdot W + W^{T}\)

\item {} 
\sphinxAtStartPar
fully connected graph: all observations are connected with weights equal to the respective similarity. Possible choices here are

\end{enumerate}
\begin{itemize}
\item {} 
\sphinxAtStartPar
the gaussian kernel: \(\exp^{-(x_i - x_j)^2/(2\sigma^2)}\); In sklearn the term \(1/(2\sigma^2) = \gamma\); the gaussian kernel is also called radial basis function kernel (rbf) or euclidean kernel. For identical vectors \(\exp^{0} = 1\); For dissimilar vectors the term assymptotically approaches 0.

\item {} 
\sphinxAtStartPar
the cosine kernel: \(\frac{x_i \cdot x_j^T}{||x_i|| ||x_j||}\); here \(||x_i||\) is the norm (length) of vector \(x_i\), normalizing the vector to length 1. When the vectors are already normalized, the cosine similarity simplifies to \(x_i \cdot x_j^T\). This is the linear kernel. The cosine measures the angle between two vectors. For identical vectors, the cosine is 1, for orthogonal vectors it is 0.

\end{itemize}


\chapter{our choice}
\label{\detokenize{Text_Clustering:our-choice}}
\sphinxAtStartPar
Since we deal with very long and sparse vectors, euclidean distance is not a good choice. This has to do with the \sphinxstyleemphasis{curse of dimensionality} (search for it). Typically, for these long tf\sphinxhyphen{}idf vectors cosine similarity is used. For bigger data sets, I can not see any reason why we should compute the similarities for all points in the set. Probably, it is sufficient to consider only th \(k\) nearest neighbors of each point. This allows us to use sparse matrices, that are far more memory efficient. The sklearn spectral clustering implementation can deal with those matrices. As we will see, this simplification further reduces noise in the data and encourages better overall solutions.


\chapter{Approximate nearest neighbors}
\label{\detokenize{Text_Clustering:approximate-nearest-neighbors}}
\sphinxAtStartPar
Computing all mutual distances between the 18846 posts can be very time and memory consuming. Theoretically, we only have to compute the triangular matrix minus the diagonal (distance matrices are symmetrical):
\(0.5 \cdot 18846^2 - 18846/2 = 177576435\) mutual distances. A clever idea here, is to compute only the k nearest neighbors of each post and treat all other posts as maximal distant. There are a lot of algorithms around for computing nearest neighbors, the most efficient relying on kd\sphinxhyphen{}trees.
However, there are also approximate nearest neighbor algorithms that are nearly 100\% accurate and are even faster than kd\sphinxhyphen{}trees. One of those is the hnsw\sphinxhyphen{}algorithm as implemented in the python nmslib (an api for the underlying c++ code). 
https://github.com/erikbern/ann\sphinxhyphen{}benchmarks

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} see discussions about \PYGZsq{}curse of dimensionality\PYGZsq{}; to be safe, we opt for cosine\PYGZhy{}similarity}
\PYG{c+c1}{\PYGZsh{} since we want to be most efficient in everything we do, we use sparse matrices and vectors}
\PYG{n}{index} \PYG{o}{=} \PYG{n}{nmslib}\PYG{o}{.}\PYG{n}{init}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hnsw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{space}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cosinesimil\PYGZus{}sparse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data\PYGZus{}type}\PYG{o}{=}\PYG{n}{nmslib}\PYG{o}{.}\PYG{n}{DataType}\PYG{o}{.}\PYG{n}{SPARSE\PYGZus{}VECTOR}\PYG{p}{)}
\PYG{n}{index}\PYG{o}{.}\PYG{n}{addDataPointBatch}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{index\PYGZus{}time\PYGZus{}params} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{post}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}
\PYG{n}{index}\PYG{o}{.}\PYG{n}{createIndex}\PYG{p}{(}\PYG{n}{index\PYGZus{}time\PYGZus{}params}\PYG{p}{,} \PYG{n}{print\PYGZus{}progress}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now we can query the index}
\PYG{c+c1}{\PYGZsh{} By computing distances for the k=1000 nearest neighbors, we have to store 1000 * 18846 = 18846000}
\PYG{c+c1}{\PYGZsh{} distances; but compared to the triangular matrix approach discussed above, we still save 158730435}
\PYG{c+c1}{\PYGZsh{} entries in our matrix.}

\PYG{n}{nn} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{neighbors} \PYG{o}{=} \PYG{n}{index}\PYG{o}{.}\PYG{n}{knnQueryBatch}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{k}\PYG{o}{=}\PYG{n}{nn}\PYG{p}{,} \PYG{n}{num\PYGZus{}threads}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{neighbors}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[(array([    0, 18553, 11184, 16876, 11946, 16873, 11485,  2729, 11312,
          6760,  7949, 17776, 13476,  3309,  2867,  8311, 12784,  1154,
          7476,  7330,  8755, 13588, 13897, 17132,  6839,  3787,  3042,
           629, 15810, 18704,   333,  3925,  9968, 16990,  8496,  2361,
          6424, 15497,  9822, 11135, 13042, 18583,  5707, 15814,  7020,
          7142, 11961, 11029,  1803, 11246, 10832, 14097, 10344, 15032,
          7931, 13927,   458, 17853, 16067, 14836,  4506, 12084,  2853,
          4459, 15130, 17520, 16621, 10478, 13146, 13207,  8265, 15036,
         12512,  7803, 17900,  6566, 17040,  3349,     7, 13893,  2967,
         14123, 12769, 17020, 18590,  1898, 10391, 14628,  1469, 13250,
          1431, 16369,  3068, 10040, 11045, 10974, 15167,  9902,  6613,
           959], dtype=int32),
  array([0.        , 0.63908607, 0.669317  , 0.6755365 , 0.6795682 ,
         0.6914151 , 0.70722437, 0.7155305 , 0.7231293 , 0.7415422 ,
         0.7510953 , 0.76321185, 0.7724261 , 0.7777292 , 0.77935755,
         0.7860545 , 0.8014517 , 0.8017465 , 0.80473745, 0.8059828 ,
         0.80765975, 0.8108567 , 0.8144285 , 0.81463706, 0.8146501 ,
         0.81604654, 0.81780756, 0.81853294, 0.8226091 , 0.8253168 ,
         0.8269756 , 0.8272094 , 0.82753253, 0.832714  , 0.83565545,
         0.83763695, 0.84240353, 0.8450652 , 0.8454548 , 0.84554344,
         0.8469504 , 0.84886557, 0.85017437, 0.8515262 , 0.8534748 ,
         0.8535583 , 0.8540908 , 0.8569978 , 0.8593518 , 0.86037207,
         0.8613587 , 0.8628519 , 0.8643788 , 0.8650072 , 0.8686269 ,
         0.8700882 , 0.8730786 , 0.8753253 , 0.87538624, 0.87732553,
         0.8785739 , 0.8803475 , 0.8814558 , 0.8818229 , 0.8836487 ,
         0.8837902 , 0.88500327, 0.8850998 , 0.88521737, 0.8865601 ,
         0.88761014, 0.8889066 , 0.8891484 , 0.8891751 , 0.8892081 ,
         0.8903198 , 0.89066046, 0.89234483, 0.89260024, 0.89307475,
         0.89561146, 0.89589816, 0.89832234, 0.89868456, 0.8989996 ,
         0.8998436 , 0.90027505, 0.9010556 , 0.9020781 , 0.9055549 ,
         0.90569705, 0.90630126, 0.90735704, 0.9079704 , 0.9080308 ,
         0.9089797 , 0.909623  , 0.9108277 , 0.9108595 , 0.9108808 ],
        dtype=float32)),
 (array([    1, 11766,  7423,  6468,  6102, 10510, 16899,  1587,  8028,
          8005,  7609,  4800, 17882, 15953,  4582, 12907,  7943, 14461,
          1578,  4132,   437,  4277,  9712,  1425,  4904,  1674, 13677,
          4301, 13416,   777, 12782, 13463, 14806, 13213,  6574, 13653,
          1140, 12095,   820,  2783, 10416,  5639, 10442,  8486,  7638,
         18446,  3751,   845, 17619,  1648,  8586,  8772, 18823,   134,
          6115, 11394, 13339, 17526, 16883,   875,  4739,   738, 13985,
          8460,  6427, 17228,  6889,  4363, 14301,  8563,  1457, 12688,
         15129, 17449,  9731, 12413,  7858, 14259,   253,  9652, 15670,
          8764, 17087,  9998,  6638,  2276,  7980, 17831, 17765, 17358,
          1175, 16537, 16167,  7780, 17516, 14695,  2838,  1503,  4027,
          5806], dtype=int32),
  array([0.        , 0.6764641 , 0.74253815, 0.7553041 , 0.76306784,
         0.7711086 , 0.7799365 , 0.780106  , 0.78269386, 0.7828499 ,
         0.7828758 , 0.784539  , 0.78670365, 0.7882724 , 0.7885906 ,
         0.797265  , 0.80187076, 0.802405  , 0.8029315 , 0.80353254,
         0.80902755, 0.8120299 , 0.8170264 , 0.81749696, 0.8178127 ,
         0.8206302 , 0.8228782 , 0.8252084 , 0.826402  , 0.82888323,
         0.82903314, 0.8292601 , 0.82927394, 0.8304367 , 0.8350926 ,
         0.8382005 , 0.8384994 , 0.84133023, 0.8426109 , 0.84430575,
         0.84561086, 0.84563506, 0.84599364, 0.8468888 , 0.8490473 ,
         0.8501638 , 0.85109127, 0.851439  , 0.85309803, 0.8534955 ,
         0.85463166, 0.8546634 , 0.856802  , 0.85953075, 0.8595509 ,
         0.8596823 , 0.8602279 , 0.861897  , 0.86335284, 0.8658271 ,
         0.8666834 , 0.8676325 , 0.86765236, 0.8682556 , 0.86828136,
         0.86998975, 0.87009454, 0.8708112 , 0.87231827, 0.87314016,
         0.8755579 , 0.87881505, 0.8794431 , 0.8799997 , 0.88102365,
         0.88231266, 0.88626516, 0.8864002 , 0.8870982 , 0.8873377 ,
         0.88741034, 0.88744533, 0.8879509 , 0.89046234, 0.8919709 ,
         0.89297944, 0.8942795 , 0.89578915, 0.89626247, 0.8970282 ,
         0.89725167, 0.89876753, 0.8994845 , 0.900252  , 0.90164286,
         0.90257215, 0.9031204 , 0.9032326 , 0.90526605, 0.9073184 ],
        dtype=float32)),
 (array([    2, 12577,  5478, 15736, 10818,  3973,  1382, 16593,  4949,
         16086,  7747,  7586,   115,   743, 11505,  2070,  6514, 14587,
          9424,  5761, 13039,  2914, 11949,  3991, 10796, 10800, 12879,
         13621,  1791,  3058,  3794,  6028,  2262, 11165, 14922,  8374,
          6057,  1229, 13813, 10241,   991,  2191, 14417,   114, 13170,
         18345, 11710, 15688,  9075, 15820, 17789, 14095, 10126,  1784,
          9368,  2596,  6639,  7615, 10537,  2603, 17523, 18486,   196,
         13365, 16950,  2754,  5598,  8372,  5777,  9200,  2008, 18006,
         17137,   731,  1236,  5967,  7182,  9658,  2427,  6958, 17731,
          5630, 13576, 11702,   858, 15778, 17675, 17458,  2978,  8354,
          5175,  9232,   736, 14002,  5695, 13958, 13424,   552,   521,
          6149], dtype=int32),
  array([0.        , 0.31400347, 0.395284  , 0.40862632, 0.48660403,
         0.6930011 , 0.70131546, 0.7452333 , 0.75990844, 0.76281995,
         0.76501834, 0.76625437, 0.77072847, 0.7735661 , 0.77506244,
         0.77915657, 0.7807083 , 0.78274506, 0.78300846, 0.7831948 ,
         0.78646934, 0.79291785, 0.79638773, 0.8006463 , 0.8021226 ,
         0.80240417, 0.8031671 , 0.8063524 , 0.8082599 , 0.8088174 ,
         0.8091037 , 0.8097666 , 0.810273  , 0.81131786, 0.812379  ,
         0.8135287 , 0.81699   , 0.82013595, 0.8219625 , 0.8223238 ,
         0.8247599 , 0.8261044 , 0.8263263 , 0.82775855, 0.8294813 ,
         0.83061796, 0.8330438 , 0.834819  , 0.83547986, 0.8369826 ,
         0.8370266 , 0.83754146, 0.83761626, 0.83841634, 0.8390262 ,
         0.84067756, 0.84150666, 0.8430389 , 0.8437043 , 0.8454485 ,
         0.8459446 , 0.84668833, 0.84833264, 0.8484504 , 0.8486481 ,
         0.85070646, 0.85274833, 0.85281026, 0.8543711 , 0.85746324,
         0.85863894, 0.86052996, 0.8620279 , 0.8627942 , 0.8636267 ,
         0.8681772 , 0.8683529 , 0.8684674 , 0.86972046, 0.8704043 ,
         0.8710359 , 0.87433565, 0.8752069 , 0.8757434 , 0.87733775,
         0.87839425, 0.8785302 , 0.87890065, 0.8792665 , 0.88187623,
         0.88302463, 0.8832308 , 0.8846249 , 0.88844943, 0.88874626,
         0.88897663, 0.8893845 , 0.8897048 , 0.8897395 , 0.8898656 ],
        dtype=float32))]
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we build our affinity matrix. Sparse matrices only store the indices and the data for entries in the matrix that acutally contain data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{sparse} \PYG{k+kn}{import} \PYG{n}{csc\PYGZus{}matrix}
\PYG{c+c1}{\PYGZsh{} next we construct a sparse matrix with the distances as measured by cosine similarity}
\PYG{n}{col} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n}{neighbors} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} column indices of nearest neighbors}
\PYG{n}{row} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{neighbors}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n}{neighbors}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} the row index for each nearest neighbor}
\PYG{c+c1}{\PYGZsh{} data = np.array([i for n in neighbors for i in n[1].tolist()]) \PYGZsh{} the similarities}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n}{neighbors} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{n}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} the similarities}

\PYG{c+c1}{\PYGZsh{} btw, if you do not understand what\PYGZsq{}s going on in the list part of the construction of the col and}
\PYG{c+c1}{\PYGZsh{} the data variable above, I stronly recommend to have a look at *list comprehension*; }
\PYG{c+c1}{\PYGZsh{} list comprehension is a super fast way to get things done in python}

\PYG{n}{affinity} \PYG{o}{=} \PYG{n}{csc\PYGZus{}matrix}\PYG{p}{(}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{p}{(}\PYG{n}{row}\PYG{p}{,} \PYG{n}{col}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{shape} \PYG{o}{=} \PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} n\PYGZus{}clusters = 20 because we know, that there are 20 different newsgroups}
\PYG{c+c1}{\PYGZsh{} n\PYGZus{}components = 21; this is cheated because it should normally be set to the same number as}
\PYG{c+c1}{\PYGZsh{}                the n\PYGZus{}clusters, but 21 yielded a better solution}
\PYG{c+c1}{\PYGZsh{} assign\PYGZus{}labels = \PYGZsq{}discretize\PYGZsq{} defines how the clusters are found within the }
\PYG{c+c1}{\PYGZsh{}                 embedding matrix (matrix composed of eigenvectors belonging to the n\PYGZus{}components }
\PYG{c+c1}{\PYGZsh{}                 smallest eigenvalues). As discussed above, \PYGZsq{}discretize\PYGZsq{} maximizes the }
\PYG{c+c1}{\PYGZsh{}                 in\PYGZhy{}component connectivity and minimizes the between\PYGZhy{}component connectivity}
\PYG{c+c1}{\PYGZsh{} affinity = \PYGZsq{}precomputed\PYGZsq{} indicates that the affinity matrix is provided by the user}
\PYG{c+c1}{\PYGZsh{} eigen\PYGZus{}solver = \PYGZsq{}amg\PYGZsq{} assumes the python\PYGZhy{}package pyamg is installed. It is by far the fastest}
\PYG{c+c1}{\PYGZsh{}                way the eigenvalue decomposition                }
\PYG{c+c1}{\PYGZsh{}}
\PYG{n}{solution} \PYG{o}{=} \PYG{n}{SpectralClustering}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}components} \PYG{o}{=} \PYG{l+m+mi}{21}\PYG{p}{,} \PYG{n}{assign\PYGZus{}labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{discretize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYGZbs{}
                              \PYG{n}{affinity} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{precomputed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYGZbs{}
                              \PYG{n}{eigen\PYGZus{}solver}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{amg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{p}{(}\PYG{n}{affinity} \PYG{o}{+} \PYG{n}{affinity}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{solution}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.3312465898113292
\end{sphinxVerbatim}

\sphinxAtStartPar
So, let’s see what our nearest\sphinxhyphen{}neighbor trick was good for. In the next cell we compute the spectral clustering for the whole tf\sphinxhyphen{}idf matrix and leave it to sklearn to compute all cosine similarities. As discussed above, since the tf\sphinxhyphen{}idf vectors are by default normalized, we can use the ‘linear’ kernel instead of the ‘cosine’ kernel. This is faster because the additional steps of getting the norms and dividing is not needed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} linear kernel instead of cosine\PYGZus{}similarity if tf\PYGZhy{}idf vectors are already normalized}
\PYG{n}{solution} \PYG{o}{=} \PYG{n}{SpectralClustering}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}components} \PYG{o}{=} \PYG{l+m+mi}{21}\PYG{p}{,} \PYG{n}{assign\PYGZus{}labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{discretize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYGZbs{}
                              \PYG{n}{affinity}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{eigen\PYGZus{}solver}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{amg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{solution}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.275483821009973
\end{sphinxVerbatim}

\sphinxAtStartPar
What would the solution be like if we would have used ordinary euclidean distance (=rbf kernel=gaussian kernel)?
gamma (\(\gamma = \frac{1}{2\sigma^2}\)) defines the size of the neighborhood and is similar to the choice of the parameter \(k\) in the approx. k\sphinxhyphen{}nearest neighbor algorithm we used in the construction of our affinity matrix.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{solution} \PYG{o}{=} \PYG{n}{SpectralClustering}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n\PYGZus{}components} \PYG{o}{=} \PYG{l+m+mi}{21}\PYG{p}{,} \PYG{n}{assign\PYGZus{}labels}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{discretize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYGZbs{}
                              \PYG{n}{affinity}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{eigen\PYGZus{}solver}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{amg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYGZbs{}
                              \PYG{n}{gamma} \PYG{o}{=} \PYG{l+m+mf}{0.7}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{solution}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.17161380707083057
\end{sphinxVerbatim}

\sphinxAtStartPar
How is ordinary k\sphinxhyphen{}means performing? Clustering is done on the tf\sphinxhyphen{}idf vectors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{solutionKMeans} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{init}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k\PYGZhy{}means++}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{max\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}init}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{solutionKMeans}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.1760642244041933
\end{sphinxVerbatim}

\sphinxAtStartPar
Could we have guessed the right number of clusters from the eigenvalues of the laplacian as suggested in all tutorials about spectral clustering?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{sparse}\PYG{n+nn}{.}\PYG{n+nn}{csgraph} \PYG{k+kn}{import} \PYG{n}{laplacian} \PYG{k}{as} \PYG{n}{csgraph\PYGZus{}laplacian}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}array}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}random\PYGZus{}state}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{fixes} \PYG{k+kn}{import} \PYG{n}{lobpcg}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{manifold}\PYG{n+nn}{.}\PYG{n+nn}{\PYGZus{}spectral\PYGZus{}embedding} \PYG{k+kn}{import} \PYG{n}{\PYGZus{}set\PYGZus{}diag}
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{sparse}
\PYG{k+kn}{from} \PYG{n+nn}{pyamg} \PYG{k+kn}{import} \PYG{n}{smoothed\PYGZus{}aggregation\PYGZus{}solver}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{norm\PYGZus{}laplacian} \PYG{o}{=} \PYG{k+kc}{True}
\PYG{n}{random\PYGZus{}state} \PYG{o}{=} \PYG{n}{check\PYGZus{}random\PYGZus{}state}\PYG{p}{(}\PYG{l+m+mi}{1234}\PYG{p}{)}

\PYG{n}{connectivity} \PYG{o}{=}\PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{p}{(}\PYG{n}{affinity} \PYG{o}{+} \PYG{n}{affinity}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}
\PYG{n}{laplacian}\PYG{p}{,} \PYG{n}{dd} \PYG{o}{=} \PYG{n}{csgraph\PYGZus{}laplacian}\PYG{p}{(}\PYG{n}{connectivity}\PYG{p}{,} \PYG{n}{normed}\PYG{o}{=}\PYG{n}{norm\PYGZus{}laplacian}\PYG{p}{,} \PYG{n}{return\PYGZus{}diag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{laplacian} \PYG{o}{=} \PYG{n}{check\PYGZus{}array}\PYG{p}{(}\PYG{n}{laplacian}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{,} \PYG{n}{accept\PYGZus{}sparse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
                                
\PYG{n}{laplacian} \PYG{o}{=} \PYG{n}{\PYGZus{}set\PYGZus{}diag}\PYG{p}{(}\PYG{n}{laplacian}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{norm\PYGZus{}laplacian}\PYG{p}{)}
\PYG{n}{diag\PYGZus{}shift} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}5} \PYG{o}{*} \PYG{n}{sparse}\PYG{o}{.}\PYG{n}{eye}\PYG{p}{(}\PYG{n}{laplacian}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{laplacian} \PYG{o}{+}\PYG{o}{=} \PYG{n}{diag\PYGZus{}shift}
\PYG{n}{ml} \PYG{o}{=} \PYG{n}{smoothed\PYGZus{}aggregation\PYGZus{}solver}\PYG{p}{(}\PYG{n}{check\PYGZus{}array}\PYG{p}{(}\PYG{n}{laplacian}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{csr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{laplacian} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{diag\PYGZus{}shift}
\PYG{n}{n\PYGZus{}components} \PYG{o}{=} \PYG{n}{laplacian}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{ml}\PYG{o}{.}\PYG{n}{aspreconditioner}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{random\PYGZus{}state}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{laplacian}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{n\PYGZus{}components} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{=} \PYG{n}{dd}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{eigs}\PYG{p}{,} \PYG{n}{diffusion\PYGZus{}map} \PYG{o}{=} \PYG{n}{lobpcg}\PYG{p}{(}\PYG{n}{laplacian}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{M}\PYG{o}{=}\PYG{n}{M}\PYG{p}{,} \PYG{n}{tol}\PYG{o}{=}\PYG{l+m+mf}{1.e\PYGZhy{}5}\PYG{p}{,} \PYG{n}{largest}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{eigs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{eigs}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{eigs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{255}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{eigs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{255}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{19}\PYG{p}{]}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{p}{[}\PYG{n}{eigs}\PYG{p}{[}\PYG{l+m+mi}{19}\PYG{p}{]}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ro}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{DBSCAN}
\label{\detokenize{Text_Clustering:dbscan}}
\sphinxAtStartPar
Density\sphinxhyphen{}based clustering of applications with noise.

\sphinxAtStartPar
DBSCAN has the concept of \sphinxstylestrong{core point}, \sphinxstylestrong{border point} and \sphinxstylestrong{outlier}. A data point is considered a core point if there are sufficient (\sphinxstylestrong{\sphinxstyleemphasis{minPts}}) data points in its vicinity of radius \sphinxstylestrong{\sphinxstyleemphasis{eps}}. Border points are those points that are connected to core points but themselves have not enough data point in their neighborhood. Outliers are the points that are not connected to a core points.

\noindent\sphinxincludegraphics{{Text_Clustering_66_0}.png}

\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/DBSCAN}{The image is taken from wikipedia}
\begin{itemize}
\item {} 
\sphinxAtStartPar
red points (e.g. point A) are \sphinxstylestrong{core points}

\item {} 
\sphinxAtStartPar
yellow points (e.g. points B, c) are \sphinxstylestrong{border points}

\item {} 
\sphinxAtStartPar
the blue point (N) is an \sphinxstylestrong{outlier}

\end{itemize}



\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{DBSCAN}
\PYG{n}{db} \PYG{o}{=} \PYG{n}{DBSCAN}\PYG{p}{(}\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{min\PYGZus{}samples}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{n}{metric}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cosine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}jobs}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{clusters} \PYG{o}{=} \PYG{n}{db}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{clusters}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZhy{}1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
       16, 17, 18, 19, 20])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{metrics}\PYG{o}{.}\PYG{n}{adjusted\PYGZus{}rand\PYGZus{}score}\PYG{p}{(}\PYG{n}{clusters}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{target}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
8.75031255964891e\PYGZhy{}05
\end{sphinxVerbatim}


\chapter{SOM}
\label{\detokenize{Text_Clustering:id1}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{!}pip install minisom
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Collecting minisom
  Downloading MiniSom\PYGZhy{}2.3.0.tar.gz (8.8 kB)
Building wheels for collected packages: minisom
  Building wheel for minisom (setup.py) ... ?25ldone
?25h  Created wheel for minisom: filename=MiniSom\PYGZhy{}2.3.0\PYGZhy{}py3\PYGZhy{}none\PYGZhy{}any.whl size=9018 sha256=82e6f8beb422ec311bb851ae0baca7c5bb967a356f0f79e354173f732e5ee698
  Stored in directory: /home/martin/.cache/pip/wheels/d4/ca/4a/488772b0399fec45ff53132ed14c948dec4b30deee3a532f80
Successfully built minisom
Installing collected packages: minisom
Successfully installed minisom\PYGZhy{}2.3.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{minisom} \PYG{k+kn}{import} \PYG{n}{MiniSom}
\PYG{n}{som} \PYG{o}{=} \PYG{n}{MiniSom}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{input\PYGZus{}len}\PYG{o}{=}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{som}\PYG{o}{.}\PYG{n}{random\PYGZus{}weights\PYGZus{}init}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{som}\PYG{o}{.}\PYG{n}{train\PYGZus{}random}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{X}\PYG{p}{,} \PYG{n}{num\PYGZus{}iteration}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{TypeError}\PYG{g+gWhitespace}{                                 }Traceback (most recent call last)
\PYG{o}{\PYGZlt{}}\PYG{n}{ipython}\PYG{o}{\PYGZhy{}}\PYG{n+nb}{input}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{87}\PYG{o}{\PYGZhy{}}\PYG{n}{bdd4d7e5b832}\PYG{o}{\PYGZgt{}} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{som}\PYG{o}{.}\PYG{n}{random\PYGZus{}weights\PYGZus{}init}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{n}{som}\PYG{o}{.}\PYG{n}{train\PYGZus{}random}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{X}\PYG{p}{,} \PYG{n}{num\PYGZus{}iteration}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/miniconda3/envs/imbalanced/lib/python3.7/site\PYGZhy{}packages/minisom.py} in \PYG{n+ni}{random\PYGZus{}weights\PYGZus{}init}\PYG{n+nt}{(self, data)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{349}         \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Initializes the weights of the SOM}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{350}\PYG{l+s+sd}{         picking random samples from data.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{351}         \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}check\PYGZus{}input\PYGZus{}len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{352}         \PYG{n}{it} \PYG{o}{=} \PYG{n}{nditer}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}activation\PYGZus{}map}\PYG{p}{,} \PYG{n}{flags}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{multi\PYGZus{}index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{353}         \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{it}\PYG{o}{.}\PYG{n}{finished}\PYG{p}{:}

\PYG{n+nn}{\PYGZti{}/miniconda3/envs/imbalanced/lib/python3.7/site\PYGZhy{}packages/minisom.py} in \PYG{n+ni}{\PYGZus{}check\PYGZus{}input\PYGZus{}len}\PYG{n+nt}{(self, data)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{304}     \PYG{k}{def} \PYG{n+nf}{\PYGZus{}check\PYGZus{}input\PYGZus{}len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{data}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{305}         \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Checks that the data in input is of the correct shape.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{306}         \PYG{n}{data\PYGZus{}len} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{307}         \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}input\PYGZus{}len} \PYG{o}{!=} \PYG{n}{data\PYGZus{}len}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{308}             \PYG{n}{msg} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Received }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{ features, expected }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{data\PYGZus{}len}\PYG{p}{,}

\PYG{n+nn}{\PYGZti{}/miniconda3/envs/imbalanced/lib/python3.7/site\PYGZhy{}packages/scipy/sparse/base.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{289}     \PYG{c+c1}{\PYGZsh{} non\PYGZhy{}zeros is more important.  For now, raise an exception!}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{290}     \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{291}         \PYG{k}{raise} \PYG{n+ne}{TypeError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sparse matrix length is ambiguous; use getnnz()}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{292}                         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ or shape[0]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{293} 

\PYG{n+ne}{TypeError}: sparse matrix length is ambiguous; use getnnz() or shape[0]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(18846, 173438)
\end{sphinxVerbatim}







\renewcommand{\indexname}{Index}
\printindex
\end{document}